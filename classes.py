# -*- coding: utf-8 -*-
"""classes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nbJa_WF-p8Zy4dJ_w1QnYybMjhkUxtZJ
"""

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import os

class TabularDataset(Dataset):
  def __init__(self, data, cat_cols=None, output_col=None):
    
    self.n=data.shape[0]
    
    if output_col: 
      self.y=data[output_col].astype(np.int64).values.reshape(-1,1)
    else: 
      self.y=np.zeros((self.n,1)) #all output =0 if output is not specified
    
    self.cat_cols=cat_cols if cat_cols else []
    self.cont_cols=[col for col in data.columns if col not in self.cat_cols
                   + [output_col]]
    
    if self.cont_cols:
      self.cont_x=data[self.cont_cols].astype(np.float32).values
    else:
      self.cont_x=np.zeros((self.n,1))
    
    if self.cat_cols:
      self.cat_x=data[cat_cols].astype(np.int64).values
    else:
      self.cat_x=np.zeros((self.n,1))
      
  def __len__(self):
    return self.n
  
  def __getitem__(self,idx):
    return [self.y[idx],self.cont_x[idx],self.cat_x[idx]]

# embedding emb i.e. categorical features
class FeedForwardNN(nn.Module):
  def __init__(self,emb_dims,no_of_cont, lin_layer_sizes,
              output_size,emb_dropout, lin_layer_dropouts):
    '''
    #emb_dims: list of two tuples
    #tuple1: no. of unqie values for that categorical variable
    #tuple 2: shape of that features data
    
    #no_of_cont: number of continuous features
    
    lin_layer_sizes: list of integers
    no. of nodes in each linear layer in the network
    
    output_size=Integer
    size of final output
    
    emb_dropout: float
    dropout used after embedding layers
    
    lin_layer_dropouts: 
    dropout after each linear layer
    
    '''
    super().__init__()
    
    #Embedding layers or layers with categorical layers
    self.emb_layers=nn.ModuleList([nn.Embedding(x,y)
                                  for x,y in emb_dims])
    # no of categorical features
    no_of_embs=sum([y for x,y in emb_dims])
    self.no_of_embs=no_of_embs
    self.no_of_cont=no_of_cont
    
    #Linear Layers
    first_lin_layer=nn.Linear(self.no_of_embs +self.no_of_cont,
                             lin_layer_sizes[0])
    self.lin_layers=nn.ModuleList([first_lin_layer]+
                                 [nn.Linear(lin_layer_sizes[i],lin_layer_sizes[i+1])
                                 for i in range(len(lin_layer_sizes)-1)])
    
    for lin_layer in self.lin_layers:
      nn.init.kaiming_normal_(lin_layer.weight.data)
     
    #Output Layer
    self.output_layer=nn.Linear(lin_layer_sizes[-1],
                                               output_size)
    nn.init.kaiming_normal_(self.output_layer.weight.data)

    #BatchNorm Layer on continuous variables
    self.first_bn_layer=nn.BatchNorm1d(self.no_of_cont)
    self.bn_layers=nn.ModuleList([self.first_bn_layer]+
                                [nn.BatchNorm1d(size) 
                                for size in lin_layer_sizes])
    
    #Dropout Layer
    self.emb_dropout_layer=nn.Dropout(emb_dropout)
    self.dropout_layers=nn.ModuleList([nn.Dropout(dropout)
                                     for dropout in lin_layer_dropouts])
    
  def forward(self,cont_data,cat_data):
    if self.no_of_embs!=0:
      #Wakanda shit is this?
      x = [emb_layer(cat_data[:,i])
          for i,emb_layer in enumerate(self.emb_layers)]
      x=torch.cat(x,1)
      x=self.emb_dropout_layer(x)
      
      if self.no_of_cont!=0:
        normalized_cont_data=self.first_bn_layer(cont_data)
        
        if self.no_of_embs!=0:
          x=torch.cat([x,normalized_cont_data],1)
        else:
          x=normalized_cont_data
      for lin_layer, dropout_layer, bn_layer in zip(self.lin_layers, self.droput_layers, self.bn_layers):
      
        x = F.relu(lin_layer(x))
        x = bn_layer(x)
        x = dropout_layer(x)

    x = self.output_layer(x)

    return x

