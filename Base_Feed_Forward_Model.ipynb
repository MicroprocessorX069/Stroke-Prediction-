{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Base_Feed Forward Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Stroke-Prediction-/blob/master/Base_Feed_Forward_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFbxYJCP7mmT",
        "colab_type": "code",
        "outputId": "445403b3-3adb-4f03-8d6d-558e105a5476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2alqmYSJA_gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# from classes3 import  FeedForwardNN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from datetime import datetime\n",
        "# import visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmRrCmkDdcm-",
        "colab_type": "code",
        "outputId": "65d60582-d4e3-492a-e259-b06a6f6658ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path=\"/content/drive/My Drive/StrokePrediction/data/healthcare-dataset-stroke-data\"\n",
        "data= pd.read_csv(os.path.join(path,\"train_2v.csv\")).dropna()\n",
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29072, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWGDWESVJgwR",
        "colab_type": "code",
        "outputId": "49821dd1-7ef8-4785-d8a8-903d30f5fb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['Weights'] = np.where(data['stroke'] == 1, 0.9, .004)\n",
        "data['Weights'].unique()\n",
        "\n",
        "val_data=data.sample(frac=0.05, random_state=123, weights='Weights')\n",
        "#train_data = data.loc[~data.index.isin(val_data.index)]\n",
        "print(len(np.where(val_data['stroke']==1)[0]),len(np.where(val_data['stroke']==0)[0]))\n",
        "data_org=data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "548 906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j22PLGFaNdym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class TabularDataset2(Dataset):\n",
        "  def __init__(self, data, cat_cols=None, output_col=None):\n",
        "    \"\"\"\n",
        "    Characterizes a Dataset for PyTorch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    data: pandas data frame\n",
        "      The data frame object for the input data. It must\n",
        "      contain all the continuous, categorical and the\n",
        "      output columns to be used.\n",
        "\n",
        "    cat_cols: List of strings\n",
        "      The names of the categorical columns in the data.\n",
        "      These columns will be passed through the embedding\n",
        "      layers in the model. These columns must be\n",
        "      label encoded beforehand. \n",
        "\n",
        "    output_col: string\n",
        "      The name of the output variable column in the data\n",
        "      provided.\n",
        "    \"\"\"\n",
        "\n",
        "    self.n = data.shape[0]\n",
        "\n",
        "    if output_col:\n",
        "      self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
        "    else:\n",
        "      self.y =  np.zeros((self.n, 1))\n",
        "\n",
        "    self.cat_cols = cat_cols if cat_cols else []\n",
        "    self.cont_cols = [col for col in data.columns\n",
        "                      if col not in self.cat_cols + [output_col]]\n",
        "\n",
        "    if self.cont_cols:\n",
        "      self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
        "    else:\n",
        "      self.cont_X = np.zeros((self.n, 1))\n",
        "\n",
        "    if self.cat_cols:\n",
        "      self.cat_X = data[cat_cols].astype(np.int64).values\n",
        "    else:\n",
        "      self.cat_X =  np.zeros((self.n, 1))\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Denotes the total number of samples.\n",
        "    \"\"\"\n",
        "    return self.n\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Generates one sample of data.\n",
        "    \"\"\"\n",
        "    return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self,emb_dims,no_of_cont, lin_layer_sizes,\n",
        "              output_size,emb_dropout, lin_layer_dropouts):\n",
        "    '''\n",
        "    #emb_dims: list of two tuples\n",
        "    #tuple1: no. of unqie values for that categorical variable\n",
        "    #tuple 2: shape of that features data\n",
        "    \n",
        "    #no_of_cont: number of continuous features\n",
        "    \n",
        "    lin_layer_sizes: list of integers\n",
        "    no. of nodes in each linear layer in the network\n",
        "    \n",
        "    output_size=Integer\n",
        "    size of final output\n",
        "    \n",
        "    emb_dropout: float\n",
        "    dropout used after embedding layers\n",
        "    \n",
        "    lin_layer_dropouts: \n",
        "    dropout after each linear layer\n",
        "    \n",
        "    '''\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding layers\n",
        "    self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
        "                                     for x, y in emb_dims])\n",
        "\n",
        "    no_of_embs = sum([y for x, y in emb_dims])\n",
        "    self.no_of_embs = no_of_embs\n",
        "    self.no_of_cont = no_of_cont\n",
        "\n",
        "    # Linear Layers\n",
        "    first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
        "                                lin_layer_sizes[0])\n",
        "\n",
        "    self.lin_layers =\\\n",
        "     nn.ModuleList([first_lin_layer] +\\\n",
        "          [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
        "           for i in range(len(lin_layer_sizes) - 1)])\n",
        "    \n",
        "    for lin_layer in self.lin_layers:\n",
        "      nn.init.kaiming_normal_(lin_layer.weight.data)\n",
        "\n",
        "    # Output Layer\n",
        "    self.output_layer = nn.Sequential(nn.Linear(lin_layer_sizes[-1],\n",
        "                                  output_size),nn.Sigmoid())\n",
        "    #nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
        "\n",
        "    # Batch Norm Layers\n",
        "    self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
        "    self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
        "                                    for size in lin_layer_sizes])\n",
        "\n",
        "    # Dropout Layers\n",
        "    self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
        "    self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
        "                                  for size in lin_layer_dropouts])\n",
        "\n",
        "  def forward(self,cont_data,cat_data):\n",
        "    if self.no_of_embs != 0:\n",
        "      x = [emb_layer(cat_data[:, i])\n",
        "           for i,emb_layer in enumerate(self.emb_layers)]\n",
        "      x = torch.cat(x, 1)\n",
        "      x = self.emb_dropout_layer(x)\n",
        "\n",
        "    if self.no_of_cont != 0:\n",
        "      normalized_cont_data = self.first_bn_layer(cont_data)\n",
        "\n",
        "      if self.no_of_embs != 0:\n",
        "        x = torch.cat([x, normalized_cont_data], 1) \n",
        "      else:\n",
        "        x = normalized_cont_data\n",
        "\n",
        "    for lin_layer, dropout_layer, bn_layer in\\\n",
        "        zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
        "      \n",
        "      x = F.relu(lin_layer(x))\n",
        "      x = bn_layer(x)\n",
        "      x = dropout_layer(x)\n",
        "    \n",
        "    x = self.output_layer(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioPokgp98z-t",
        "colab_type": "text"
      },
      "source": [
        "##Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKaku-1X2mWM",
        "colab_type": "code",
        "outputId": "51b91916-9dc6-4eee-d53e-475382c17d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data.stroke.value_counts()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    28524\n",
              "1      548\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XPPcxl5hF9I",
        "colab_type": "text"
      },
      "source": [
        "##Features segregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-7p_V0hIfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features=[\"gender\",\"hypertension\",\"heart_disease\",\n",
        "                      \"ever_married\",\"work_type\",\"Residence_type\",\n",
        "                     \"smoking_status\"]\n",
        "output_feature=\"stroke\"\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders={}\n",
        "for cat_col in categorical_features:\n",
        "  label_encoders[cat_col]=LabelEncoder()\n",
        "  data[cat_col]=label_encoders[cat_col].fit_transform(data[cat_col])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2O4j3KsA7qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.drop(\"Weights\",axis=1)\n",
        "data_org=data\n",
        "data=data.drop(\"id\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G2jVY2giwvB",
        "colab_type": "text"
      },
      "source": [
        "##Creating object of class 'Tabular Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPGsEHSri4P0",
        "colab_type": "code",
        "outputId": "ec3759f6-94de-4a72-de60-db17b5b04e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dataset=TabularDataset2(data=data,cat_cols=categorical_features,\n",
        "                      output_col=output_feature)\n",
        "dataset[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.], dtype=float32),\n",
              " array([58.  , 87.96, 39.2 ], dtype=float32),\n",
              " array([1, 1, 0, 1, 2, 1, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-5xFyF1Yty",
        "colab_type": "text"
      },
      "source": [
        "##Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXQP-wy1a1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=800\n",
        "dataloader=DataLoader(dataset,batchsize,shuffle=True,num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw-BYm0eK_vA",
        "colab_type": "text"
      },
      "source": [
        "##Getting the dimensions for embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ5xq_mLJ9sI",
        "colab_type": "code",
        "outputId": "e46beb08-c3e1-4042-b326-ba7557c5f3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cat_dims=[int(data[col].nunique()) for col in categorical_features] #no of unique values for each categorical variable\n",
        "emb_dims=[(x,min(128,(x+1)//2)) for x in cat_dims] #reducing the embedding to half size.\n",
        "emb_dims"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 2), (2, 1), (2, 1), (2, 1), (5, 3), (2, 1), (3, 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ10_nIiK62z",
        "colab_type": "text"
      },
      "source": [
        "##Creating an instance of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqVKaSZRK_SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Hyper parameters\n",
        "no_of_cont=3 #no of continuous variables \n",
        "lin_layer_sizes=[128,64]\n",
        "output_size=1\n",
        "emb_dropout=0.04\n",
        "lin_layer_dropouts=[0.001,0.01]\n",
        "model= FeedForwardNN(emb_dims, no_of_cont, lin_layer_sizes,\n",
        "                    output_size, emb_dropout,\n",
        "                    lin_layer_dropouts).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMsF-kmVWsDc",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBT1MCuAWuOs",
        "colab_type": "code",
        "outputId": "86203de6-f0c5-4516-b507-64f3284dcedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47737
        }
      },
      "source": [
        "# vis = Visualizations()\n",
        "\n",
        "no_of_epochs = 5000\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# criterion=nn.MSELoss()\n",
        "criterion=nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "loss_values = []\n",
        "for epoch in range(no_of_epochs):\n",
        "      i=0\n",
        "      \n",
        "      \n",
        "      for y, cont_x, cat_x in dataloader:\n",
        "        i+=200\n",
        "        cat_x = cat_x.to(device)\n",
        "        cont_x = cont_x.to(device)\n",
        "        y  = y.to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        preds = model(cont_x, cat_x)\n",
        "                \n",
        "        loss = criterion(preds, y)\n",
        "\n",
        "        # Backward Pass and Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        if(i==200):\n",
        "          print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                .format(epoch+1, no_of_epochs, i+1, batchsize, loss.item()))\n",
        "          \n",
        "      loss_values.append(loss)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5000], Step [201/1454], Loss: 0.7047\n",
            "Epoch [2/5000], Step [201/1454], Loss: 0.0938\n",
            "Epoch [3/5000], Step [201/1454], Loss: 0.0770\n",
            "Epoch [4/5000], Step [201/1454], Loss: 0.0963\n",
            "Epoch [5/5000], Step [201/1454], Loss: 0.0767\n",
            "Epoch [6/5000], Step [201/1454], Loss: 0.0751\n",
            "Epoch [7/5000], Step [201/1454], Loss: 0.0907\n",
            "Epoch [8/5000], Step [201/1454], Loss: 0.0710\n",
            "Epoch [9/5000], Step [201/1454], Loss: 0.0607\n",
            "Epoch [10/5000], Step [201/1454], Loss: 0.0877\n",
            "Epoch [11/5000], Step [201/1454], Loss: 0.0750\n",
            "Epoch [12/5000], Step [201/1454], Loss: 0.0746\n",
            "Epoch [13/5000], Step [201/1454], Loss: 0.0848\n",
            "Epoch [14/5000], Step [201/1454], Loss: 0.0807\n",
            "Epoch [15/5000], Step [201/1454], Loss: 0.0743\n",
            "Epoch [16/5000], Step [201/1454], Loss: 0.1045\n",
            "Epoch [17/5000], Step [201/1454], Loss: 0.0466\n",
            "Epoch [18/5000], Step [201/1454], Loss: 0.0589\n",
            "Epoch [19/5000], Step [201/1454], Loss: 0.0755\n",
            "Epoch [20/5000], Step [201/1454], Loss: 0.0717\n",
            "Epoch [21/5000], Step [201/1454], Loss: 0.0586\n",
            "Epoch [22/5000], Step [201/1454], Loss: 0.0842\n",
            "Epoch [23/5000], Step [201/1454], Loss: 0.0974\n",
            "Epoch [24/5000], Step [201/1454], Loss: 0.0812\n",
            "Epoch [25/5000], Step [201/1454], Loss: 0.1031\n",
            "Epoch [26/5000], Step [201/1454], Loss: 0.0734\n",
            "Epoch [27/5000], Step [201/1454], Loss: 0.0592\n",
            "Epoch [28/5000], Step [201/1454], Loss: 0.0794\n",
            "Epoch [29/5000], Step [201/1454], Loss: 0.0621\n",
            "Epoch [30/5000], Step [201/1454], Loss: 0.0682\n",
            "Epoch [31/5000], Step [201/1454], Loss: 0.0794\n",
            "Epoch [32/5000], Step [201/1454], Loss: 0.1112\n",
            "Epoch [33/5000], Step [201/1454], Loss: 0.0628\n",
            "Epoch [34/5000], Step [201/1454], Loss: 0.0647\n",
            "Epoch [35/5000], Step [201/1454], Loss: 0.0843\n",
            "Epoch [36/5000], Step [201/1454], Loss: 0.0549\n",
            "Epoch [37/5000], Step [201/1454], Loss: 0.0566\n",
            "Epoch [38/5000], Step [201/1454], Loss: 0.0572\n",
            "Epoch [39/5000], Step [201/1454], Loss: 0.1043\n",
            "Epoch [40/5000], Step [201/1454], Loss: 0.0917\n",
            "Epoch [41/5000], Step [201/1454], Loss: 0.0955\n",
            "Epoch [42/5000], Step [201/1454], Loss: 0.0864\n",
            "Epoch [43/5000], Step [201/1454], Loss: 0.0713\n",
            "Epoch [44/5000], Step [201/1454], Loss: 0.0770\n",
            "Epoch [45/5000], Step [201/1454], Loss: 0.0904\n",
            "Epoch [46/5000], Step [201/1454], Loss: 0.0661\n",
            "Epoch [47/5000], Step [201/1454], Loss: 0.0670\n",
            "Epoch [48/5000], Step [201/1454], Loss: 0.0578\n",
            "Epoch [49/5000], Step [201/1454], Loss: 0.0739\n",
            "Epoch [50/5000], Step [201/1454], Loss: 0.0772\n",
            "Epoch [51/5000], Step [201/1454], Loss: 0.0615\n",
            "Epoch [52/5000], Step [201/1454], Loss: 0.0503\n",
            "Epoch [53/5000], Step [201/1454], Loss: 0.0819\n",
            "Epoch [54/5000], Step [201/1454], Loss: 0.0515\n",
            "Epoch [55/5000], Step [201/1454], Loss: 0.0860\n",
            "Epoch [56/5000], Step [201/1454], Loss: 0.1059\n",
            "Epoch [57/5000], Step [201/1454], Loss: 0.0621\n",
            "Epoch [58/5000], Step [201/1454], Loss: 0.0579\n",
            "Epoch [59/5000], Step [201/1454], Loss: 0.0975\n",
            "Epoch [60/5000], Step [201/1454], Loss: 0.0734\n",
            "Epoch [61/5000], Step [201/1454], Loss: 0.0944\n",
            "Epoch [62/5000], Step [201/1454], Loss: 0.0780\n",
            "Epoch [63/5000], Step [201/1454], Loss: 0.0678\n",
            "Epoch [64/5000], Step [201/1454], Loss: 0.0582\n",
            "Epoch [65/5000], Step [201/1454], Loss: 0.0893\n",
            "Epoch [66/5000], Step [201/1454], Loss: 0.0615\n",
            "Epoch [67/5000], Step [201/1454], Loss: 0.0766\n",
            "Epoch [68/5000], Step [201/1454], Loss: 0.0647\n",
            "Epoch [69/5000], Step [201/1454], Loss: 0.0716\n",
            "Epoch [70/5000], Step [201/1454], Loss: 0.0662\n",
            "Epoch [71/5000], Step [201/1454], Loss: 0.0653\n",
            "Epoch [72/5000], Step [201/1454], Loss: 0.0863\n",
            "Epoch [73/5000], Step [201/1454], Loss: 0.0599\n",
            "Epoch [74/5000], Step [201/1454], Loss: 0.0820\n",
            "Epoch [75/5000], Step [201/1454], Loss: 0.0536\n",
            "Epoch [76/5000], Step [201/1454], Loss: 0.0758\n",
            "Epoch [77/5000], Step [201/1454], Loss: 0.0556\n",
            "Epoch [78/5000], Step [201/1454], Loss: 0.0432\n",
            "Epoch [79/5000], Step [201/1454], Loss: 0.0673\n",
            "Epoch [80/5000], Step [201/1454], Loss: 0.0634\n",
            "Epoch [81/5000], Step [201/1454], Loss: 0.0800\n",
            "Epoch [82/5000], Step [201/1454], Loss: 0.0745\n",
            "Epoch [83/5000], Step [201/1454], Loss: 0.0626\n",
            "Epoch [84/5000], Step [201/1454], Loss: 0.0635\n",
            "Epoch [85/5000], Step [201/1454], Loss: 0.0738\n",
            "Epoch [86/5000], Step [201/1454], Loss: 0.0583\n",
            "Epoch [87/5000], Step [201/1454], Loss: 0.0451\n",
            "Epoch [88/5000], Step [201/1454], Loss: 0.0743\n",
            "Epoch [89/5000], Step [201/1454], Loss: 0.0459\n",
            "Epoch [90/5000], Step [201/1454], Loss: 0.0750\n",
            "Epoch [91/5000], Step [201/1454], Loss: 0.0679\n",
            "Epoch [92/5000], Step [201/1454], Loss: 0.0556\n",
            "Epoch [93/5000], Step [201/1454], Loss: 0.0643\n",
            "Epoch [94/5000], Step [201/1454], Loss: 0.0800\n",
            "Epoch [95/5000], Step [201/1454], Loss: 0.0618\n",
            "Epoch [96/5000], Step [201/1454], Loss: 0.0642\n",
            "Epoch [97/5000], Step [201/1454], Loss: 0.0765\n",
            "Epoch [98/5000], Step [201/1454], Loss: 0.0702\n",
            "Epoch [99/5000], Step [201/1454], Loss: 0.0698\n",
            "Epoch [100/5000], Step [201/1454], Loss: 0.0616\n",
            "Epoch [101/5000], Step [201/1454], Loss: 0.0669\n",
            "Epoch [102/5000], Step [201/1454], Loss: 0.0335\n",
            "Epoch [103/5000], Step [201/1454], Loss: 0.0693\n",
            "Epoch [104/5000], Step [201/1454], Loss: 0.0738\n",
            "Epoch [105/5000], Step [201/1454], Loss: 0.0584\n",
            "Epoch [106/5000], Step [201/1454], Loss: 0.0537\n",
            "Epoch [107/5000], Step [201/1454], Loss: 0.0853\n",
            "Epoch [108/5000], Step [201/1454], Loss: 0.0734\n",
            "Epoch [109/5000], Step [201/1454], Loss: 0.0765\n",
            "Epoch [110/5000], Step [201/1454], Loss: 0.0738\n",
            "Epoch [111/5000], Step [201/1454], Loss: 0.0823\n",
            "Epoch [112/5000], Step [201/1454], Loss: 0.0766\n",
            "Epoch [113/5000], Step [201/1454], Loss: 0.0651\n",
            "Epoch [114/5000], Step [201/1454], Loss: 0.0503\n",
            "Epoch [115/5000], Step [201/1454], Loss: 0.0543\n",
            "Epoch [116/5000], Step [201/1454], Loss: 0.0919\n",
            "Epoch [117/5000], Step [201/1454], Loss: 0.0715\n",
            "Epoch [118/5000], Step [201/1454], Loss: 0.0406\n",
            "Epoch [119/5000], Step [201/1454], Loss: 0.0508\n",
            "Epoch [120/5000], Step [201/1454], Loss: 0.0703\n",
            "Epoch [121/5000], Step [201/1454], Loss: 0.0571\n",
            "Epoch [122/5000], Step [201/1454], Loss: 0.0616\n",
            "Epoch [123/5000], Step [201/1454], Loss: 0.0635\n",
            "Epoch [124/5000], Step [201/1454], Loss: 0.0799\n",
            "Epoch [125/5000], Step [201/1454], Loss: 0.0704\n",
            "Epoch [126/5000], Step [201/1454], Loss: 0.0715\n",
            "Epoch [127/5000], Step [201/1454], Loss: 0.0860\n",
            "Epoch [128/5000], Step [201/1454], Loss: 0.0486\n",
            "Epoch [129/5000], Step [201/1454], Loss: 0.0697\n",
            "Epoch [130/5000], Step [201/1454], Loss: 0.0773\n",
            "Epoch [131/5000], Step [201/1454], Loss: 0.0615\n",
            "Epoch [132/5000], Step [201/1454], Loss: 0.0780\n",
            "Epoch [133/5000], Step [201/1454], Loss: 0.0724\n",
            "Epoch [134/5000], Step [201/1454], Loss: 0.0666\n",
            "Epoch [135/5000], Step [201/1454], Loss: 0.0549\n",
            "Epoch [136/5000], Step [201/1454], Loss: 0.0653\n",
            "Epoch [137/5000], Step [201/1454], Loss: 0.0823\n",
            "Epoch [138/5000], Step [201/1454], Loss: 0.0494\n",
            "Epoch [139/5000], Step [201/1454], Loss: 0.0423\n",
            "Epoch [140/5000], Step [201/1454], Loss: 0.0882\n",
            "Epoch [141/5000], Step [201/1454], Loss: 0.0624\n",
            "Epoch [142/5000], Step [201/1454], Loss: 0.0814\n",
            "Epoch [143/5000], Step [201/1454], Loss: 0.0694\n",
            "Epoch [144/5000], Step [201/1454], Loss: 0.0452\n",
            "Epoch [145/5000], Step [201/1454], Loss: 0.0439\n",
            "Epoch [146/5000], Step [201/1454], Loss: 0.0569\n",
            "Epoch [147/5000], Step [201/1454], Loss: 0.0517\n",
            "Epoch [148/5000], Step [201/1454], Loss: 0.0432\n",
            "Epoch [149/5000], Step [201/1454], Loss: 0.0644\n",
            "Epoch [150/5000], Step [201/1454], Loss: 0.0725\n",
            "Epoch [151/5000], Step [201/1454], Loss: 0.0730\n",
            "Epoch [152/5000], Step [201/1454], Loss: 0.0554\n",
            "Epoch [153/5000], Step [201/1454], Loss: 0.0531\n",
            "Epoch [154/5000], Step [201/1454], Loss: 0.0954\n",
            "Epoch [155/5000], Step [201/1454], Loss: 0.0613\n",
            "Epoch [156/5000], Step [201/1454], Loss: 0.0421\n",
            "Epoch [157/5000], Step [201/1454], Loss: 0.0964\n",
            "Epoch [158/5000], Step [201/1454], Loss: 0.0602\n",
            "Epoch [159/5000], Step [201/1454], Loss: 0.0609\n",
            "Epoch [160/5000], Step [201/1454], Loss: 0.0810\n",
            "Epoch [161/5000], Step [201/1454], Loss: 0.0655\n",
            "Epoch [162/5000], Step [201/1454], Loss: 0.0625\n",
            "Epoch [163/5000], Step [201/1454], Loss: 0.0729\n",
            "Epoch [164/5000], Step [201/1454], Loss: 0.0858\n",
            "Epoch [165/5000], Step [201/1454], Loss: 0.0678\n",
            "Epoch [166/5000], Step [201/1454], Loss: 0.0570\n",
            "Epoch [167/5000], Step [201/1454], Loss: 0.0512\n",
            "Epoch [168/5000], Step [201/1454], Loss: 0.0615\n",
            "Epoch [169/5000], Step [201/1454], Loss: 0.0591\n",
            "Epoch [170/5000], Step [201/1454], Loss: 0.0533\n",
            "Epoch [171/5000], Step [201/1454], Loss: 0.0693\n",
            "Epoch [172/5000], Step [201/1454], Loss: 0.0454\n",
            "Epoch [173/5000], Step [201/1454], Loss: 0.0705\n",
            "Epoch [174/5000], Step [201/1454], Loss: 0.1006\n",
            "Epoch [175/5000], Step [201/1454], Loss: 0.0507\n",
            "Epoch [176/5000], Step [201/1454], Loss: 0.0565\n",
            "Epoch [177/5000], Step [201/1454], Loss: 0.0473\n",
            "Epoch [178/5000], Step [201/1454], Loss: 0.0616\n",
            "Epoch [179/5000], Step [201/1454], Loss: 0.0827\n",
            "Epoch [180/5000], Step [201/1454], Loss: 0.0777\n",
            "Epoch [181/5000], Step [201/1454], Loss: 0.0895\n",
            "Epoch [182/5000], Step [201/1454], Loss: 0.0651\n",
            "Epoch [183/5000], Step [201/1454], Loss: 0.0783\n",
            "Epoch [184/5000], Step [201/1454], Loss: 0.0668\n",
            "Epoch [185/5000], Step [201/1454], Loss: 0.0767\n",
            "Epoch [186/5000], Step [201/1454], Loss: 0.0846\n",
            "Epoch [187/5000], Step [201/1454], Loss: 0.0490\n",
            "Epoch [188/5000], Step [201/1454], Loss: 0.0585\n",
            "Epoch [189/5000], Step [201/1454], Loss: 0.0840\n",
            "Epoch [190/5000], Step [201/1454], Loss: 0.0324\n",
            "Epoch [191/5000], Step [201/1454], Loss: 0.0651\n",
            "Epoch [192/5000], Step [201/1454], Loss: 0.0561\n",
            "Epoch [193/5000], Step [201/1454], Loss: 0.0826\n",
            "Epoch [194/5000], Step [201/1454], Loss: 0.0555\n",
            "Epoch [195/5000], Step [201/1454], Loss: 0.0831\n",
            "Epoch [196/5000], Step [201/1454], Loss: 0.0437\n",
            "Epoch [197/5000], Step [201/1454], Loss: 0.0922\n",
            "Epoch [198/5000], Step [201/1454], Loss: 0.0554\n",
            "Epoch [199/5000], Step [201/1454], Loss: 0.0445\n",
            "Epoch [200/5000], Step [201/1454], Loss: 0.0544\n",
            "Epoch [201/5000], Step [201/1454], Loss: 0.0680\n",
            "Epoch [202/5000], Step [201/1454], Loss: 0.0708\n",
            "Epoch [203/5000], Step [201/1454], Loss: 0.0835\n",
            "Epoch [204/5000], Step [201/1454], Loss: 0.0613\n",
            "Epoch [205/5000], Step [201/1454], Loss: 0.0590\n",
            "Epoch [206/5000], Step [201/1454], Loss: 0.0716\n",
            "Epoch [207/5000], Step [201/1454], Loss: 0.0434\n",
            "Epoch [208/5000], Step [201/1454], Loss: 0.0405\n",
            "Epoch [209/5000], Step [201/1454], Loss: 0.0598\n",
            "Epoch [210/5000], Step [201/1454], Loss: 0.0575\n",
            "Epoch [211/5000], Step [201/1454], Loss: 0.0668\n",
            "Epoch [212/5000], Step [201/1454], Loss: 0.0593\n",
            "Epoch [213/5000], Step [201/1454], Loss: 0.0694\n",
            "Epoch [214/5000], Step [201/1454], Loss: 0.0713\n",
            "Epoch [215/5000], Step [201/1454], Loss: 0.0536\n",
            "Epoch [216/5000], Step [201/1454], Loss: 0.0830\n",
            "Epoch [217/5000], Step [201/1454], Loss: 0.0456\n",
            "Epoch [218/5000], Step [201/1454], Loss: 0.0550\n",
            "Epoch [219/5000], Step [201/1454], Loss: 0.0851\n",
            "Epoch [220/5000], Step [201/1454], Loss: 0.0529\n",
            "Epoch [221/5000], Step [201/1454], Loss: 0.0958\n",
            "Epoch [222/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [223/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [224/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [225/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [226/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [227/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [228/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [229/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [230/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [231/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [232/5000], Step [201/1454], Loss: 0.4858\n",
            "Epoch [233/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [234/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [235/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [236/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [237/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [238/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [239/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [240/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [241/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [242/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [243/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [244/5000], Step [201/1454], Loss: 0.8703\n",
            "Epoch [245/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [246/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [247/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [248/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [249/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [250/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [251/5000], Step [201/1454], Loss: 0.4525\n",
            "Epoch [252/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [253/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [254/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [255/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [256/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [257/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [258/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [259/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [260/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [261/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [262/5000], Step [201/1454], Loss: 0.7454\n",
            "Epoch [263/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [264/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [265/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [266/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [267/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [268/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [269/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [270/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [271/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [272/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [273/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [274/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [275/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [276/5000], Step [201/1454], Loss: 0.9009\n",
            "Epoch [277/5000], Step [201/1454], Loss: 0.6889\n",
            "Epoch [278/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [279/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [280/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [281/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [282/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [283/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [284/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [285/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [286/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [287/5000], Step [201/1454], Loss: 0.5868\n",
            "Epoch [288/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [289/5000], Step [201/1454], Loss: 0.7551\n",
            "Epoch [290/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [291/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [292/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [293/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [294/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [295/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [296/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [297/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [298/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [299/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [300/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [301/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [302/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [303/5000], Step [201/1454], Loss: 0.8500\n",
            "Epoch [304/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [305/5000], Step [201/1454], Loss: 0.5204\n",
            "Epoch [306/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [307/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [308/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [309/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [310/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [311/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [312/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [313/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [314/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [315/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [316/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [317/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [318/5000], Step [201/1454], Loss: 0.5711\n",
            "Epoch [319/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [320/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [321/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [322/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [323/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [324/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [325/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [326/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [327/5000], Step [201/1454], Loss: 0.5251\n",
            "Epoch [328/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [329/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [330/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [331/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [332/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [333/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [334/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [335/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [336/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [337/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [338/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [339/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [340/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [341/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [342/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [343/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [344/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [345/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [346/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [347/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [348/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [349/5000], Step [201/1454], Loss: 0.3515\n",
            "Epoch [350/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [351/5000], Step [201/1454], Loss: 0.7584\n",
            "Epoch [352/5000], Step [201/1454], Loss: 0.5517\n",
            "Epoch [353/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [354/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [355/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [356/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [357/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [358/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [359/5000], Step [201/1454], Loss: 0.5513\n",
            "Epoch [360/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [361/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [362/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [363/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [364/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [365/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [366/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [367/5000], Step [201/1454], Loss: 0.9367\n",
            "Epoch [368/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [369/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [370/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [371/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [372/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [373/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [374/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [375/5000], Step [201/1454], Loss: 0.7089\n",
            "Epoch [376/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [377/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [378/5000], Step [201/1454], Loss: 0.6247\n",
            "Epoch [379/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [380/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [381/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [382/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [383/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [384/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [385/5000], Step [201/1454], Loss: 0.4525\n",
            "Epoch [386/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [387/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [388/5000], Step [201/1454], Loss: 0.5968\n",
            "Epoch [389/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [390/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [391/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [392/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [393/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [394/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [395/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [396/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [397/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [398/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [399/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [400/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [401/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [402/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [403/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [404/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [405/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [406/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [407/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [408/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [409/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [410/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [411/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [412/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [413/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [414/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [415/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [416/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [417/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [418/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [419/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [420/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [421/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [422/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [423/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [424/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [425/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [426/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [427/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [428/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [429/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [430/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [431/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [432/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [433/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [434/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [435/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [436/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [437/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [438/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [439/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [440/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [441/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [442/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [443/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [444/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [445/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [446/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [447/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [448/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [449/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [450/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [451/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [452/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [453/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [454/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [455/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [456/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [457/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [458/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [459/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [460/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [461/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [462/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [463/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [464/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [465/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [466/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [467/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [468/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [469/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [470/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [471/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [472/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [473/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [474/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [475/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [476/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [477/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [478/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [479/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [480/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [481/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [482/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [483/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [484/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [485/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [486/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [487/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [488/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [489/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [490/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [491/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [492/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [493/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [494/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [495/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [496/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [497/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [498/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [499/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [500/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [501/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [502/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [503/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [504/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [505/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [506/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [507/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [508/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [509/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [510/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [511/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [512/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [513/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [514/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [515/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [516/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [517/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [518/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [519/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [520/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [521/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [522/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [523/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [524/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [525/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [526/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [527/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [528/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [529/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [530/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [531/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [532/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [533/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [534/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [535/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [536/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [537/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [538/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [539/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [540/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [541/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [542/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [543/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [544/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [545/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [546/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [547/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [548/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [549/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [550/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [551/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [552/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [553/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [554/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [555/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [556/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [557/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [558/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [559/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [560/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [561/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [562/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [563/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [564/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [565/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [566/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [567/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [568/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [569/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [570/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [571/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [572/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [573/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [574/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [575/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [576/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [577/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [578/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [579/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [580/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [581/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [582/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [583/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [584/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [585/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [586/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [587/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [588/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [589/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [590/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [591/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [592/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [593/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [594/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [595/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [596/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [597/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [598/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [599/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [600/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [601/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [602/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [603/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [604/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [605/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [606/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [607/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [608/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [609/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [610/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [611/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [612/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [613/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [614/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [615/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [616/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [617/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [618/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [619/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [620/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [621/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [622/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [623/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [624/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [625/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [626/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [627/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [628/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [629/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [630/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [631/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [632/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [633/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [634/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [635/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [636/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [637/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [638/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [639/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [640/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [641/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [642/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [643/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [644/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [645/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [646/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [647/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [648/5000], Step [201/1454], Loss: 0.9325\n",
            "Epoch [649/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [650/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [651/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [652/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [653/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [654/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [655/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [656/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [657/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [658/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [659/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [660/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [661/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [662/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [663/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [664/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [665/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [666/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [667/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [668/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [669/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [670/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [671/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [672/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [673/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [674/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [675/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [676/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [677/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [678/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [679/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [680/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [681/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [682/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [683/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [684/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [685/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [686/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [687/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [688/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [689/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [690/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [691/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [692/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [693/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [694/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [695/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [696/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [697/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [698/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [699/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [700/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [701/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [702/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [703/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [704/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [705/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [706/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [707/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [708/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [709/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [710/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [711/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [712/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [713/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [714/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [715/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [716/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [717/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [718/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [719/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [720/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [721/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [722/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [723/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [724/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [725/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [726/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [727/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [728/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [729/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [730/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [731/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [732/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [733/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [734/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [735/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [736/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [737/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [738/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [739/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [740/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [741/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [742/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [743/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [744/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [745/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [746/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [747/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [748/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [749/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [750/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [751/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [752/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [753/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [754/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [755/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [756/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [757/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [758/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [759/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [760/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [761/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [762/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [763/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [764/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [765/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [766/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [767/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [768/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [769/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [770/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [771/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [772/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [773/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [774/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [775/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [776/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [777/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [778/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [779/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [780/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [781/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [782/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [783/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [784/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [785/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [786/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [787/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [788/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [789/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [790/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [791/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [792/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [793/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [794/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [795/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [796/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [797/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [798/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [799/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [800/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [801/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [802/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [803/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [804/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [805/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [806/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [807/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [808/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [809/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [810/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [811/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [812/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [813/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [814/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [815/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [816/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [817/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [818/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [819/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [820/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [821/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [822/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [823/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [824/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [825/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [826/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [827/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [828/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [829/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [830/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [831/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [832/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [833/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [834/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [835/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [836/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [837/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [838/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [839/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [840/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [841/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [842/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [843/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [844/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [845/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [846/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [847/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [848/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [849/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [850/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [851/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [852/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [853/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [854/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [855/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [856/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [857/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [858/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [859/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [860/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [861/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [862/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [863/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [864/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [865/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [866/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [867/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [868/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [869/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [870/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [871/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [872/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [873/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [874/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [875/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [876/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [877/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [878/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [879/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [880/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [881/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [882/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [883/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [884/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [885/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [886/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [887/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [888/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [889/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [890/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [891/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [892/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [893/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [894/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [895/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [896/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [897/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [898/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [899/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [900/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [901/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [902/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [903/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [904/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [905/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [906/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [907/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [908/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [909/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [910/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [911/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [912/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [913/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [914/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [915/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [916/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [917/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [918/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [919/5000], Step [201/1454], Loss: 1.0362\n",
            "Epoch [920/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [921/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [922/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [923/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [924/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [925/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [926/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [927/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [928/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [929/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [930/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [931/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [932/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [933/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [934/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [935/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [936/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [937/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [938/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [939/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [940/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [941/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [942/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [943/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [944/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [945/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [946/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [947/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [948/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [949/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [950/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [951/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [952/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [953/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [954/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [955/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [956/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [957/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [958/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [959/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [960/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [961/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [962/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [963/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [964/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [965/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [966/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [967/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [968/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [969/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [970/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [971/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [972/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [973/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [974/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [975/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [976/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [977/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [978/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [979/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [980/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [981/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [982/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [983/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [984/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [985/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [986/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [987/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [988/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [989/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [990/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [991/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [992/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [993/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [994/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [995/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [996/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [997/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [998/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [999/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1000/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1001/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1002/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1003/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1004/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1005/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1006/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1007/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1008/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1009/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1010/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1011/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1012/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1013/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1014/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1015/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1016/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1017/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1018/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1019/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1020/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1021/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1022/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1023/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1024/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1025/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1026/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1027/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1028/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1029/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1030/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1031/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1032/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1033/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1034/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1035/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1036/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1037/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1038/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1039/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1040/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1041/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1042/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1043/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1044/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1045/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1046/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1047/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1048/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1049/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1050/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1051/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1052/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1053/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1054/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1055/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1056/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1057/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1058/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1059/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1060/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1061/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1062/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1063/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1064/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1065/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1066/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1067/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1068/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1069/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1070/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1071/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1072/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1073/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1074/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1075/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1076/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1077/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1078/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1079/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1080/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1081/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1082/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1083/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1084/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1085/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1086/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1087/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1088/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1089/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [1090/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1091/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1092/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1093/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1094/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1095/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1096/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1097/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1098/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1099/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1100/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1101/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1102/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1103/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1104/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1105/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1106/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1107/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1108/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1109/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1110/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1111/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1112/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1113/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1114/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [1115/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1116/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1117/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1118/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1119/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1120/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1121/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1122/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1123/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1124/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1125/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1126/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1127/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1128/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1129/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1130/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1131/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1132/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1133/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1134/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1135/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1136/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1137/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1138/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1139/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1140/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1141/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1142/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1143/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1144/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1145/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1146/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1147/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1148/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1149/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1150/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1151/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1152/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1153/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1154/5000], Step [201/1454], Loss: 0.1036\n",
            "Epoch [1155/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1156/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1157/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1158/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1159/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1160/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1161/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1162/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1163/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1164/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1165/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1166/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1167/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1168/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1169/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1170/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1171/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1172/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1173/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1174/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1175/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1176/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1177/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1178/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1179/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1180/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1181/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1182/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1183/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1184/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1185/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1186/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1187/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1188/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1189/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1190/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1191/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1192/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1193/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1194/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1195/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1196/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1197/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1198/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1199/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1200/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1201/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1202/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1203/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1204/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1205/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1206/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1207/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1208/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1209/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1210/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1211/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1212/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1213/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1214/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1215/5000], Step [201/1454], Loss: 0.1382\n",
            "Epoch [1216/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1217/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1218/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1219/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1220/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1221/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1222/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1223/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1224/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1225/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1226/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1227/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1228/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1229/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1230/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1231/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1232/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1233/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1234/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1235/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1236/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1237/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1238/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1239/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1240/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1241/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1242/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1243/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1244/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1245/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1246/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1247/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1248/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1249/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1250/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1251/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1252/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1253/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1254/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1255/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1256/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1257/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1258/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1259/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1260/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1261/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1262/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1263/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1264/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1265/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1266/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1267/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1268/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1269/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1270/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1271/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1272/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1273/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1274/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1275/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1276/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1277/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1278/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1279/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1280/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1281/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1282/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1283/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1284/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1285/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1286/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1287/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1288/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1289/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1290/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1291/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1292/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1293/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1294/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1295/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1296/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1297/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1298/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1299/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1300/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1301/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1302/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1303/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1304/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1305/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1306/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1307/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1308/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1309/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1310/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1311/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1312/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1313/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1314/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1315/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1316/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1317/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1318/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1319/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1320/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1321/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1322/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1323/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1324/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1325/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1326/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1327/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1328/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1329/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1330/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1331/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1332/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1333/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1334/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1335/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1336/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1337/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1338/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1339/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1340/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1341/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1342/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1343/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1344/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1345/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1346/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1347/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1348/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1349/5000], Step [201/1454], Loss: 0.1727\n",
            "Epoch [1350/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1351/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1352/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1353/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1354/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1355/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1356/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1357/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1358/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1359/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1360/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1361/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1362/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1363/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1364/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1365/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1366/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1367/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1368/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1369/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1370/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1371/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [1372/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1373/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1374/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1375/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1376/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1377/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1378/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1379/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1380/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1381/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1382/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1383/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1384/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1385/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1386/5000], Step [201/1454], Loss: 0.9671\n",
            "Epoch [1387/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1388/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1389/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1390/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1391/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1392/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1393/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1394/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1395/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1396/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1397/5000], Step [201/1454], Loss: 0.9325\n",
            "Epoch [1398/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1399/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1400/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1401/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1402/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1403/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1404/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1405/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1406/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1407/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1408/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1409/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1410/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1411/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1412/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1413/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1414/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1415/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1416/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1417/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [1418/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1419/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1420/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1421/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1422/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1423/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1424/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1425/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1426/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1427/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1428/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1429/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1430/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1431/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1432/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1433/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1434/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [1435/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1436/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1437/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1438/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1439/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1440/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1441/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1442/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1443/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1444/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1445/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1446/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1447/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1448/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1449/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1450/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [1451/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1452/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1453/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1454/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1455/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1456/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1457/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1458/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1459/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1460/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1461/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1462/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1463/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1464/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1465/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1466/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1467/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1468/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1469/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1470/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1471/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1472/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1473/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1474/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1475/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1476/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1477/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1478/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1479/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [1480/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1481/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1482/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1483/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1484/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1485/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1486/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1487/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1488/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1489/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1490/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1491/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1492/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1493/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1494/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1495/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1496/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1497/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1498/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1499/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1500/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1501/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1502/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [1503/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1504/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1505/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1506/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1507/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1508/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1509/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1510/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1511/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1512/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1513/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1514/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1515/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1516/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1517/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1518/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1519/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1520/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1521/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1522/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1523/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1524/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1525/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1526/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1527/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1528/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1529/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1530/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1531/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1532/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [1533/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1534/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1535/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1536/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1537/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1538/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1539/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1540/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1541/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1542/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1543/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1544/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1545/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1546/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1547/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1548/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1549/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1550/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1551/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1552/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1553/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1554/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1555/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1556/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1557/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1558/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1559/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1560/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1561/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1562/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1563/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1564/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1565/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1566/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1567/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1568/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1569/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1570/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1571/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1572/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1573/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1574/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1575/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1576/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1577/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1578/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1579/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1580/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1581/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1582/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1583/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1584/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1585/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1586/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1587/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1588/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1589/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1590/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1591/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1592/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1593/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1594/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1595/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1596/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1597/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1598/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1599/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1600/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1601/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1602/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1603/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1604/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1605/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1606/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1607/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1608/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1609/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1610/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1611/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1612/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1613/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1614/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1615/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1616/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1617/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1618/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1619/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1620/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1621/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1622/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1623/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1624/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1625/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1626/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1627/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1628/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1629/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1630/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1631/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1632/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1633/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1634/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1635/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1636/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1637/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1638/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1639/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1640/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1641/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1642/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1643/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1644/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1645/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1646/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1647/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1648/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1649/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1650/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1651/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1652/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1653/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1654/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1655/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1656/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1657/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1658/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1659/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1660/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1661/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1662/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1663/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1664/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1665/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1666/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1667/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1668/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1669/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1670/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1671/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1672/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1673/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1674/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1675/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1676/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1677/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1678/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1679/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1680/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1681/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1682/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1683/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1684/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1685/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1686/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1687/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1688/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1689/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1690/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1691/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1692/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1693/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1694/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1695/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1696/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1697/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1698/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1699/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1700/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1701/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1702/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1703/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1704/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1705/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1706/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1707/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1708/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1709/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1710/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1711/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1712/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1713/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1714/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1715/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1716/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1717/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1718/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1719/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1720/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1721/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1722/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1723/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1724/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1725/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1726/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1727/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1728/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1729/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1730/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1731/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1732/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1733/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1734/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [1735/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1736/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1737/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1738/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1739/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1740/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1741/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1742/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1743/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1744/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1745/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1746/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1747/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1748/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1749/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1750/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1751/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1752/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1753/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1754/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1755/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1756/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1757/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1758/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1759/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [1760/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1761/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1762/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1763/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1764/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1765/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1766/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1767/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1768/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1769/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1770/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1771/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1772/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1773/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1774/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1775/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1776/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1777/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1778/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1779/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1780/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1781/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1782/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1783/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1784/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1785/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1786/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1787/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1788/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1789/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1790/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1791/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1792/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1793/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1794/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1795/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1796/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1797/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1798/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [1799/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1800/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1801/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1802/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1803/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1804/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1805/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1806/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1807/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1808/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1809/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1810/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [1811/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1812/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1813/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1814/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1815/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1816/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1817/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1818/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1819/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1820/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1821/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1822/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1823/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1824/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1825/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1826/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1827/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1828/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1829/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1830/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1831/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1832/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1833/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1834/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1835/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1836/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1837/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1838/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1839/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1840/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1841/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1842/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1843/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1844/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1845/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1846/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1847/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1848/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1849/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1850/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1851/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1852/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1853/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1854/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1855/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1856/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1857/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1858/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1859/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1860/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1861/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1862/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1863/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1864/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1865/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1866/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1867/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1868/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1869/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1870/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1871/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1872/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1873/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1874/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1875/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1876/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1877/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1878/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1879/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1880/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1881/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1882/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1883/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1884/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1885/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1886/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1887/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1888/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1889/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1890/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1891/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1892/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1893/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1894/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1895/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1896/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1897/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1898/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1899/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1900/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1901/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1902/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1903/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1904/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1905/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [1906/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1907/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1908/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1909/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1910/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1911/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1912/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1913/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1914/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1915/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1916/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [1917/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1918/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1919/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1920/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1921/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1922/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1923/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1924/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1925/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1926/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1927/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1928/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1929/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1930/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1931/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1932/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1933/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1934/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1935/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1936/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1937/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1938/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1939/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1940/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1941/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [1942/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1943/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1944/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1945/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1946/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1947/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1948/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1949/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1950/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1951/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1952/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1953/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1954/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1955/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1956/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1957/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1958/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1959/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [1960/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1961/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [1962/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1963/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1964/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1965/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1966/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1967/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [1968/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1969/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1970/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1971/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1972/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1973/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1974/5000], Step [201/1454], Loss: 0.1727\n",
            "Epoch [1975/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1976/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1977/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1978/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1979/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1980/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [1981/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1982/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1983/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [1984/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1985/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1986/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1987/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [1988/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [1989/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [1990/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [1991/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [1992/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1993/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1994/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [1995/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [1996/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [1997/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [1998/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [1999/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2000/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2001/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2002/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2003/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2004/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2005/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2006/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2007/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2008/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2009/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2010/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2011/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2012/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2013/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2014/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2015/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2016/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2017/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2018/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2019/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2020/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2021/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2022/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2023/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2024/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2025/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2026/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2027/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2028/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2029/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2030/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2031/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2032/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2033/5000], Step [201/1454], Loss: 1.0362\n",
            "Epoch [2034/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2035/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2036/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2037/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2038/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2039/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2040/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2041/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2042/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2043/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2044/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [2045/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2046/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2047/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2048/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2049/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2050/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2051/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2052/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2053/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2054/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2055/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2056/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2057/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2058/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2059/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2060/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2061/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2062/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2063/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2064/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2065/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2066/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2067/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2068/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2069/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2070/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2071/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2072/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2073/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2074/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2075/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2076/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2077/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2078/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2079/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2080/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2081/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2082/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2083/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2084/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2085/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2086/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2087/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2088/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2089/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2090/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2091/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2092/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2093/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2094/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2095/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2096/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2097/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2098/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2099/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2100/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2101/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2102/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2103/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2104/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2105/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [2106/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2107/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2108/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2109/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2110/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2111/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2112/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2113/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2114/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2115/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2116/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2117/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2118/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2119/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2120/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2121/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2122/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2123/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2124/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2125/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2126/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2127/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2128/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2129/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2130/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2131/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2132/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2133/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2134/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2135/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2136/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2137/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2138/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2139/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2140/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2141/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2142/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2143/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2144/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2145/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2146/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2147/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2148/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2149/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2150/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2151/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2152/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2153/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2154/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [2155/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2156/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2157/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2158/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2159/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2160/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2161/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2162/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2163/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2164/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2165/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2166/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2167/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2168/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2169/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2170/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2171/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2172/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2173/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2174/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2175/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2176/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2177/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2178/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2179/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2180/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2181/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2182/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2183/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2184/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2185/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2186/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2187/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2188/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2189/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2190/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2191/5000], Step [201/1454], Loss: 1.0016\n",
            "Epoch [2192/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2193/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2194/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2195/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2196/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2197/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2198/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2199/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2200/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2201/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2202/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2203/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2204/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2205/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2206/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2207/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2208/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2209/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2210/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2211/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2212/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2213/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2214/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2215/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2216/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2217/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2218/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2219/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2220/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2221/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2222/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2223/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2224/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2225/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2226/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2227/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2228/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2229/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2230/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2231/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2232/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2233/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2234/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2235/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2236/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2237/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2238/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2239/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2240/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2241/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2242/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2243/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2244/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2245/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2246/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2247/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2248/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2249/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2250/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2251/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2252/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2253/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2254/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2255/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2256/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2257/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2258/5000], Step [201/1454], Loss: 0.9671\n",
            "Epoch [2259/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2260/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2261/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2262/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2263/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2264/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2265/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2266/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2267/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2268/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2269/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2270/5000], Step [201/1454], Loss: 0.9325\n",
            "Epoch [2271/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2272/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2273/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2274/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2275/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2276/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2277/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2278/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2279/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2280/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2281/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2282/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2283/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2284/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2285/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2286/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2287/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2288/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2289/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2290/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2291/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2292/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2293/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2294/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2295/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2296/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2297/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2298/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2299/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2300/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2301/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2302/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2303/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2304/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2305/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2306/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2307/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2308/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2309/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2310/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2311/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2312/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2313/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2314/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2315/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2316/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2317/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2318/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2319/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2320/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2321/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2322/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2323/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2324/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2325/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2326/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2327/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2328/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2329/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2330/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2331/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2332/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2333/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2334/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2335/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2336/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2337/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2338/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2339/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2340/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2341/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2342/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2343/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2344/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2345/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2346/5000], Step [201/1454], Loss: 0.9325\n",
            "Epoch [2347/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2348/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2349/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2350/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2351/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2352/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2353/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2354/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2355/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2356/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2357/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2358/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2359/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2360/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2361/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2362/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2363/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2364/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2365/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2366/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2367/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2368/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2369/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2370/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2371/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2372/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2373/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2374/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2375/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2376/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2377/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2378/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2379/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2380/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2381/5000], Step [201/1454], Loss: 0.1727\n",
            "Epoch [2382/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2383/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2384/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2385/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2386/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2387/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2388/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2389/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2390/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2391/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2392/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2393/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2394/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2395/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2396/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2397/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2398/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2399/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2400/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2401/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2402/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2403/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2404/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2405/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2406/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2407/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2408/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2409/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2410/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2411/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2412/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2413/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2414/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [2415/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2416/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2417/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2418/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2419/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2420/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2421/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2422/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2423/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2424/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2425/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2426/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2427/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2428/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2429/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2430/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2431/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2432/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2433/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2434/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2435/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2436/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2437/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2438/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2439/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2440/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2441/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2442/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [2443/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2444/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2445/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2446/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2447/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2448/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2449/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2450/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2451/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2452/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2453/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2454/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2455/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2456/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2457/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2458/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2459/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2460/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2461/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2462/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2463/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2464/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2465/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2466/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2467/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2468/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2469/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2470/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2471/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2472/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2473/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2474/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2475/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2476/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2477/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2478/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2479/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2480/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2481/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2482/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2483/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2484/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2485/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2486/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2487/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2488/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2489/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2490/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2491/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2492/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2493/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2494/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2495/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2496/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2497/5000], Step [201/1454], Loss: 0.2072\n",
            "Epoch [2498/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2499/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2500/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2501/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2502/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2503/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2504/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2505/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2506/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2507/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2508/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2509/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2510/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2511/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2512/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2513/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2514/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2515/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2516/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2517/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2518/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2519/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2520/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2521/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2522/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2523/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2524/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2525/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2526/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2527/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2528/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2529/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2530/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2531/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2532/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2533/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2534/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2535/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2536/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2537/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2538/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2539/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2540/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2541/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2542/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2543/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2544/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2545/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2546/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2547/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2548/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2549/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2550/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2551/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2552/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2553/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2554/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2555/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2556/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2557/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2558/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2559/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2560/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2561/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2562/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2563/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2564/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2565/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2566/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2567/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2568/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2569/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2570/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2571/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2572/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2573/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2574/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2575/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2576/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2577/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2578/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2579/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2580/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2581/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2582/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2583/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2584/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2585/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2586/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2587/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2588/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2589/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2590/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2591/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2592/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2593/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2594/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2595/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2596/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2597/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2598/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2599/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2600/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2601/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2602/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2603/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2604/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2605/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2606/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2607/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2608/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2609/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2610/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2611/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2612/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2613/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2614/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2615/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2616/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2617/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2618/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2619/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2620/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2621/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2622/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2623/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2624/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2625/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2626/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2627/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2628/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2629/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2630/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2631/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2632/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2633/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2634/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2635/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2636/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2637/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2638/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2639/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2640/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2641/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2642/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2643/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2644/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2645/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2646/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2647/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2648/5000], Step [201/1454], Loss: 0.8289\n",
            "Epoch [2649/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2650/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2651/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2652/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2653/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2654/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2655/5000], Step [201/1454], Loss: 0.8980\n",
            "Epoch [2656/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2657/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2658/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2659/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2660/5000], Step [201/1454], Loss: 0.7944\n",
            "Epoch [2661/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2662/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2663/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2664/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2665/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2666/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2667/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2668/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2669/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2670/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2671/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2672/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2673/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2674/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2675/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2676/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2677/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2678/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2679/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2680/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2681/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2682/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2683/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2684/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2685/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2686/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2687/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [2688/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2689/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2690/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2691/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2692/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2693/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2694/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2695/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2696/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2697/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2698/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2699/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2700/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2701/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2702/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2703/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2704/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2705/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2706/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2707/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2708/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2709/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2710/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2711/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2712/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2713/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2714/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2715/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2716/5000], Step [201/1454], Loss: 0.2418\n",
            "Epoch [2717/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2718/5000], Step [201/1454], Loss: 0.1727\n",
            "Epoch [2719/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2720/5000], Step [201/1454], Loss: 0.8635\n",
            "Epoch [2721/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2722/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2723/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2724/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2725/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2726/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2727/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2728/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2729/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2730/5000], Step [201/1454], Loss: 0.7253\n",
            "Epoch [2731/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2732/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2733/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2734/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2735/5000], Step [201/1454], Loss: 0.3799\n",
            "Epoch [2736/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2737/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2738/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2739/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2740/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2741/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2742/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2743/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2744/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2745/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2746/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2747/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2748/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2749/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2750/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2751/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2752/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2753/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2754/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2755/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2756/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2757/5000], Step [201/1454], Loss: 0.5526\n",
            "Epoch [2758/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2759/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2760/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2761/5000], Step [201/1454], Loss: 0.6562\n",
            "Epoch [2762/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2763/5000], Step [201/1454], Loss: 0.3108\n",
            "Epoch [2764/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2765/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2766/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2767/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2768/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2769/5000], Step [201/1454], Loss: 0.6908\n",
            "Epoch [2770/5000], Step [201/1454], Loss: 0.4490\n",
            "Epoch [2771/5000], Step [201/1454], Loss: 0.3454\n",
            "Epoch [2772/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2773/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2774/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2775/5000], Step [201/1454], Loss: 0.5872\n",
            "Epoch [2776/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2777/5000], Step [201/1454], Loss: 0.4835\n",
            "Epoch [2778/5000], Step [201/1454], Loss: 0.4145\n",
            "Epoch [2779/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2780/5000], Step [201/1454], Loss: 0.5181\n",
            "Epoch [2781/5000], Step [201/1454], Loss: 0.7599\n",
            "Epoch [2782/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2783/5000], Step [201/1454], Loss: 0.6217\n",
            "Epoch [2784/5000], Step [201/1454], Loss: 0.9325\n",
            "Epoch [2785/5000], Step [201/1454], Loss: 0.2763\n",
            "Epoch [2786/5000], Step [201/1454], Loss: 0.4145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ad99a0c20066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLmSNWE5Cpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "5a86bbb9-1fd9-4a4d-eade-6d99805fcafc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temp=0\n",
        "batch=20\n",
        "avg_loss_values=[]\n",
        "for i,loss in enumerate(loss_values):\n",
        "  temp+=loss\n",
        "  if i%batch==0:\n",
        "    avg_loss_values.append(temp/batch)\n",
        "    temp=0\n",
        "   \n",
        "plt.plot(avg_loss_values)\n",
        "plt.ylim((0,0.3))\n",
        "plt.show()    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwSnTMo/zVdOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYJvQSZBnVyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.modules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAAvkWGhto3Z",
        "colab_type": "text"
      },
      "source": [
        "##Setting up Test Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caESXMB9tohO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "label_encoders={}\n",
        "for cat_col in categorical_features:\n",
        "  label_encoders[cat_col]=LabelEncoder()\n",
        "  val_data[cat_col]=label_encoders[cat_col].fit_transform(val_data[cat_col])\n",
        "  \n",
        "val_data=val_data.drop(\"Weights\",axis=1)\n",
        "val_data_org=val_data\n",
        "val_data=val_data.drop(\"id\",axis=1)\n",
        "\n",
        "val_dataset=TabularDataset2(data=val_data,cat_cols=categorical_features,\n",
        "                      output_col=output_feature)\n",
        "val_dataset[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTzTD-9117i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=1454\n",
        "val_dataloader=DataLoader(val_dataset,batchsize,shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOWrIfF4FvW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e6ba7f31-496b-45dd-dea6-37b9d1a975d6"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "prec_values,rec_values,f1_values=[],[],[]\n",
        "\n",
        "with torch.no_grad():\n",
        "  loss,i=0,0\n",
        "  \n",
        "  for y, cont_x, cat_x in val_dataloader:\n",
        "    cat_x = cat_x.to(device)\n",
        "    cont_x = cont_x.to(device)\n",
        "    y  = y.to(device)\n",
        "    preds=model(cont_x, cat_x)\n",
        "    #preds=torch.round(preds)\n",
        "#     print(y[:5])\n",
        "    loss+=criterion(preds,y)\n",
        "    i+=1\n",
        "    y = y.cpu()\n",
        "    preds = preds.cpu()\n",
        "    precision,recall,fscore,_=precision_recall_fscore_support(y, torch.round(preds), average='macro')\n",
        "    prec_values.append(precision)\n",
        "    rec_values.append(recall)\n",
        "    f1_values.append(fscore)\n",
        "\n",
        "  print('Loss of the model: {} '.format( loss / i))\n",
        "torch.save(model.state_dict(),'model.ckpt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss of the model: 10.413891792297363 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhyWTIrI7ngb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "# import scikitplot as skplt\n",
        "# y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
        "# y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
        "\n",
        "# print (y)\n",
        "# print (preds)\n",
        "\n",
        "y_true = y.tolist()\n",
        "y_pred = torch.round(preds).tolist()\n",
        "print (precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
        "\n",
        "print (accuracy_score(y_true, y_pred))\n",
        "print (\"Accuracy:\",accuracy_score(y_true, y_pred, normalize=False))\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "# skplt.metrics.plot_roc_curve(y_true, y_pred)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}