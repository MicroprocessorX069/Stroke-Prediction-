{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WideAndDeep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Stroke-Prediction-/blob/master/WideAndDeep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFbxYJCP7mmT",
        "colab_type": "code",
        "outputId": "5f6b7f11-9164-4d91-b48d-29d39f8515fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2alqmYSJA_gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# from classes3 import  FeedForwardNN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from datetime import datetime\n",
        "# import visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmRrCmkDdcm-",
        "colab_type": "code",
        "outputId": "903121bf-f007-4398-a278-f67888b0c5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path=\"/content/drive/My Drive/Projects/StrokePrediction/data/healthcare-dataset-stroke-data\"\n",
        "data= pd.read_csv(os.path.join(path,\"train_2v.csv\")).dropna()\n",
        "data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29072, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWGDWESVJgwR",
        "colab_type": "code",
        "outputId": "d55cf755-04db-4352-98cd-b87acff6f957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['Weights'] = np.where(data['stroke'] == 1, 0.9, .004)\n",
        "data['Weights'].unique()\n",
        "\n",
        "val_data=data.sample(frac=0.05, random_state=123, weights='Weights')\n",
        "#train_data = data.loc[~data.index.isin(val_data.index)]\n",
        "print(len(np.where(val_data['stroke']==1)[0]),len(np.where(val_data['stroke']==0)[0]))\n",
        "data_org=data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "548 906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKUo_c4vIkJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_path=\"/content/drive/My Drive/Projects/StrokePrediction/data/healthcare-dataset-stroke-data\"\n",
        "# test_data= pd.read_csv(os.path.join(path,\"test.csv\")).dropna()\n",
        "# test_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j22PLGFaNdym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class TabularDataset2(Dataset):\n",
        "  def __init__(self, data, cat_cols=None, output_col=None):\n",
        "    \"\"\"\n",
        "    Characterizes a Dataset for PyTorch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    data: pandas data frame\n",
        "      The data frame object for the input data. It must\n",
        "      contain all the continuous, categorical and the\n",
        "      output columns to be used.\n",
        "\n",
        "    cat_cols: List of strings\n",
        "      The names of the categorical columns in the data.\n",
        "      These columns will be passed through the embedding\n",
        "      layers in the model. These columns must be\n",
        "      label encoded beforehand. \n",
        "\n",
        "    output_col: string\n",
        "      The name of the output variable column in the data\n",
        "      provided.\n",
        "    \"\"\"\n",
        "\n",
        "    self.n = data.shape[0]\n",
        "\n",
        "    if output_col:\n",
        "      self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
        "    else:\n",
        "      self.y =  np.zeros((self.n, 1))\n",
        "\n",
        "    self.cat_cols = cat_cols if cat_cols else []\n",
        "    self.cont_cols = [col for col in data.columns\n",
        "                      if col not in self.cat_cols + [output_col]]\n",
        "\n",
        "    if self.cont_cols:\n",
        "      self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
        "    else:\n",
        "      self.cont_X = np.zeros((self.n, 1))\n",
        "\n",
        "    if self.cat_cols:\n",
        "      self.cat_X = data[cat_cols].astype(np.int64).values\n",
        "    else:\n",
        "      self.cat_X =  np.zeros((self.n, 1))\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Denotes the total number of samples.\n",
        "    \"\"\"\n",
        "    return self.n\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Generates one sample of data.\n",
        "    \"\"\"\n",
        "    return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self,emb_dims,no_of_cont, lin_layer_sizes,\n",
        "              output_size,emb_dropout, lin_layer_dropouts):\n",
        "    '''\n",
        "    #emb_dims: list of two tuples\n",
        "    #tuple1: no. of unqie values for that categorical variable\n",
        "    #tuple 2: shape of that features data\n",
        "    \n",
        "    #no_of_cont: number of continuous features\n",
        "    \n",
        "    lin_layer_sizes: list of integers\n",
        "    no. of nodes in each linear layer in the network\n",
        "    \n",
        "    output_size=Integer\n",
        "    size of final output\n",
        "    \n",
        "    emb_dropout: float\n",
        "    dropout used after embedding layers\n",
        "    \n",
        "    lin_layer_dropouts: \n",
        "    dropout after each linear layer\n",
        "    \n",
        "    '''\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding layers\n",
        "    self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
        "                                     for x, y in emb_dims])\n",
        "\n",
        "    no_of_embs = sum([y for x, y in emb_dims])\n",
        "    self.no_of_embs = no_of_embs\n",
        "    self.no_of_cont = no_of_cont\n",
        "\n",
        "    # Linear Layers\n",
        "    first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
        "                                lin_layer_sizes[0])\n",
        "\n",
        "    self.lin_layers =\\\n",
        "     nn.ModuleList([first_lin_layer] +\\\n",
        "          [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
        "           for i in range(len(lin_layer_sizes) - 1)])\n",
        "    \n",
        "    for lin_layer in self.lin_layers:\n",
        "      nn.init.kaiming_normal_(lin_layer.weight.data)\n",
        "\n",
        "    # Output Layer\n",
        "    self.output_layer = nn.Sequential(nn.Linear(30,\n",
        "                                  output_size),nn.Sigmoid())\n",
        "    #nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
        "\n",
        "    # Batch Norm Layers\n",
        "    self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
        "    self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
        "                                    for size in lin_layer_sizes])\n",
        "\n",
        "    # Dropout Layers\n",
        "    self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
        "    self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
        "                                  for size in lin_layer_dropouts])\n",
        "\n",
        "  def forward(self,cont_data,cat_data):\n",
        "    if self.no_of_embs != 0:\n",
        "      x = [emb_layer(cat_data[:, i])\n",
        "           for i,emb_layer in enumerate(self.emb_layers)]\n",
        "      x = torch.cat(x, 1)\n",
        "      x = self.emb_dropout_layer(x)\n",
        "      \n",
        "\n",
        "    if self.no_of_cont != 0:\n",
        "      normalized_cont_data = self.first_bn_layer(cont_data)\n",
        "\n",
        "      if self.no_of_embs != 0:\n",
        "        x = torch.cat([x, normalized_cont_data], 1) \n",
        "        y=x\n",
        "      else:\n",
        "        x = normalized_cont_data\n",
        "        y=x\n",
        "        \n",
        "    for lin_layer, dropout_layer, bn_layer in\\\n",
        "        zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
        "      \n",
        "      x = F.relu(lin_layer(x))\n",
        "      x = bn_layer(x)\n",
        "      x = dropout_layer(x)\n",
        "    \n",
        "    x=torch.cat([x,y],1)\n",
        "    #print(x.size(),y.size())\n",
        "    x = self.output_layer(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "def weighted_binary_cross_entropy(output, target, weights=None):\n",
        "    output = torch.clamp(output,min=1e-8,max=1-1e-8)  \n",
        "    if weights is not None:\n",
        "        \n",
        "        loss = weights[1] * (target * torch.log(output)) + \\\n",
        "               weights[0] * ((1 - target) * torch.log(1 - output))\n",
        "    else:\n",
        "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
        "\n",
        "    return torch.neg(torch.mean(loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioPokgp98z-t",
        "colab_type": "text"
      },
      "source": [
        "##Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKaku-1X2mWM",
        "colab_type": "code",
        "outputId": "05f68998-70fd-4336-83f4-69c0ee169bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data.stroke.value_counts()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    28524\n",
              "1      548\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XPPcxl5hF9I",
        "colab_type": "text"
      },
      "source": [
        "##Features segregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-7p_V0hIfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features=[\"gender\",\"hypertension\",\"heart_disease\",\n",
        "                      \"ever_married\",\"work_type\",\"Residence_type\",\n",
        "                     \"smoking_status\"]\n",
        "output_feature=\"stroke\"\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders={}\n",
        "for cat_col in categorical_features:\n",
        "  label_encoders[cat_col]=LabelEncoder()\n",
        "  data[cat_col]=label_encoders[cat_col].fit_transform(data[cat_col])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2O4j3KsA7qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.drop(\"Weights\",axis=1)\n",
        "data_org=data\n",
        "data=data.drop(\"id\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G2jVY2giwvB",
        "colab_type": "text"
      },
      "source": [
        "##Creating object of class 'Tabular Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPGsEHSri4P0",
        "colab_type": "code",
        "outputId": "28829508-b508-4e98-d1a6-f174d77a16cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dataset=TabularDataset2(data=data,cat_cols=categorical_features,\n",
        "                      output_col=output_feature)\n",
        "dataset[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.], dtype=float32),\n",
              " array([58.  , 87.96, 39.2 ], dtype=float32),\n",
              " array([1, 1, 0, 1, 2, 1, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-5xFyF1Yty",
        "colab_type": "text"
      },
      "source": [
        "##Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXQP-wy1a1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=800\n",
        "dataloader=DataLoader(dataset,batchsize,shuffle=True,num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw-BYm0eK_vA",
        "colab_type": "text"
      },
      "source": [
        "##Getting the dimensions for embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ5xq_mLJ9sI",
        "colab_type": "code",
        "outputId": "e8f2875a-cdac-47ed-cc5c-66671ef53bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cat_dims=[int(data[col].nunique()) for col in categorical_features] #no of unique values for each categorical variable\n",
        "emb_dims=[(x,min(128,(x+1)//2)) for x in cat_dims] #reducing the embedding to half size.\n",
        "emb_dims"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 2), (2, 1), (2, 1), (2, 1), (5, 3), (2, 1), (3, 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ10_nIiK62z",
        "colab_type": "text"
      },
      "source": [
        "##Creating an instance of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqVKaSZRK_SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Hyper parameters\n",
        "no_of_cont=3 #no of continuous variables \n",
        "lin_layer_sizes=[64,32,16]\n",
        "output_size=1\n",
        "emb_dropout=0.04\n",
        "lin_layer_dropouts=[0.001,0.01,0.01]\n",
        "model= FeedForwardNN(emb_dims, no_of_cont, lin_layer_sizes,\n",
        "                    output_size, emb_dropout,\n",
        "                    lin_layer_dropouts).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMsF-kmVWsDc",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBT1MCuAWuOs",
        "colab_type": "code",
        "outputId": "2b580aff-67cd-4eea-d252-9534f7f584a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33422
        }
      },
      "source": [
        "# vis = Visualizations()\n",
        "\n",
        "no_of_epochs = 5000\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# criterion=nn.MSELoss()\n",
        "criterion=nn.BCELoss()\n",
        "class_weights=[0.5,1.2]\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.006)\n",
        "flag=False\n",
        "loss_values = []\n",
        "for epoch in range(no_of_epochs):\n",
        "      i=0\n",
        "      \n",
        "      for y, cont_x, cat_x in dataloader:\n",
        "        i+=1\n",
        "        cat_x = cat_x.to(device)\n",
        "        cont_x = cont_x.to(device)\n",
        "        y  = y.to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        preds = model(cont_x, cat_x)\n",
        "        if(torch.isnan(preds[0])):\n",
        "          print(\"Oh no\")\n",
        "          flag=True\n",
        "          break\n",
        "        \n",
        "        #loss = criterion(preds, y)\n",
        "        loss=weighted_binary_cross_entropy(preds,y)\n",
        "        \n",
        "        # Backward Pass and Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        if(i==1):\n",
        "          print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                .format(epoch+1, no_of_epochs, i+1, batchsize, loss.item()))\n",
        "          \n",
        "      if flag:\n",
        "        break\n",
        "      loss_values.append(loss)\n",
        "      \n",
        "      \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5000], Step [2/800], Loss: 0.4821\n",
            "Epoch [2/5000], Step [2/800], Loss: 0.0895\n",
            "Epoch [3/5000], Step [2/800], Loss: 0.0957\n",
            "Epoch [4/5000], Step [2/800], Loss: 0.0755\n",
            "Epoch [5/5000], Step [2/800], Loss: 0.0724\n",
            "Epoch [6/5000], Step [2/800], Loss: 0.0696\n",
            "Epoch [7/5000], Step [2/800], Loss: 0.0744\n",
            "Epoch [8/5000], Step [2/800], Loss: 0.1089\n",
            "Epoch [9/5000], Step [2/800], Loss: 0.0964\n",
            "Epoch [10/5000], Step [2/800], Loss: 0.0863\n",
            "Epoch [11/5000], Step [2/800], Loss: 0.0740\n",
            "Epoch [12/5000], Step [2/800], Loss: 0.0634\n",
            "Epoch [13/5000], Step [2/800], Loss: 0.0833\n",
            "Epoch [14/5000], Step [2/800], Loss: 0.0814\n",
            "Epoch [15/5000], Step [2/800], Loss: 0.0764\n",
            "Epoch [16/5000], Step [2/800], Loss: 0.0576\n",
            "Epoch [17/5000], Step [2/800], Loss: 0.1041\n",
            "Epoch [18/5000], Step [2/800], Loss: 0.0878\n",
            "Epoch [19/5000], Step [2/800], Loss: 0.0668\n",
            "Epoch [20/5000], Step [2/800], Loss: 0.0645\n",
            "Epoch [21/5000], Step [2/800], Loss: 0.0697\n",
            "Epoch [22/5000], Step [2/800], Loss: 0.0772\n",
            "Epoch [23/5000], Step [2/800], Loss: 0.0435\n",
            "Epoch [24/5000], Step [2/800], Loss: 0.0843\n",
            "Epoch [25/5000], Step [2/800], Loss: 0.1006\n",
            "Epoch [26/5000], Step [2/800], Loss: 0.0801\n",
            "Epoch [27/5000], Step [2/800], Loss: 0.0737\n",
            "Epoch [28/5000], Step [2/800], Loss: 0.0526\n",
            "Epoch [29/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [30/5000], Step [2/800], Loss: 0.0836\n",
            "Epoch [31/5000], Step [2/800], Loss: 0.0616\n",
            "Epoch [32/5000], Step [2/800], Loss: 0.0783\n",
            "Epoch [33/5000], Step [2/800], Loss: 0.0651\n",
            "Epoch [34/5000], Step [2/800], Loss: 0.0650\n",
            "Epoch [35/5000], Step [2/800], Loss: 0.1000\n",
            "Epoch [36/5000], Step [2/800], Loss: 0.0966\n",
            "Epoch [37/5000], Step [2/800], Loss: 0.0921\n",
            "Epoch [38/5000], Step [2/800], Loss: 0.0833\n",
            "Epoch [39/5000], Step [2/800], Loss: 0.0750\n",
            "Epoch [40/5000], Step [2/800], Loss: 0.0567\n",
            "Epoch [41/5000], Step [2/800], Loss: 0.0621\n",
            "Epoch [42/5000], Step [2/800], Loss: 0.0729\n",
            "Epoch [43/5000], Step [2/800], Loss: 0.0595\n",
            "Epoch [44/5000], Step [2/800], Loss: 0.1017\n",
            "Epoch [45/5000], Step [2/800], Loss: 0.0823\n",
            "Epoch [46/5000], Step [2/800], Loss: 0.0759\n",
            "Epoch [47/5000], Step [2/800], Loss: 0.0650\n",
            "Epoch [48/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [49/5000], Step [2/800], Loss: 0.0869\n",
            "Epoch [50/5000], Step [2/800], Loss: 0.0779\n",
            "Epoch [51/5000], Step [2/800], Loss: 0.0661\n",
            "Epoch [52/5000], Step [2/800], Loss: 0.0550\n",
            "Epoch [53/5000], Step [2/800], Loss: 0.0527\n",
            "Epoch [54/5000], Step [2/800], Loss: 0.0789\n",
            "Epoch [55/5000], Step [2/800], Loss: 0.0512\n",
            "Epoch [56/5000], Step [2/800], Loss: 0.0547\n",
            "Epoch [57/5000], Step [2/800], Loss: 0.0627\n",
            "Epoch [58/5000], Step [2/800], Loss: 0.0555\n",
            "Epoch [59/5000], Step [2/800], Loss: 0.0719\n",
            "Epoch [60/5000], Step [2/800], Loss: 0.0750\n",
            "Epoch [61/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [62/5000], Step [2/800], Loss: 0.0851\n",
            "Epoch [63/5000], Step [2/800], Loss: 0.0557\n",
            "Epoch [64/5000], Step [2/800], Loss: 0.0757\n",
            "Epoch [65/5000], Step [2/800], Loss: 0.0874\n",
            "Epoch [66/5000], Step [2/800], Loss: 0.0732\n",
            "Epoch [67/5000], Step [2/800], Loss: 0.0824\n",
            "Epoch [68/5000], Step [2/800], Loss: 0.0699\n",
            "Epoch [69/5000], Step [2/800], Loss: 0.0705\n",
            "Epoch [70/5000], Step [2/800], Loss: 0.0778\n",
            "Epoch [71/5000], Step [2/800], Loss: 0.0641\n",
            "Epoch [72/5000], Step [2/800], Loss: 0.0736\n",
            "Epoch [73/5000], Step [2/800], Loss: 0.0810\n",
            "Epoch [74/5000], Step [2/800], Loss: 0.0908\n",
            "Epoch [75/5000], Step [2/800], Loss: 0.0741\n",
            "Epoch [76/5000], Step [2/800], Loss: 0.0623\n",
            "Epoch [77/5000], Step [2/800], Loss: 0.0585\n",
            "Epoch [78/5000], Step [2/800], Loss: 0.1008\n",
            "Epoch [79/5000], Step [2/800], Loss: 0.0713\n",
            "Epoch [80/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [81/5000], Step [2/800], Loss: 0.0574\n",
            "Epoch [82/5000], Step [2/800], Loss: 0.0716\n",
            "Epoch [83/5000], Step [2/800], Loss: 0.0605\n",
            "Epoch [84/5000], Step [2/800], Loss: 0.0665\n",
            "Epoch [85/5000], Step [2/800], Loss: 0.0762\n",
            "Epoch [86/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [87/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [88/5000], Step [2/800], Loss: 0.0664\n",
            "Epoch [89/5000], Step [2/800], Loss: 0.0601\n",
            "Epoch [90/5000], Step [2/800], Loss: 0.0794\n",
            "Epoch [91/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [92/5000], Step [2/800], Loss: 0.0470\n",
            "Epoch [93/5000], Step [2/800], Loss: 0.0776\n",
            "Epoch [94/5000], Step [2/800], Loss: 0.0546\n",
            "Epoch [95/5000], Step [2/800], Loss: 0.0561\n",
            "Epoch [96/5000], Step [2/800], Loss: 0.0454\n",
            "Epoch [97/5000], Step [2/800], Loss: 0.0531\n",
            "Epoch [98/5000], Step [2/800], Loss: 0.0496\n",
            "Epoch [99/5000], Step [2/800], Loss: 0.0482\n",
            "Epoch [100/5000], Step [2/800], Loss: 0.0659\n",
            "Epoch [101/5000], Step [2/800], Loss: 0.0795\n",
            "Epoch [102/5000], Step [2/800], Loss: 0.0843\n",
            "Epoch [103/5000], Step [2/800], Loss: 0.0538\n",
            "Epoch [104/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [105/5000], Step [2/800], Loss: 0.0637\n",
            "Epoch [106/5000], Step [2/800], Loss: 0.0701\n",
            "Epoch [107/5000], Step [2/800], Loss: 0.0703\n",
            "Epoch [108/5000], Step [2/800], Loss: 0.0743\n",
            "Epoch [109/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [110/5000], Step [2/800], Loss: 0.0663\n",
            "Epoch [111/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [112/5000], Step [2/800], Loss: 0.0672\n",
            "Epoch [113/5000], Step [2/800], Loss: 0.0677\n",
            "Epoch [114/5000], Step [2/800], Loss: 0.0695\n",
            "Epoch [115/5000], Step [2/800], Loss: 0.0616\n",
            "Epoch [116/5000], Step [2/800], Loss: 0.0723\n",
            "Epoch [117/5000], Step [2/800], Loss: 0.0518\n",
            "Epoch [118/5000], Step [2/800], Loss: 0.0747\n",
            "Epoch [119/5000], Step [2/800], Loss: 0.0600\n",
            "Epoch [120/5000], Step [2/800], Loss: 0.0617\n",
            "Epoch [121/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [122/5000], Step [2/800], Loss: 0.0718\n",
            "Epoch [123/5000], Step [2/800], Loss: 0.0562\n",
            "Epoch [124/5000], Step [2/800], Loss: 0.0670\n",
            "Epoch [125/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [126/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [127/5000], Step [2/800], Loss: 0.0836\n",
            "Epoch [128/5000], Step [2/800], Loss: 0.0887\n",
            "Epoch [129/5000], Step [2/800], Loss: 0.0810\n",
            "Epoch [130/5000], Step [2/800], Loss: 0.0837\n",
            "Epoch [131/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [132/5000], Step [2/800], Loss: 0.0586\n",
            "Epoch [133/5000], Step [2/800], Loss: 0.0534\n",
            "Epoch [134/5000], Step [2/800], Loss: 0.0514\n",
            "Epoch [135/5000], Step [2/800], Loss: 0.0639\n",
            "Epoch [136/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [137/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [138/5000], Step [2/800], Loss: 0.0459\n",
            "Epoch [139/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [140/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [141/5000], Step [2/800], Loss: 0.0500\n",
            "Epoch [142/5000], Step [2/800], Loss: 0.0524\n",
            "Epoch [143/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [144/5000], Step [2/800], Loss: 0.0482\n",
            "Epoch [145/5000], Step [2/800], Loss: 0.0529\n",
            "Epoch [146/5000], Step [2/800], Loss: 0.0611\n",
            "Epoch [147/5000], Step [2/800], Loss: 0.0625\n",
            "Epoch [148/5000], Step [2/800], Loss: 0.0589\n",
            "Epoch [149/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [150/5000], Step [2/800], Loss: 0.0699\n",
            "Epoch [151/5000], Step [2/800], Loss: 0.0467\n",
            "Epoch [152/5000], Step [2/800], Loss: 0.0740\n",
            "Epoch [153/5000], Step [2/800], Loss: 0.0573\n",
            "Epoch [154/5000], Step [2/800], Loss: 0.0665\n",
            "Epoch [155/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [156/5000], Step [2/800], Loss: 0.0413\n",
            "Epoch [157/5000], Step [2/800], Loss: 0.0452\n",
            "Epoch [158/5000], Step [2/800], Loss: 0.0467\n",
            "Epoch [159/5000], Step [2/800], Loss: 0.0404\n",
            "Epoch [160/5000], Step [2/800], Loss: 0.0673\n",
            "Epoch [161/5000], Step [2/800], Loss: 0.0529\n",
            "Epoch [162/5000], Step [2/800], Loss: 0.0623\n",
            "Epoch [163/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [164/5000], Step [2/800], Loss: 0.0563\n",
            "Epoch [165/5000], Step [2/800], Loss: 0.0592\n",
            "Epoch [166/5000], Step [2/800], Loss: 0.0536\n",
            "Epoch [167/5000], Step [2/800], Loss: 0.0595\n",
            "Epoch [168/5000], Step [2/800], Loss: 0.0750\n",
            "Epoch [169/5000], Step [2/800], Loss: 0.0721\n",
            "Epoch [170/5000], Step [2/800], Loss: 0.0740\n",
            "Epoch [171/5000], Step [2/800], Loss: 0.0497\n",
            "Epoch [172/5000], Step [2/800], Loss: 0.0548\n",
            "Epoch [173/5000], Step [2/800], Loss: 0.0542\n",
            "Epoch [174/5000], Step [2/800], Loss: 0.0666\n",
            "Epoch [175/5000], Step [2/800], Loss: 0.0411\n",
            "Epoch [176/5000], Step [2/800], Loss: 0.0641\n",
            "Epoch [177/5000], Step [2/800], Loss: 0.0556\n",
            "Epoch [178/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [179/5000], Step [2/800], Loss: 0.0498\n",
            "Epoch [180/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [181/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [182/5000], Step [2/800], Loss: 0.0546\n",
            "Epoch [183/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [184/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [185/5000], Step [2/800], Loss: 0.0649\n",
            "Epoch [186/5000], Step [2/800], Loss: 0.0753\n",
            "Epoch [187/5000], Step [2/800], Loss: 0.0451\n",
            "Epoch [188/5000], Step [2/800], Loss: 0.0408\n",
            "Epoch [189/5000], Step [2/800], Loss: 0.0703\n",
            "Epoch [190/5000], Step [2/800], Loss: 0.0554\n",
            "Epoch [191/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [192/5000], Step [2/800], Loss: 0.0377\n",
            "Epoch [193/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [194/5000], Step [2/800], Loss: 0.0383\n",
            "Epoch [195/5000], Step [2/800], Loss: 0.0583\n",
            "Epoch [196/5000], Step [2/800], Loss: 0.0562\n",
            "Epoch [197/5000], Step [2/800], Loss: 0.0505\n",
            "Epoch [198/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [199/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [200/5000], Step [2/800], Loss: 0.0473\n",
            "Epoch [201/5000], Step [2/800], Loss: 0.0453\n",
            "Epoch [202/5000], Step [2/800], Loss: 0.0689\n",
            "Epoch [203/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [204/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [205/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [206/5000], Step [2/800], Loss: 0.0635\n",
            "Epoch [207/5000], Step [2/800], Loss: 0.0639\n",
            "Epoch [208/5000], Step [2/800], Loss: 0.0484\n",
            "Epoch [209/5000], Step [2/800], Loss: 0.0658\n",
            "Epoch [210/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [211/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [212/5000], Step [2/800], Loss: 0.0391\n",
            "Epoch [213/5000], Step [2/800], Loss: 0.0475\n",
            "Epoch [214/5000], Step [2/800], Loss: 0.0674\n",
            "Epoch [215/5000], Step [2/800], Loss: 0.0526\n",
            "Epoch [216/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [217/5000], Step [2/800], Loss: 0.0552\n",
            "Epoch [218/5000], Step [2/800], Loss: 0.0487\n",
            "Epoch [219/5000], Step [2/800], Loss: 0.0599\n",
            "Epoch [220/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [221/5000], Step [2/800], Loss: 0.0608\n",
            "Epoch [222/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [223/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [224/5000], Step [2/800], Loss: 0.0320\n",
            "Epoch [225/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [226/5000], Step [2/800], Loss: 0.0662\n",
            "Epoch [227/5000], Step [2/800], Loss: 0.0404\n",
            "Epoch [228/5000], Step [2/800], Loss: 0.0610\n",
            "Epoch [229/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [230/5000], Step [2/800], Loss: 0.0354\n",
            "Epoch [231/5000], Step [2/800], Loss: 0.0490\n",
            "Epoch [232/5000], Step [2/800], Loss: 0.0790\n",
            "Epoch [233/5000], Step [2/800], Loss: 0.0482\n",
            "Epoch [234/5000], Step [2/800], Loss: 0.0609\n",
            "Epoch [235/5000], Step [2/800], Loss: 0.0602\n",
            "Epoch [236/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [237/5000], Step [2/800], Loss: 0.0435\n",
            "Epoch [238/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [239/5000], Step [2/800], Loss: 0.0578\n",
            "Epoch [240/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [241/5000], Step [2/800], Loss: 0.0407\n",
            "Epoch [242/5000], Step [2/800], Loss: 0.0631\n",
            "Epoch [243/5000], Step [2/800], Loss: 0.0351\n",
            "Epoch [244/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [245/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [246/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [247/5000], Step [2/800], Loss: 0.0629\n",
            "Epoch [248/5000], Step [2/800], Loss: 0.0554\n",
            "Epoch [249/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [250/5000], Step [2/800], Loss: 0.0548\n",
            "Epoch [251/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [252/5000], Step [2/800], Loss: 0.0558\n",
            "Epoch [253/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [254/5000], Step [2/800], Loss: 0.0402\n",
            "Epoch [255/5000], Step [2/800], Loss: 0.0324\n",
            "Epoch [256/5000], Step [2/800], Loss: 0.0482\n",
            "Epoch [257/5000], Step [2/800], Loss: 0.0449\n",
            "Epoch [258/5000], Step [2/800], Loss: 0.0456\n",
            "Epoch [259/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [260/5000], Step [2/800], Loss: 0.0408\n",
            "Epoch [261/5000], Step [2/800], Loss: 0.0488\n",
            "Epoch [262/5000], Step [2/800], Loss: 0.0498\n",
            "Epoch [263/5000], Step [2/800], Loss: 0.0460\n",
            "Epoch [264/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [265/5000], Step [2/800], Loss: 0.0508\n",
            "Epoch [266/5000], Step [2/800], Loss: 0.0463\n",
            "Epoch [267/5000], Step [2/800], Loss: 0.0454\n",
            "Epoch [268/5000], Step [2/800], Loss: 0.0533\n",
            "Epoch [269/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [270/5000], Step [2/800], Loss: 0.0596\n",
            "Epoch [271/5000], Step [2/800], Loss: 0.0415\n",
            "Epoch [272/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [273/5000], Step [2/800], Loss: 0.0491\n",
            "Epoch [274/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [275/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [276/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [277/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [278/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [279/5000], Step [2/800], Loss: 0.0547\n",
            "Epoch [280/5000], Step [2/800], Loss: 0.0807\n",
            "Epoch [281/5000], Step [2/800], Loss: 0.0474\n",
            "Epoch [282/5000], Step [2/800], Loss: 0.0629\n",
            "Epoch [283/5000], Step [2/800], Loss: 0.0587\n",
            "Epoch [284/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [285/5000], Step [2/800], Loss: 0.0598\n",
            "Epoch [286/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [287/5000], Step [2/800], Loss: 0.0479\n",
            "Epoch [288/5000], Step [2/800], Loss: 0.0493\n",
            "Epoch [289/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [290/5000], Step [2/800], Loss: 0.0483\n",
            "Epoch [291/5000], Step [2/800], Loss: 0.0684\n",
            "Epoch [292/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [293/5000], Step [2/800], Loss: 0.0453\n",
            "Epoch [294/5000], Step [2/800], Loss: 0.0616\n",
            "Epoch [295/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [296/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [297/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [298/5000], Step [2/800], Loss: 0.0686\n",
            "Epoch [299/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [300/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [301/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [302/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [303/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [304/5000], Step [2/800], Loss: 0.0457\n",
            "Epoch [305/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [306/5000], Step [2/800], Loss: 0.0401\n",
            "Epoch [307/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [308/5000], Step [2/800], Loss: 0.0487\n",
            "Epoch [309/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [310/5000], Step [2/800], Loss: 0.0307\n",
            "Epoch [311/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [312/5000], Step [2/800], Loss: 0.0522\n",
            "Epoch [313/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [314/5000], Step [2/800], Loss: 0.0535\n",
            "Epoch [315/5000], Step [2/800], Loss: 0.0716\n",
            "Epoch [316/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [317/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [318/5000], Step [2/800], Loss: 0.0518\n",
            "Epoch [319/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [320/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [321/5000], Step [2/800], Loss: 0.0408\n",
            "Epoch [322/5000], Step [2/800], Loss: 0.0496\n",
            "Epoch [323/5000], Step [2/800], Loss: 0.0692\n",
            "Epoch [324/5000], Step [2/800], Loss: 0.0480\n",
            "Epoch [325/5000], Step [2/800], Loss: 0.0555\n",
            "Epoch [326/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [327/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [328/5000], Step [2/800], Loss: 0.0665\n",
            "Epoch [329/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [330/5000], Step [2/800], Loss: 0.0339\n",
            "Epoch [331/5000], Step [2/800], Loss: 0.0445\n",
            "Epoch [332/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [333/5000], Step [2/800], Loss: 0.0712\n",
            "Epoch [334/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [335/5000], Step [2/800], Loss: 0.0800\n",
            "Epoch [336/5000], Step [2/800], Loss: 0.0474\n",
            "Epoch [337/5000], Step [2/800], Loss: 0.0563\n",
            "Epoch [338/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [339/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [340/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [341/5000], Step [2/800], Loss: 0.0552\n",
            "Epoch [342/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [343/5000], Step [2/800], Loss: 0.0450\n",
            "Epoch [344/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [345/5000], Step [2/800], Loss: 0.0452\n",
            "Epoch [346/5000], Step [2/800], Loss: 0.0574\n",
            "Epoch [347/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [348/5000], Step [2/800], Loss: 0.0354\n",
            "Epoch [349/5000], Step [2/800], Loss: 0.0483\n",
            "Epoch [350/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [351/5000], Step [2/800], Loss: 0.0419\n",
            "Epoch [352/5000], Step [2/800], Loss: 0.0337\n",
            "Epoch [353/5000], Step [2/800], Loss: 0.0530\n",
            "Epoch [354/5000], Step [2/800], Loss: 0.0630\n",
            "Epoch [355/5000], Step [2/800], Loss: 0.0416\n",
            "Epoch [356/5000], Step [2/800], Loss: 0.0631\n",
            "Epoch [357/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [358/5000], Step [2/800], Loss: 0.0589\n",
            "Epoch [359/5000], Step [2/800], Loss: 0.0545\n",
            "Epoch [360/5000], Step [2/800], Loss: 0.0338\n",
            "Epoch [361/5000], Step [2/800], Loss: 0.0570\n",
            "Epoch [362/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [363/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [364/5000], Step [2/800], Loss: 0.0339\n",
            "Epoch [365/5000], Step [2/800], Loss: 0.0493\n",
            "Epoch [366/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [367/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [368/5000], Step [2/800], Loss: 0.0412\n",
            "Epoch [369/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [370/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [371/5000], Step [2/800], Loss: 0.0524\n",
            "Epoch [372/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [373/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [374/5000], Step [2/800], Loss: 0.0240\n",
            "Epoch [375/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [376/5000], Step [2/800], Loss: 0.0566\n",
            "Epoch [377/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [378/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [379/5000], Step [2/800], Loss: 0.0528\n",
            "Epoch [380/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [381/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [382/5000], Step [2/800], Loss: 0.0545\n",
            "Epoch [383/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [384/5000], Step [2/800], Loss: 0.0366\n",
            "Epoch [385/5000], Step [2/800], Loss: 0.0287\n",
            "Epoch [386/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [387/5000], Step [2/800], Loss: 0.0543\n",
            "Epoch [388/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [389/5000], Step [2/800], Loss: 0.0519\n",
            "Epoch [390/5000], Step [2/800], Loss: 0.0542\n",
            "Epoch [391/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [392/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [393/5000], Step [2/800], Loss: 0.0212\n",
            "Epoch [394/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [395/5000], Step [2/800], Loss: 0.0579\n",
            "Epoch [396/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [397/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [398/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [399/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [400/5000], Step [2/800], Loss: 0.0676\n",
            "Epoch [401/5000], Step [2/800], Loss: 0.0425\n",
            "Epoch [402/5000], Step [2/800], Loss: 0.0457\n",
            "Epoch [403/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [404/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [405/5000], Step [2/800], Loss: 0.0351\n",
            "Epoch [406/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [407/5000], Step [2/800], Loss: 0.0622\n",
            "Epoch [408/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [409/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [410/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [411/5000], Step [2/800], Loss: 0.0236\n",
            "Epoch [412/5000], Step [2/800], Loss: 0.0530\n",
            "Epoch [413/5000], Step [2/800], Loss: 0.0599\n",
            "Epoch [414/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [415/5000], Step [2/800], Loss: 0.0707\n",
            "Epoch [416/5000], Step [2/800], Loss: 0.0510\n",
            "Epoch [417/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [418/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [419/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [420/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [421/5000], Step [2/800], Loss: 0.0709\n",
            "Epoch [422/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [423/5000], Step [2/800], Loss: 0.0475\n",
            "Epoch [424/5000], Step [2/800], Loss: 0.0663\n",
            "Epoch [425/5000], Step [2/800], Loss: 0.0581\n",
            "Epoch [426/5000], Step [2/800], Loss: 0.0414\n",
            "Epoch [427/5000], Step [2/800], Loss: 0.0460\n",
            "Epoch [428/5000], Step [2/800], Loss: 0.0554\n",
            "Epoch [429/5000], Step [2/800], Loss: 0.0647\n",
            "Epoch [430/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [431/5000], Step [2/800], Loss: 0.0373\n",
            "Epoch [432/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [433/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [434/5000], Step [2/800], Loss: 0.0535\n",
            "Epoch [435/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [436/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [437/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [438/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [439/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [440/5000], Step [2/800], Loss: 0.0219\n",
            "Epoch [441/5000], Step [2/800], Loss: 0.0513\n",
            "Epoch [442/5000], Step [2/800], Loss: 0.0478\n",
            "Epoch [443/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [444/5000], Step [2/800], Loss: 0.0691\n",
            "Epoch [445/5000], Step [2/800], Loss: 0.0383\n",
            "Epoch [446/5000], Step [2/800], Loss: 0.0451\n",
            "Epoch [447/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [448/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [449/5000], Step [2/800], Loss: 0.0470\n",
            "Epoch [450/5000], Step [2/800], Loss: 0.0499\n",
            "Epoch [451/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [452/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [453/5000], Step [2/800], Loss: 0.0550\n",
            "Epoch [454/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [455/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [456/5000], Step [2/800], Loss: 0.0466\n",
            "Epoch [457/5000], Step [2/800], Loss: 0.0487\n",
            "Epoch [458/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [459/5000], Step [2/800], Loss: 0.0515\n",
            "Epoch [460/5000], Step [2/800], Loss: 0.0649\n",
            "Epoch [461/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [462/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [463/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [464/5000], Step [2/800], Loss: 0.0622\n",
            "Epoch [465/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [466/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [467/5000], Step [2/800], Loss: 0.0715\n",
            "Epoch [468/5000], Step [2/800], Loss: 0.0471\n",
            "Epoch [469/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [470/5000], Step [2/800], Loss: 0.0722\n",
            "Epoch [471/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [472/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [473/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [474/5000], Step [2/800], Loss: 0.0324\n",
            "Epoch [475/5000], Step [2/800], Loss: 0.0524\n",
            "Epoch [476/5000], Step [2/800], Loss: 0.0516\n",
            "Epoch [477/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [478/5000], Step [2/800], Loss: 0.0416\n",
            "Epoch [479/5000], Step [2/800], Loss: 0.0450\n",
            "Epoch [480/5000], Step [2/800], Loss: 0.0244\n",
            "Epoch [481/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [482/5000], Step [2/800], Loss: 0.0416\n",
            "Epoch [483/5000], Step [2/800], Loss: 0.0466\n",
            "Epoch [484/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [485/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [486/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [487/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [488/5000], Step [2/800], Loss: 0.0564\n",
            "Epoch [489/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [490/5000], Step [2/800], Loss: 0.0305\n",
            "Epoch [491/5000], Step [2/800], Loss: 0.0452\n",
            "Epoch [492/5000], Step [2/800], Loss: 0.0593\n",
            "Epoch [493/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [494/5000], Step [2/800], Loss: 0.0271\n",
            "Epoch [495/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [496/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [497/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [498/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [499/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [500/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [501/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [502/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [503/5000], Step [2/800], Loss: 0.0324\n",
            "Epoch [504/5000], Step [2/800], Loss: 0.0535\n",
            "Epoch [505/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [506/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [507/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [508/5000], Step [2/800], Loss: 0.0523\n",
            "Epoch [509/5000], Step [2/800], Loss: 0.0488\n",
            "Epoch [510/5000], Step [2/800], Loss: 0.0300\n",
            "Epoch [511/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [512/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [513/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [514/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [515/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [516/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [517/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [518/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [519/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [520/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [521/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [522/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [523/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [524/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [525/5000], Step [2/800], Loss: 0.0432\n",
            "Epoch [526/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [527/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [528/5000], Step [2/800], Loss: 0.0470\n",
            "Epoch [529/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [530/5000], Step [2/800], Loss: 0.0780\n",
            "Epoch [531/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [532/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [533/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [534/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [535/5000], Step [2/800], Loss: 0.0499\n",
            "Epoch [536/5000], Step [2/800], Loss: 0.0445\n",
            "Epoch [537/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [538/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [539/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [540/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [541/5000], Step [2/800], Loss: 0.0308\n",
            "Epoch [542/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [543/5000], Step [2/800], Loss: 0.0308\n",
            "Epoch [544/5000], Step [2/800], Loss: 0.0408\n",
            "Epoch [545/5000], Step [2/800], Loss: 0.0300\n",
            "Epoch [546/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [547/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [548/5000], Step [2/800], Loss: 0.0474\n",
            "Epoch [549/5000], Step [2/800], Loss: 0.0509\n",
            "Epoch [550/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [551/5000], Step [2/800], Loss: 0.0375\n",
            "Epoch [552/5000], Step [2/800], Loss: 0.0536\n",
            "Epoch [553/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [554/5000], Step [2/800], Loss: 0.0466\n",
            "Epoch [555/5000], Step [2/800], Loss: 0.0529\n",
            "Epoch [556/5000], Step [2/800], Loss: 0.0229\n",
            "Epoch [557/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [558/5000], Step [2/800], Loss: 0.0487\n",
            "Epoch [559/5000], Step [2/800], Loss: 0.0317\n",
            "Epoch [560/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [561/5000], Step [2/800], Loss: 0.0591\n",
            "Epoch [562/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [563/5000], Step [2/800], Loss: 0.0506\n",
            "Epoch [564/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [565/5000], Step [2/800], Loss: 0.0341\n",
            "Epoch [566/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [567/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [568/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [569/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [570/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [571/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [572/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [573/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [574/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [575/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [576/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [577/5000], Step [2/800], Loss: 0.0550\n",
            "Epoch [578/5000], Step [2/800], Loss: 0.0328\n",
            "Epoch [579/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [580/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [581/5000], Step [2/800], Loss: 0.0505\n",
            "Epoch [582/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [583/5000], Step [2/800], Loss: 0.0537\n",
            "Epoch [584/5000], Step [2/800], Loss: 0.0254\n",
            "Epoch [585/5000], Step [2/800], Loss: 0.0512\n",
            "Epoch [586/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [587/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [588/5000], Step [2/800], Loss: 0.0411\n",
            "Epoch [589/5000], Step [2/800], Loss: 0.0271\n",
            "Epoch [590/5000], Step [2/800], Loss: 0.0454\n",
            "Epoch [591/5000], Step [2/800], Loss: 0.0260\n",
            "Epoch [592/5000], Step [2/800], Loss: 0.0241\n",
            "Epoch [593/5000], Step [2/800], Loss: 0.0402\n",
            "Epoch [594/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [595/5000], Step [2/800], Loss: 0.0477\n",
            "Epoch [596/5000], Step [2/800], Loss: 0.0479\n",
            "Epoch [597/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [598/5000], Step [2/800], Loss: 0.0418\n",
            "Epoch [599/5000], Step [2/800], Loss: 0.0508\n",
            "Epoch [600/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [601/5000], Step [2/800], Loss: 0.0602\n",
            "Epoch [602/5000], Step [2/800], Loss: 0.0192\n",
            "Epoch [603/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [604/5000], Step [2/800], Loss: 0.0208\n",
            "Epoch [605/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [606/5000], Step [2/800], Loss: 0.0454\n",
            "Epoch [607/5000], Step [2/800], Loss: 0.0481\n",
            "Epoch [608/5000], Step [2/800], Loss: 0.0557\n",
            "Epoch [609/5000], Step [2/800], Loss: 0.0683\n",
            "Epoch [610/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [611/5000], Step [2/800], Loss: 0.0470\n",
            "Epoch [612/5000], Step [2/800], Loss: 0.0517\n",
            "Epoch [613/5000], Step [2/800], Loss: 0.0490\n",
            "Epoch [614/5000], Step [2/800], Loss: 0.0492\n",
            "Epoch [615/5000], Step [2/800], Loss: 0.0438\n",
            "Epoch [616/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [617/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [618/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [619/5000], Step [2/800], Loss: 0.0419\n",
            "Epoch [620/5000], Step [2/800], Loss: 0.0492\n",
            "Epoch [621/5000], Step [2/800], Loss: 0.0272\n",
            "Epoch [622/5000], Step [2/800], Loss: 0.0484\n",
            "Epoch [623/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [624/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [625/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [626/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [627/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [628/5000], Step [2/800], Loss: 0.0492\n",
            "Epoch [629/5000], Step [2/800], Loss: 0.0530\n",
            "Epoch [630/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [631/5000], Step [2/800], Loss: 0.0404\n",
            "Epoch [632/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [633/5000], Step [2/800], Loss: 0.0409\n",
            "Epoch [634/5000], Step [2/800], Loss: 0.0560\n",
            "Epoch [635/5000], Step [2/800], Loss: 0.0500\n",
            "Epoch [636/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [637/5000], Step [2/800], Loss: 0.0404\n",
            "Epoch [638/5000], Step [2/800], Loss: 0.0511\n",
            "Epoch [639/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [640/5000], Step [2/800], Loss: 0.0462\n",
            "Epoch [641/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [642/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [643/5000], Step [2/800], Loss: 0.0307\n",
            "Epoch [644/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [645/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [646/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [647/5000], Step [2/800], Loss: 0.0409\n",
            "Epoch [648/5000], Step [2/800], Loss: 0.0233\n",
            "Epoch [649/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [650/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [651/5000], Step [2/800], Loss: 0.0480\n",
            "Epoch [652/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [653/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [654/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [655/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [656/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [657/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [658/5000], Step [2/800], Loss: 0.0593\n",
            "Epoch [659/5000], Step [2/800], Loss: 0.0354\n",
            "Epoch [660/5000], Step [2/800], Loss: 0.0370\n",
            "Epoch [661/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [662/5000], Step [2/800], Loss: 0.0305\n",
            "Epoch [663/5000], Step [2/800], Loss: 0.0462\n",
            "Epoch [664/5000], Step [2/800], Loss: 0.0731\n",
            "Epoch [665/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [666/5000], Step [2/800], Loss: 0.0428\n",
            "Epoch [667/5000], Step [2/800], Loss: 0.0391\n",
            "Epoch [668/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [669/5000], Step [2/800], Loss: 0.0416\n",
            "Epoch [670/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [671/5000], Step [2/800], Loss: 0.0271\n",
            "Epoch [672/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [673/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [674/5000], Step [2/800], Loss: 0.0337\n",
            "Epoch [675/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [676/5000], Step [2/800], Loss: 0.0375\n",
            "Epoch [677/5000], Step [2/800], Loss: 0.0658\n",
            "Epoch [678/5000], Step [2/800], Loss: 0.0230\n",
            "Epoch [679/5000], Step [2/800], Loss: 0.0361\n",
            "Epoch [680/5000], Step [2/800], Loss: 0.0317\n",
            "Epoch [681/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [682/5000], Step [2/800], Loss: 0.0501\n",
            "Epoch [683/5000], Step [2/800], Loss: 0.0768\n",
            "Epoch [684/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [685/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [686/5000], Step [2/800], Loss: 0.0445\n",
            "Epoch [687/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [688/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [689/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [690/5000], Step [2/800], Loss: 0.0374\n",
            "Epoch [691/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [692/5000], Step [2/800], Loss: 0.0305\n",
            "Epoch [693/5000], Step [2/800], Loss: 0.0354\n",
            "Epoch [694/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [695/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [696/5000], Step [2/800], Loss: 0.0526\n",
            "Epoch [697/5000], Step [2/800], Loss: 0.0253\n",
            "Epoch [698/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [699/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [700/5000], Step [2/800], Loss: 0.0467\n",
            "Epoch [701/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [702/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [703/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [704/5000], Step [2/800], Loss: 0.0412\n",
            "Epoch [705/5000], Step [2/800], Loss: 0.0273\n",
            "Epoch [706/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [707/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [708/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [709/5000], Step [2/800], Loss: 0.0327\n",
            "Epoch [710/5000], Step [2/800], Loss: 0.0488\n",
            "Epoch [711/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [712/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [713/5000], Step [2/800], Loss: 0.0499\n",
            "Epoch [714/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [715/5000], Step [2/800], Loss: 0.0163\n",
            "Epoch [716/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [717/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [718/5000], Step [2/800], Loss: 0.0617\n",
            "Epoch [719/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [720/5000], Step [2/800], Loss: 0.0412\n",
            "Epoch [721/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [722/5000], Step [2/800], Loss: 0.0228\n",
            "Epoch [723/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [724/5000], Step [2/800], Loss: 0.0389\n",
            "Epoch [725/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [726/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [727/5000], Step [2/800], Loss: 0.0511\n",
            "Epoch [728/5000], Step [2/800], Loss: 0.0460\n",
            "Epoch [729/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [730/5000], Step [2/800], Loss: 0.0452\n",
            "Epoch [731/5000], Step [2/800], Loss: 0.0506\n",
            "Epoch [732/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [733/5000], Step [2/800], Loss: 0.0236\n",
            "Epoch [734/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [735/5000], Step [2/800], Loss: 0.0500\n",
            "Epoch [736/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [737/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [738/5000], Step [2/800], Loss: 0.0459\n",
            "Epoch [739/5000], Step [2/800], Loss: 0.0391\n",
            "Epoch [740/5000], Step [2/800], Loss: 0.0573\n",
            "Epoch [741/5000], Step [2/800], Loss: 0.0401\n",
            "Epoch [742/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [743/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [744/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [745/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [746/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [747/5000], Step [2/800], Loss: 0.0435\n",
            "Epoch [748/5000], Step [2/800], Loss: 0.0467\n",
            "Epoch [749/5000], Step [2/800], Loss: 0.0449\n",
            "Epoch [750/5000], Step [2/800], Loss: 0.0472\n",
            "Epoch [751/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [752/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [753/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [754/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [755/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [756/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [757/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [758/5000], Step [2/800], Loss: 0.0214\n",
            "Epoch [759/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [760/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [761/5000], Step [2/800], Loss: 0.0240\n",
            "Epoch [762/5000], Step [2/800], Loss: 0.0374\n",
            "Epoch [763/5000], Step [2/800], Loss: 0.0209\n",
            "Epoch [764/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [765/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [766/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [767/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [768/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [769/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [770/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [771/5000], Step [2/800], Loss: 0.0279\n",
            "Epoch [772/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [773/5000], Step [2/800], Loss: 0.0206\n",
            "Epoch [774/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [775/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [776/5000], Step [2/800], Loss: 0.0455\n",
            "Epoch [777/5000], Step [2/800], Loss: 0.0264\n",
            "Epoch [778/5000], Step [2/800], Loss: 0.0438\n",
            "Epoch [779/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [780/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [781/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [782/5000], Step [2/800], Loss: 0.0661\n",
            "Epoch [783/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [784/5000], Step [2/800], Loss: 0.0457\n",
            "Epoch [785/5000], Step [2/800], Loss: 0.0488\n",
            "Epoch [786/5000], Step [2/800], Loss: 0.0360\n",
            "Epoch [787/5000], Step [2/800], Loss: 0.0449\n",
            "Epoch [788/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [789/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [790/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [791/5000], Step [2/800], Loss: 0.0441\n",
            "Epoch [792/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [793/5000], Step [2/800], Loss: 0.0549\n",
            "Epoch [794/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [795/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [796/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [797/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [798/5000], Step [2/800], Loss: 0.0231\n",
            "Epoch [799/5000], Step [2/800], Loss: 0.0277\n",
            "Epoch [800/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [801/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [802/5000], Step [2/800], Loss: 0.0470\n",
            "Epoch [803/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [804/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [805/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [806/5000], Step [2/800], Loss: 0.0241\n",
            "Epoch [807/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [808/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [809/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [810/5000], Step [2/800], Loss: 0.0375\n",
            "Epoch [811/5000], Step [2/800], Loss: 0.0546\n",
            "Epoch [812/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [813/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [814/5000], Step [2/800], Loss: 0.0412\n",
            "Epoch [815/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [816/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [817/5000], Step [2/800], Loss: 0.0575\n",
            "Epoch [818/5000], Step [2/800], Loss: 0.0625\n",
            "Epoch [819/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [820/5000], Step [2/800], Loss: 0.0557\n",
            "Epoch [821/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [822/5000], Step [2/800], Loss: 0.0432\n",
            "Epoch [823/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [824/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [825/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [826/5000], Step [2/800], Loss: 0.0249\n",
            "Epoch [827/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [828/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [829/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [830/5000], Step [2/800], Loss: 0.0487\n",
            "Epoch [831/5000], Step [2/800], Loss: 0.0540\n",
            "Epoch [832/5000], Step [2/800], Loss: 0.0586\n",
            "Epoch [833/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [834/5000], Step [2/800], Loss: 0.0187\n",
            "Epoch [835/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [836/5000], Step [2/800], Loss: 0.0425\n",
            "Epoch [837/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [838/5000], Step [2/800], Loss: 0.0428\n",
            "Epoch [839/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [840/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [841/5000], Step [2/800], Loss: 0.0600\n",
            "Epoch [842/5000], Step [2/800], Loss: 0.0519\n",
            "Epoch [843/5000], Step [2/800], Loss: 0.0414\n",
            "Epoch [844/5000], Step [2/800], Loss: 0.0240\n",
            "Epoch [845/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [846/5000], Step [2/800], Loss: 0.0486\n",
            "Epoch [847/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [848/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [849/5000], Step [2/800], Loss: 0.0300\n",
            "Epoch [850/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [851/5000], Step [2/800], Loss: 0.0474\n",
            "Epoch [852/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [853/5000], Step [2/800], Loss: 0.0217\n",
            "Epoch [854/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [855/5000], Step [2/800], Loss: 0.0480\n",
            "Epoch [856/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [857/5000], Step [2/800], Loss: 0.0248\n",
            "Epoch [858/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [859/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [860/5000], Step [2/800], Loss: 0.0365\n",
            "Epoch [861/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [862/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [863/5000], Step [2/800], Loss: 0.0407\n",
            "Epoch [864/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [865/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [866/5000], Step [2/800], Loss: 0.0579\n",
            "Epoch [867/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [868/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [869/5000], Step [2/800], Loss: 0.0320\n",
            "Epoch [870/5000], Step [2/800], Loss: 0.0209\n",
            "Epoch [871/5000], Step [2/800], Loss: 0.0191\n",
            "Epoch [872/5000], Step [2/800], Loss: 0.0491\n",
            "Epoch [873/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [874/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [875/5000], Step [2/800], Loss: 0.0409\n",
            "Epoch [876/5000], Step [2/800], Loss: 0.0435\n",
            "Epoch [877/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [878/5000], Step [2/800], Loss: 0.0258\n",
            "Epoch [879/5000], Step [2/800], Loss: 0.0319\n",
            "Epoch [880/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [881/5000], Step [2/800], Loss: 0.0273\n",
            "Epoch [882/5000], Step [2/800], Loss: 0.0481\n",
            "Epoch [883/5000], Step [2/800], Loss: 0.0223\n",
            "Epoch [884/5000], Step [2/800], Loss: 0.0324\n",
            "Epoch [885/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [886/5000], Step [2/800], Loss: 0.0530\n",
            "Epoch [887/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [888/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [889/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [890/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [891/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [892/5000], Step [2/800], Loss: 0.0527\n",
            "Epoch [893/5000], Step [2/800], Loss: 0.0618\n",
            "Epoch [894/5000], Step [2/800], Loss: 0.0407\n",
            "Epoch [895/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [896/5000], Step [2/800], Loss: 0.0707\n",
            "Epoch [897/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [898/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [899/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [900/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [901/5000], Step [2/800], Loss: 0.0409\n",
            "Epoch [902/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [903/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [904/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [905/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [906/5000], Step [2/800], Loss: 0.0250\n",
            "Epoch [907/5000], Step [2/800], Loss: 0.0520\n",
            "Epoch [908/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [909/5000], Step [2/800], Loss: 0.0218\n",
            "Epoch [910/5000], Step [2/800], Loss: 0.0249\n",
            "Epoch [911/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [912/5000], Step [2/800], Loss: 0.0309\n",
            "Epoch [913/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [914/5000], Step [2/800], Loss: 0.0577\n",
            "Epoch [915/5000], Step [2/800], Loss: 0.0504\n",
            "Epoch [916/5000], Step [2/800], Loss: 0.0289\n",
            "Epoch [917/5000], Step [2/800], Loss: 0.0441\n",
            "Epoch [918/5000], Step [2/800], Loss: 0.0538\n",
            "Epoch [919/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [920/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [921/5000], Step [2/800], Loss: 0.0389\n",
            "Epoch [922/5000], Step [2/800], Loss: 0.0453\n",
            "Epoch [923/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [924/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [925/5000], Step [2/800], Loss: 0.0383\n",
            "Epoch [926/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [927/5000], Step [2/800], Loss: 0.0510\n",
            "Epoch [928/5000], Step [2/800], Loss: 0.0236\n",
            "Epoch [929/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [930/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [931/5000], Step [2/800], Loss: 0.0250\n",
            "Epoch [932/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [933/5000], Step [2/800], Loss: 0.0215\n",
            "Epoch [934/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [935/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [936/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [937/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [938/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [939/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [940/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [941/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [942/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [943/5000], Step [2/800], Loss: 0.0480\n",
            "Epoch [944/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [945/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [946/5000], Step [2/800], Loss: 0.0557\n",
            "Epoch [947/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [948/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [949/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [950/5000], Step [2/800], Loss: 0.0491\n",
            "Epoch [951/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [952/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [953/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [954/5000], Step [2/800], Loss: 0.0317\n",
            "Epoch [955/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [956/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [957/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [958/5000], Step [2/800], Loss: 0.0449\n",
            "Epoch [959/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [960/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [961/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [962/5000], Step [2/800], Loss: 0.0469\n",
            "Epoch [963/5000], Step [2/800], Loss: 0.0632\n",
            "Epoch [964/5000], Step [2/800], Loss: 0.0455\n",
            "Epoch [965/5000], Step [2/800], Loss: 0.0371\n",
            "Epoch [966/5000], Step [2/800], Loss: 0.0327\n",
            "Epoch [967/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [968/5000], Step [2/800], Loss: 0.0373\n",
            "Epoch [969/5000], Step [2/800], Loss: 0.0365\n",
            "Epoch [970/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [971/5000], Step [2/800], Loss: 0.0172\n",
            "Epoch [972/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [973/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [974/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [975/5000], Step [2/800], Loss: 0.0509\n",
            "Epoch [976/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [977/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [978/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [979/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [980/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [981/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [982/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [983/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [984/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [985/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [986/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [987/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [988/5000], Step [2/800], Loss: 0.0249\n",
            "Epoch [989/5000], Step [2/800], Loss: 0.0526\n",
            "Epoch [990/5000], Step [2/800], Loss: 0.0592\n",
            "Epoch [991/5000], Step [2/800], Loss: 0.0219\n",
            "Epoch [992/5000], Step [2/800], Loss: 0.0389\n",
            "Epoch [993/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [994/5000], Step [2/800], Loss: 0.0172\n",
            "Epoch [995/5000], Step [2/800], Loss: 0.0454\n",
            "Epoch [996/5000], Step [2/800], Loss: 0.0691\n",
            "Epoch [997/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [998/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [999/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1000/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [1001/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1002/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1003/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [1004/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [1005/5000], Step [2/800], Loss: 0.0508\n",
            "Epoch [1006/5000], Step [2/800], Loss: 0.0493\n",
            "Epoch [1007/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1008/5000], Step [2/800], Loss: 0.0773\n",
            "Epoch [1009/5000], Step [2/800], Loss: 0.0418\n",
            "Epoch [1010/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [1011/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [1012/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1013/5000], Step [2/800], Loss: 0.0225\n",
            "Epoch [1014/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [1015/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [1016/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1017/5000], Step [2/800], Loss: 0.0347\n",
            "Epoch [1018/5000], Step [2/800], Loss: 0.0393\n",
            "Epoch [1019/5000], Step [2/800], Loss: 0.0479\n",
            "Epoch [1020/5000], Step [2/800], Loss: 0.0204\n",
            "Epoch [1021/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1022/5000], Step [2/800], Loss: 0.0193\n",
            "Epoch [1023/5000], Step [2/800], Loss: 0.0446\n",
            "Epoch [1024/5000], Step [2/800], Loss: 0.0441\n",
            "Epoch [1025/5000], Step [2/800], Loss: 0.0441\n",
            "Epoch [1026/5000], Step [2/800], Loss: 0.0627\n",
            "Epoch [1027/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1028/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1029/5000], Step [2/800], Loss: 0.0374\n",
            "Epoch [1030/5000], Step [2/800], Loss: 0.0220\n",
            "Epoch [1031/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [1032/5000], Step [2/800], Loss: 0.0544\n",
            "Epoch [1033/5000], Step [2/800], Loss: 0.0264\n",
            "Epoch [1034/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [1035/5000], Step [2/800], Loss: 0.0188\n",
            "Epoch [1036/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [1037/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1038/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [1039/5000], Step [2/800], Loss: 0.0457\n",
            "Epoch [1040/5000], Step [2/800], Loss: 0.0453\n",
            "Epoch [1041/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [1042/5000], Step [2/800], Loss: 0.0401\n",
            "Epoch [1043/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [1044/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1045/5000], Step [2/800], Loss: 0.0215\n",
            "Epoch [1046/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [1047/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1048/5000], Step [2/800], Loss: 0.0609\n",
            "Epoch [1049/5000], Step [2/800], Loss: 0.0351\n",
            "Epoch [1050/5000], Step [2/800], Loss: 0.0287\n",
            "Epoch [1051/5000], Step [2/800], Loss: 0.0478\n",
            "Epoch [1052/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1053/5000], Step [2/800], Loss: 0.0412\n",
            "Epoch [1054/5000], Step [2/800], Loss: 0.0724\n",
            "Epoch [1055/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1056/5000], Step [2/800], Loss: 0.0590\n",
            "Epoch [1057/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [1058/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [1059/5000], Step [2/800], Loss: 0.0700\n",
            "Epoch [1060/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1061/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1062/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [1063/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [1064/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [1065/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [1066/5000], Step [2/800], Loss: 0.0219\n",
            "Epoch [1067/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1068/5000], Step [2/800], Loss: 0.0341\n",
            "Epoch [1069/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1070/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [1071/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [1072/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [1073/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1074/5000], Step [2/800], Loss: 0.0233\n",
            "Epoch [1075/5000], Step [2/800], Loss: 0.0403\n",
            "Epoch [1076/5000], Step [2/800], Loss: 0.0397\n",
            "Epoch [1077/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [1078/5000], Step [2/800], Loss: 0.0481\n",
            "Epoch [1079/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1080/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [1081/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1082/5000], Step [2/800], Loss: 0.0401\n",
            "Epoch [1083/5000], Step [2/800], Loss: 0.0415\n",
            "Epoch [1084/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1085/5000], Step [2/800], Loss: 0.0419\n",
            "Epoch [1086/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1087/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [1088/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1089/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [1090/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1091/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1092/5000], Step [2/800], Loss: 0.0541\n",
            "Epoch [1093/5000], Step [2/800], Loss: 0.0453\n",
            "Epoch [1094/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [1095/5000], Step [2/800], Loss: 0.0394\n",
            "Epoch [1096/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1097/5000], Step [2/800], Loss: 0.0242\n",
            "Epoch [1098/5000], Step [2/800], Loss: 0.0370\n",
            "Epoch [1099/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1100/5000], Step [2/800], Loss: 0.0366\n",
            "Epoch [1101/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1102/5000], Step [2/800], Loss: 0.0300\n",
            "Epoch [1103/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [1104/5000], Step [2/800], Loss: 0.0552\n",
            "Epoch [1105/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1106/5000], Step [2/800], Loss: 0.0461\n",
            "Epoch [1107/5000], Step [2/800], Loss: 0.0320\n",
            "Epoch [1108/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [1109/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [1110/5000], Step [2/800], Loss: 0.0383\n",
            "Epoch [1111/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [1112/5000], Step [2/800], Loss: 0.0201\n",
            "Epoch [1113/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [1114/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1115/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [1116/5000], Step [2/800], Loss: 0.0361\n",
            "Epoch [1117/5000], Step [2/800], Loss: 0.0464\n",
            "Epoch [1118/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1119/5000], Step [2/800], Loss: 0.0259\n",
            "Epoch [1120/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1121/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [1122/5000], Step [2/800], Loss: 0.0178\n",
            "Epoch [1123/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1124/5000], Step [2/800], Loss: 0.0241\n",
            "Epoch [1125/5000], Step [2/800], Loss: 0.0418\n",
            "Epoch [1126/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [1127/5000], Step [2/800], Loss: 0.0177\n",
            "Epoch [1128/5000], Step [2/800], Loss: 0.0268\n",
            "Epoch [1129/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [1130/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [1131/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1132/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1133/5000], Step [2/800], Loss: 0.0541\n",
            "Epoch [1134/5000], Step [2/800], Loss: 0.0239\n",
            "Epoch [1135/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [1136/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1137/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [1138/5000], Step [2/800], Loss: 0.0327\n",
            "Epoch [1139/5000], Step [2/800], Loss: 0.0458\n",
            "Epoch [1140/5000], Step [2/800], Loss: 0.0305\n",
            "Epoch [1141/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [1142/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1143/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [1144/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [1145/5000], Step [2/800], Loss: 0.0472\n",
            "Epoch [1146/5000], Step [2/800], Loss: 0.0341\n",
            "Epoch [1147/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1148/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [1149/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1150/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1151/5000], Step [2/800], Loss: 0.0263\n",
            "Epoch [1152/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [1153/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1154/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [1155/5000], Step [2/800], Loss: 0.0309\n",
            "Epoch [1156/5000], Step [2/800], Loss: 0.0263\n",
            "Epoch [1157/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [1158/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1159/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1160/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [1161/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1162/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1163/5000], Step [2/800], Loss: 0.0413\n",
            "Epoch [1164/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [1165/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1166/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [1167/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [1168/5000], Step [2/800], Loss: 0.0214\n",
            "Epoch [1169/5000], Step [2/800], Loss: 0.0477\n",
            "Epoch [1170/5000], Step [2/800], Loss: 0.0178\n",
            "Epoch [1171/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [1172/5000], Step [2/800], Loss: 0.0479\n",
            "Epoch [1173/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1174/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [1175/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [1176/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1177/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [1178/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [1179/5000], Step [2/800], Loss: 0.0284\n",
            "Epoch [1180/5000], Step [2/800], Loss: 0.0493\n",
            "Epoch [1181/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [1182/5000], Step [2/800], Loss: 0.0268\n",
            "Epoch [1183/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1184/5000], Step [2/800], Loss: 0.0475\n",
            "Epoch [1185/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [1186/5000], Step [2/800], Loss: 0.0548\n",
            "Epoch [1187/5000], Step [2/800], Loss: 0.0191\n",
            "Epoch [1188/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1189/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [1190/5000], Step [2/800], Loss: 0.0518\n",
            "Epoch [1191/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [1192/5000], Step [2/800], Loss: 0.0347\n",
            "Epoch [1193/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [1194/5000], Step [2/800], Loss: 0.0259\n",
            "Epoch [1195/5000], Step [2/800], Loss: 0.0507\n",
            "Epoch [1196/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [1197/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [1198/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1199/5000], Step [2/800], Loss: 0.0259\n",
            "Epoch [1200/5000], Step [2/800], Loss: 0.0196\n",
            "Epoch [1201/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [1202/5000], Step [2/800], Loss: 0.0432\n",
            "Epoch [1203/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [1204/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1205/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [1206/5000], Step [2/800], Loss: 0.0455\n",
            "Epoch [1207/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1208/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1209/5000], Step [2/800], Loss: 0.0434\n",
            "Epoch [1210/5000], Step [2/800], Loss: 0.0250\n",
            "Epoch [1211/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1212/5000], Step [2/800], Loss: 0.0307\n",
            "Epoch [1213/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [1214/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [1215/5000], Step [2/800], Loss: 0.0170\n",
            "Epoch [1216/5000], Step [2/800], Loss: 0.0317\n",
            "Epoch [1217/5000], Step [2/800], Loss: 0.0510\n",
            "Epoch [1218/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1219/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [1220/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [1221/5000], Step [2/800], Loss: 0.0289\n",
            "Epoch [1222/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1223/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1224/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [1225/5000], Step [2/800], Loss: 0.0708\n",
            "Epoch [1226/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [1227/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [1228/5000], Step [2/800], Loss: 0.0511\n",
            "Epoch [1229/5000], Step [2/800], Loss: 0.0307\n",
            "Epoch [1230/5000], Step [2/800], Loss: 0.0465\n",
            "Epoch [1231/5000], Step [2/800], Loss: 0.0404\n",
            "Epoch [1232/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1233/5000], Step [2/800], Loss: 0.0277\n",
            "Epoch [1234/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1235/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [1236/5000], Step [2/800], Loss: 0.0594\n",
            "Epoch [1237/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [1238/5000], Step [2/800], Loss: 0.0517\n",
            "Epoch [1239/5000], Step [2/800], Loss: 0.0446\n",
            "Epoch [1240/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [1241/5000], Step [2/800], Loss: 0.0268\n",
            "Epoch [1242/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [1243/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [1244/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1245/5000], Step [2/800], Loss: 0.0564\n",
            "Epoch [1246/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [1247/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [1248/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [1249/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1250/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [1251/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [1252/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [1253/5000], Step [2/800], Loss: 0.0194\n",
            "Epoch [1254/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [1255/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [1256/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [1257/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1258/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [1259/5000], Step [2/800], Loss: 0.0247\n",
            "Epoch [1260/5000], Step [2/800], Loss: 0.0510\n",
            "Epoch [1261/5000], Step [2/800], Loss: 0.0351\n",
            "Epoch [1262/5000], Step [2/800], Loss: 0.0494\n",
            "Epoch [1263/5000], Step [2/800], Loss: 0.0204\n",
            "Epoch [1264/5000], Step [2/800], Loss: 0.0547\n",
            "Epoch [1265/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [1266/5000], Step [2/800], Loss: 0.0242\n",
            "Epoch [1267/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1268/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1269/5000], Step [2/800], Loss: 0.0592\n",
            "Epoch [1270/5000], Step [2/800], Loss: 0.0536\n",
            "Epoch [1271/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1272/5000], Step [2/800], Loss: 0.0347\n",
            "Epoch [1273/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1274/5000], Step [2/800], Loss: 0.0356\n",
            "Epoch [1275/5000], Step [2/800], Loss: 0.0650\n",
            "Epoch [1276/5000], Step [2/800], Loss: 0.0352\n",
            "Epoch [1277/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1278/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [1279/5000], Step [2/800], Loss: 0.0232\n",
            "Epoch [1280/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [1281/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1282/5000], Step [2/800], Loss: 0.0596\n",
            "Epoch [1283/5000], Step [2/800], Loss: 0.0553\n",
            "Epoch [1284/5000], Step [2/800], Loss: 0.0190\n",
            "Epoch [1285/5000], Step [2/800], Loss: 0.0456\n",
            "Epoch [1286/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [1287/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [1288/5000], Step [2/800], Loss: 0.0512\n",
            "Epoch [1289/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1290/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1291/5000], Step [2/800], Loss: 0.0202\n",
            "Epoch [1292/5000], Step [2/800], Loss: 0.0249\n",
            "Epoch [1293/5000], Step [2/800], Loss: 0.0121\n",
            "Epoch [1294/5000], Step [2/800], Loss: 0.0259\n",
            "Epoch [1295/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1296/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1297/5000], Step [2/800], Loss: 0.0463\n",
            "Epoch [1298/5000], Step [2/800], Loss: 0.0441\n",
            "Epoch [1299/5000], Step [2/800], Loss: 0.0473\n",
            "Epoch [1300/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1301/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [1302/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1303/5000], Step [2/800], Loss: 0.0469\n",
            "Epoch [1304/5000], Step [2/800], Loss: 0.0713\n",
            "Epoch [1305/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [1306/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [1307/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1308/5000], Step [2/800], Loss: 0.0244\n",
            "Epoch [1309/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [1310/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1311/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1312/5000], Step [2/800], Loss: 0.0472\n",
            "Epoch [1313/5000], Step [2/800], Loss: 0.0277\n",
            "Epoch [1314/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [1315/5000], Step [2/800], Loss: 0.0243\n",
            "Epoch [1316/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1317/5000], Step [2/800], Loss: 0.0520\n",
            "Epoch [1318/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1319/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [1320/5000], Step [2/800], Loss: 0.0495\n",
            "Epoch [1321/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1322/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [1323/5000], Step [2/800], Loss: 0.0245\n",
            "Epoch [1324/5000], Step [2/800], Loss: 0.0175\n",
            "Epoch [1325/5000], Step [2/800], Loss: 0.0269\n",
            "Epoch [1326/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [1327/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1328/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [1329/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1330/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [1331/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [1332/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1333/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [1334/5000], Step [2/800], Loss: 0.0215\n",
            "Epoch [1335/5000], Step [2/800], Loss: 0.0205\n",
            "Epoch [1336/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [1337/5000], Step [2/800], Loss: 0.0383\n",
            "Epoch [1338/5000], Step [2/800], Loss: 0.0468\n",
            "Epoch [1339/5000], Step [2/800], Loss: 0.0273\n",
            "Epoch [1340/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1341/5000], Step [2/800], Loss: 0.0411\n",
            "Epoch [1342/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1343/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [1344/5000], Step [2/800], Loss: 0.0517\n",
            "Epoch [1345/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [1346/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [1347/5000], Step [2/800], Loss: 0.0361\n",
            "Epoch [1348/5000], Step [2/800], Loss: 0.0457\n",
            "Epoch [1349/5000], Step [2/800], Loss: 0.0428\n",
            "Epoch [1350/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1351/5000], Step [2/800], Loss: 0.0245\n",
            "Epoch [1352/5000], Step [2/800], Loss: 0.0379\n",
            "Epoch [1353/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [1354/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [1355/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [1356/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [1357/5000], Step [2/800], Loss: 0.0508\n",
            "Epoch [1358/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [1359/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1360/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1361/5000], Step [2/800], Loss: 0.0177\n",
            "Epoch [1362/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [1363/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1364/5000], Step [2/800], Loss: 0.0339\n",
            "Epoch [1365/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1366/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [1367/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [1368/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [1369/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1370/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1371/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1372/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [1373/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1374/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1375/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1376/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1377/5000], Step [2/800], Loss: 0.0224\n",
            "Epoch [1378/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1379/5000], Step [2/800], Loss: 0.0423\n",
            "Epoch [1380/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [1381/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1382/5000], Step [2/800], Loss: 0.0415\n",
            "Epoch [1383/5000], Step [2/800], Loss: 0.0309\n",
            "Epoch [1384/5000], Step [2/800], Loss: 0.0231\n",
            "Epoch [1385/5000], Step [2/800], Loss: 0.0406\n",
            "Epoch [1386/5000], Step [2/800], Loss: 0.0532\n",
            "Epoch [1387/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1388/5000], Step [2/800], Loss: 0.0360\n",
            "Epoch [1389/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1390/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1391/5000], Step [2/800], Loss: 0.0481\n",
            "Epoch [1392/5000], Step [2/800], Loss: 0.0239\n",
            "Epoch [1393/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [1394/5000], Step [2/800], Loss: 0.0614\n",
            "Epoch [1395/5000], Step [2/800], Loss: 0.0210\n",
            "Epoch [1396/5000], Step [2/800], Loss: 0.0407\n",
            "Epoch [1397/5000], Step [2/800], Loss: 0.0347\n",
            "Epoch [1398/5000], Step [2/800], Loss: 0.0518\n",
            "Epoch [1399/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1400/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1401/5000], Step [2/800], Loss: 0.0230\n",
            "Epoch [1402/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [1403/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [1404/5000], Step [2/800], Loss: 0.0320\n",
            "Epoch [1405/5000], Step [2/800], Loss: 0.0200\n",
            "Epoch [1406/5000], Step [2/800], Loss: 0.0225\n",
            "Epoch [1407/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [1408/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1409/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [1410/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1411/5000], Step [2/800], Loss: 0.0514\n",
            "Epoch [1412/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1413/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [1414/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1415/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1416/5000], Step [2/800], Loss: 0.0471\n",
            "Epoch [1417/5000], Step [2/800], Loss: 0.0505\n",
            "Epoch [1418/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1419/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1420/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [1421/5000], Step [2/800], Loss: 0.0600\n",
            "Epoch [1422/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [1423/5000], Step [2/800], Loss: 0.0415\n",
            "Epoch [1424/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [1425/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1426/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [1427/5000], Step [2/800], Loss: 0.0195\n",
            "Epoch [1428/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [1429/5000], Step [2/800], Loss: 0.0373\n",
            "Epoch [1430/5000], Step [2/800], Loss: 0.0287\n",
            "Epoch [1431/5000], Step [2/800], Loss: 0.0374\n",
            "Epoch [1432/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1433/5000], Step [2/800], Loss: 0.0308\n",
            "Epoch [1434/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [1435/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [1436/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1437/5000], Step [2/800], Loss: 0.0426\n",
            "Epoch [1438/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1439/5000], Step [2/800], Loss: 0.0218\n",
            "Epoch [1440/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [1441/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1442/5000], Step [2/800], Loss: 0.0191\n",
            "Epoch [1443/5000], Step [2/800], Loss: 0.0543\n",
            "Epoch [1444/5000], Step [2/800], Loss: 0.0244\n",
            "Epoch [1445/5000], Step [2/800], Loss: 0.0252\n",
            "Epoch [1446/5000], Step [2/800], Loss: 0.0386\n",
            "Epoch [1447/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1448/5000], Step [2/800], Loss: 0.0432\n",
            "Epoch [1449/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1450/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [1451/5000], Step [2/800], Loss: 0.0495\n",
            "Epoch [1452/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [1453/5000], Step [2/800], Loss: 0.0507\n",
            "Epoch [1454/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1455/5000], Step [2/800], Loss: 0.0497\n",
            "Epoch [1456/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [1457/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1458/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [1459/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [1460/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [1461/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1462/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [1463/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [1464/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [1465/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1466/5000], Step [2/800], Loss: 0.0638\n",
            "Epoch [1467/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1468/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [1469/5000], Step [2/800], Loss: 0.0231\n",
            "Epoch [1470/5000], Step [2/800], Loss: 0.0557\n",
            "Epoch [1471/5000], Step [2/800], Loss: 0.0456\n",
            "Epoch [1472/5000], Step [2/800], Loss: 0.0425\n",
            "Epoch [1473/5000], Step [2/800], Loss: 0.0332\n",
            "Epoch [1474/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1475/5000], Step [2/800], Loss: 0.0254\n",
            "Epoch [1476/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [1477/5000], Step [2/800], Loss: 0.0259\n",
            "Epoch [1478/5000], Step [2/800], Loss: 0.0320\n",
            "Epoch [1479/5000], Step [2/800], Loss: 0.0197\n",
            "Epoch [1480/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1481/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1482/5000], Step [2/800], Loss: 0.0220\n",
            "Epoch [1483/5000], Step [2/800], Loss: 0.0533\n",
            "Epoch [1484/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1485/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1486/5000], Step [2/800], Loss: 0.0365\n",
            "Epoch [1487/5000], Step [2/800], Loss: 0.0514\n",
            "Epoch [1488/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1489/5000], Step [2/800], Loss: 0.0471\n",
            "Epoch [1490/5000], Step [2/800], Loss: 0.0197\n",
            "Epoch [1491/5000], Step [2/800], Loss: 0.0319\n",
            "Epoch [1492/5000], Step [2/800], Loss: 0.0246\n",
            "Epoch [1493/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [1494/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [1495/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1496/5000], Step [2/800], Loss: 0.0210\n",
            "Epoch [1497/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1498/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [1499/5000], Step [2/800], Loss: 0.0252\n",
            "Epoch [1500/5000], Step [2/800], Loss: 0.0239\n",
            "Epoch [1501/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1502/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1503/5000], Step [2/800], Loss: 0.0254\n",
            "Epoch [1504/5000], Step [2/800], Loss: 0.0480\n",
            "Epoch [1505/5000], Step [2/800], Loss: 0.0558\n",
            "Epoch [1506/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1507/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1508/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1509/5000], Step [2/800], Loss: 0.0431\n",
            "Epoch [1510/5000], Step [2/800], Loss: 0.0194\n",
            "Epoch [1511/5000], Step [2/800], Loss: 0.0215\n",
            "Epoch [1512/5000], Step [2/800], Loss: 0.0556\n",
            "Epoch [1513/5000], Step [2/800], Loss: 0.0244\n",
            "Epoch [1514/5000], Step [2/800], Loss: 0.0294\n",
            "Epoch [1515/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [1516/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1517/5000], Step [2/800], Loss: 0.0564\n",
            "Epoch [1518/5000], Step [2/800], Loss: 0.0287\n",
            "Epoch [1519/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1520/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1521/5000], Step [2/800], Loss: 0.0227\n",
            "Epoch [1522/5000], Step [2/800], Loss: 0.0283\n",
            "Epoch [1523/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1524/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [1525/5000], Step [2/800], Loss: 0.0247\n",
            "Epoch [1526/5000], Step [2/800], Loss: 0.0456\n",
            "Epoch [1527/5000], Step [2/800], Loss: 0.0188\n",
            "Epoch [1528/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1529/5000], Step [2/800], Loss: 0.0474\n",
            "Epoch [1530/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [1531/5000], Step [2/800], Loss: 0.0408\n",
            "Epoch [1532/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [1533/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1534/5000], Step [2/800], Loss: 0.0277\n",
            "Epoch [1535/5000], Step [2/800], Loss: 0.0426\n",
            "Epoch [1536/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1537/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1538/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [1539/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [1540/5000], Step [2/800], Loss: 0.0567\n",
            "Epoch [1541/5000], Step [2/800], Loss: 0.0375\n",
            "Epoch [1542/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [1543/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1544/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1545/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1546/5000], Step [2/800], Loss: 0.0221\n",
            "Epoch [1547/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [1548/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [1549/5000], Step [2/800], Loss: 0.0175\n",
            "Epoch [1550/5000], Step [2/800], Loss: 0.0197\n",
            "Epoch [1551/5000], Step [2/800], Loss: 0.0446\n",
            "Epoch [1552/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [1553/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [1554/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [1555/5000], Step [2/800], Loss: 0.0428\n",
            "Epoch [1556/5000], Step [2/800], Loss: 0.0277\n",
            "Epoch [1557/5000], Step [2/800], Loss: 0.0403\n",
            "Epoch [1558/5000], Step [2/800], Loss: 0.0464\n",
            "Epoch [1559/5000], Step [2/800], Loss: 0.0201\n",
            "Epoch [1560/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [1561/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1562/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1563/5000], Step [2/800], Loss: 0.0219\n",
            "Epoch [1564/5000], Step [2/800], Loss: 0.0514\n",
            "Epoch [1565/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1566/5000], Step [2/800], Loss: 0.0576\n",
            "Epoch [1567/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1568/5000], Step [2/800], Loss: 0.0299\n",
            "Epoch [1569/5000], Step [2/800], Loss: 0.0354\n",
            "Epoch [1570/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1571/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [1572/5000], Step [2/800], Loss: 0.0433\n",
            "Epoch [1573/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1574/5000], Step [2/800], Loss: 0.0514\n",
            "Epoch [1575/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1576/5000], Step [2/800], Loss: 0.0642\n",
            "Epoch [1577/5000], Step [2/800], Loss: 0.0248\n",
            "Epoch [1578/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1579/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [1580/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1581/5000], Step [2/800], Loss: 0.0356\n",
            "Epoch [1582/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1583/5000], Step [2/800], Loss: 0.0548\n",
            "Epoch [1584/5000], Step [2/800], Loss: 0.0580\n",
            "Epoch [1585/5000], Step [2/800], Loss: 0.0485\n",
            "Epoch [1586/5000], Step [2/800], Loss: 0.0584\n",
            "Epoch [1587/5000], Step [2/800], Loss: 0.0519\n",
            "Epoch [1588/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1589/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1590/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1591/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1592/5000], Step [2/800], Loss: 0.0451\n",
            "Epoch [1593/5000], Step [2/800], Loss: 0.0414\n",
            "Epoch [1594/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1595/5000], Step [2/800], Loss: 0.0370\n",
            "Epoch [1596/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [1597/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [1598/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [1599/5000], Step [2/800], Loss: 0.0411\n",
            "Epoch [1600/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [1601/5000], Step [2/800], Loss: 0.0200\n",
            "Epoch [1602/5000], Step [2/800], Loss: 0.0211\n",
            "Epoch [1603/5000], Step [2/800], Loss: 0.0563\n",
            "Epoch [1604/5000], Step [2/800], Loss: 0.0339\n",
            "Epoch [1605/5000], Step [2/800], Loss: 0.0294\n",
            "Epoch [1606/5000], Step [2/800], Loss: 0.0211\n",
            "Epoch [1607/5000], Step [2/800], Loss: 0.0199\n",
            "Epoch [1608/5000], Step [2/800], Loss: 0.0235\n",
            "Epoch [1609/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [1610/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1611/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [1612/5000], Step [2/800], Loss: 0.0597\n",
            "Epoch [1613/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [1614/5000], Step [2/800], Loss: 0.0525\n",
            "Epoch [1615/5000], Step [2/800], Loss: 0.0291\n",
            "Epoch [1616/5000], Step [2/800], Loss: 0.0539\n",
            "Epoch [1617/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1618/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1619/5000], Step [2/800], Loss: 0.0505\n",
            "Epoch [1620/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1621/5000], Step [2/800], Loss: 0.0417\n",
            "Epoch [1622/5000], Step [2/800], Loss: 0.0234\n",
            "Epoch [1623/5000], Step [2/800], Loss: 0.0248\n",
            "Epoch [1624/5000], Step [2/800], Loss: 0.0245\n",
            "Epoch [1625/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [1626/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1627/5000], Step [2/800], Loss: 0.0525\n",
            "Epoch [1628/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [1629/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [1630/5000], Step [2/800], Loss: 0.0486\n",
            "Epoch [1631/5000], Step [2/800], Loss: 0.0258\n",
            "Epoch [1632/5000], Step [2/800], Loss: 0.0365\n",
            "Epoch [1633/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [1634/5000], Step [2/800], Loss: 0.0426\n",
            "Epoch [1635/5000], Step [2/800], Loss: 0.0180\n",
            "Epoch [1636/5000], Step [2/800], Loss: 0.0423\n",
            "Epoch [1637/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1638/5000], Step [2/800], Loss: 0.0209\n",
            "Epoch [1639/5000], Step [2/800], Loss: 0.0506\n",
            "Epoch [1640/5000], Step [2/800], Loss: 0.0300\n",
            "Epoch [1641/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1642/5000], Step [2/800], Loss: 0.0463\n",
            "Epoch [1643/5000], Step [2/800], Loss: 0.0187\n",
            "Epoch [1644/5000], Step [2/800], Loss: 0.0323\n",
            "Epoch [1645/5000], Step [2/800], Loss: 0.0399\n",
            "Epoch [1646/5000], Step [2/800], Loss: 0.0317\n",
            "Epoch [1647/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1648/5000], Step [2/800], Loss: 0.0235\n",
            "Epoch [1649/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1650/5000], Step [2/800], Loss: 0.0319\n",
            "Epoch [1651/5000], Step [2/800], Loss: 0.0224\n",
            "Epoch [1652/5000], Step [2/800], Loss: 0.0183\n",
            "Epoch [1653/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [1654/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [1655/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [1656/5000], Step [2/800], Loss: 0.0309\n",
            "Epoch [1657/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1658/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [1659/5000], Step [2/800], Loss: 0.0303\n",
            "Epoch [1660/5000], Step [2/800], Loss: 0.0451\n",
            "Epoch [1661/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [1662/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1663/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [1664/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [1665/5000], Step [2/800], Loss: 0.0245\n",
            "Epoch [1666/5000], Step [2/800], Loss: 0.0212\n",
            "Epoch [1667/5000], Step [2/800], Loss: 0.0442\n",
            "Epoch [1668/5000], Step [2/800], Loss: 0.0245\n",
            "Epoch [1669/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [1670/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [1671/5000], Step [2/800], Loss: 0.0535\n",
            "Epoch [1672/5000], Step [2/800], Loss: 0.0449\n",
            "Epoch [1673/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [1674/5000], Step [2/800], Loss: 0.0341\n",
            "Epoch [1675/5000], Step [2/800], Loss: 0.0402\n",
            "Epoch [1676/5000], Step [2/800], Loss: 0.0260\n",
            "Epoch [1677/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [1678/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1679/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1680/5000], Step [2/800], Loss: 0.0416\n",
            "Epoch [1681/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1682/5000], Step [2/800], Loss: 0.0187\n",
            "Epoch [1683/5000], Step [2/800], Loss: 0.0203\n",
            "Epoch [1684/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [1685/5000], Step [2/800], Loss: 0.0384\n",
            "Epoch [1686/5000], Step [2/800], Loss: 0.0359\n",
            "Epoch [1687/5000], Step [2/800], Loss: 0.0216\n",
            "Epoch [1688/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1689/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1690/5000], Step [2/800], Loss: 0.0423\n",
            "Epoch [1691/5000], Step [2/800], Loss: 0.0285\n",
            "Epoch [1692/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1693/5000], Step [2/800], Loss: 0.0197\n",
            "Epoch [1694/5000], Step [2/800], Loss: 0.0235\n",
            "Epoch [1695/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [1696/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1697/5000], Step [2/800], Loss: 0.0503\n",
            "Epoch [1698/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1699/5000], Step [2/800], Loss: 0.0448\n",
            "Epoch [1700/5000], Step [2/800], Loss: 0.0228\n",
            "Epoch [1701/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1702/5000], Step [2/800], Loss: 0.0292\n",
            "Epoch [1703/5000], Step [2/800], Loss: 0.0411\n",
            "Epoch [1704/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [1705/5000], Step [2/800], Loss: 0.0206\n",
            "Epoch [1706/5000], Step [2/800], Loss: 0.0247\n",
            "Epoch [1707/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1708/5000], Step [2/800], Loss: 0.0278\n",
            "Epoch [1709/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1710/5000], Step [2/800], Loss: 0.0679\n",
            "Epoch [1711/5000], Step [2/800], Loss: 0.0662\n",
            "Epoch [1712/5000], Step [2/800], Loss: 0.0269\n",
            "Epoch [1713/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1714/5000], Step [2/800], Loss: 0.0279\n",
            "Epoch [1715/5000], Step [2/800], Loss: 0.0199\n",
            "Epoch [1716/5000], Step [2/800], Loss: 0.0388\n",
            "Epoch [1717/5000], Step [2/800], Loss: 0.0211\n",
            "Epoch [1718/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [1719/5000], Step [2/800], Loss: 0.0446\n",
            "Epoch [1720/5000], Step [2/800], Loss: 0.0337\n",
            "Epoch [1721/5000], Step [2/800], Loss: 0.0184\n",
            "Epoch [1722/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1723/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1724/5000], Step [2/800], Loss: 0.0509\n",
            "Epoch [1725/5000], Step [2/800], Loss: 0.0269\n",
            "Epoch [1726/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1727/5000], Step [2/800], Loss: 0.0270\n",
            "Epoch [1728/5000], Step [2/800], Loss: 0.0243\n",
            "Epoch [1729/5000], Step [2/800], Loss: 0.0227\n",
            "Epoch [1730/5000], Step [2/800], Loss: 0.0319\n",
            "Epoch [1731/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [1732/5000], Step [2/800], Loss: 0.0268\n",
            "Epoch [1733/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1734/5000], Step [2/800], Loss: 0.0397\n",
            "Epoch [1735/5000], Step [2/800], Loss: 0.0602\n",
            "Epoch [1736/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1737/5000], Step [2/800], Loss: 0.0398\n",
            "Epoch [1738/5000], Step [2/800], Loss: 0.0236\n",
            "Epoch [1739/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1740/5000], Step [2/800], Loss: 0.0620\n",
            "Epoch [1741/5000], Step [2/800], Loss: 0.0468\n",
            "Epoch [1742/5000], Step [2/800], Loss: 0.0340\n",
            "Epoch [1743/5000], Step [2/800], Loss: 0.0390\n",
            "Epoch [1744/5000], Step [2/800], Loss: 0.0321\n",
            "Epoch [1745/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [1746/5000], Step [2/800], Loss: 0.0331\n",
            "Epoch [1747/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [1748/5000], Step [2/800], Loss: 0.0618\n",
            "Epoch [1749/5000], Step [2/800], Loss: 0.0490\n",
            "Epoch [1750/5000], Step [2/800], Loss: 0.0421\n",
            "Epoch [1751/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1752/5000], Step [2/800], Loss: 0.0361\n",
            "Epoch [1753/5000], Step [2/800], Loss: 0.0574\n",
            "Epoch [1754/5000], Step [2/800], Loss: 0.0492\n",
            "Epoch [1755/5000], Step [2/800], Loss: 0.0204\n",
            "Epoch [1756/5000], Step [2/800], Loss: 0.0296\n",
            "Epoch [1757/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1758/5000], Step [2/800], Loss: 0.0207\n",
            "Epoch [1759/5000], Step [2/800], Loss: 0.0199\n",
            "Epoch [1760/5000], Step [2/800], Loss: 0.0476\n",
            "Epoch [1761/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [1762/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [1763/5000], Step [2/800], Loss: 0.0378\n",
            "Epoch [1764/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [1765/5000], Step [2/800], Loss: 0.0289\n",
            "Epoch [1766/5000], Step [2/800], Loss: 0.0489\n",
            "Epoch [1767/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [1768/5000], Step [2/800], Loss: 0.0327\n",
            "Epoch [1769/5000], Step [2/800], Loss: 0.0178\n",
            "Epoch [1770/5000], Step [2/800], Loss: 0.0202\n",
            "Epoch [1771/5000], Step [2/800], Loss: 0.0334\n",
            "Epoch [1772/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [1773/5000], Step [2/800], Loss: 0.0208\n",
            "Epoch [1774/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [1775/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [1776/5000], Step [2/800], Loss: 0.0443\n",
            "Epoch [1777/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1778/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1779/5000], Step [2/800], Loss: 0.0353\n",
            "Epoch [1780/5000], Step [2/800], Loss: 0.0235\n",
            "Epoch [1781/5000], Step [2/800], Loss: 0.0232\n",
            "Epoch [1782/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [1783/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1784/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1785/5000], Step [2/800], Loss: 0.0422\n",
            "Epoch [1786/5000], Step [2/800], Loss: 0.0253\n",
            "Epoch [1787/5000], Step [2/800], Loss: 0.0363\n",
            "Epoch [1788/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1789/5000], Step [2/800], Loss: 0.0214\n",
            "Epoch [1790/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1791/5000], Step [2/800], Loss: 0.0349\n",
            "Epoch [1792/5000], Step [2/800], Loss: 0.0322\n",
            "Epoch [1793/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [1794/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1795/5000], Step [2/800], Loss: 0.0445\n",
            "Epoch [1796/5000], Step [2/800], Loss: 0.0460\n",
            "Epoch [1797/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1798/5000], Step [2/800], Loss: 0.0447\n",
            "Epoch [1799/5000], Step [2/800], Loss: 0.0368\n",
            "Epoch [1800/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [1801/5000], Step [2/800], Loss: 0.0280\n",
            "Epoch [1802/5000], Step [2/800], Loss: 0.0253\n",
            "Epoch [1803/5000], Step [2/800], Loss: 0.0195\n",
            "Epoch [1804/5000], Step [2/800], Loss: 0.0250\n",
            "Epoch [1805/5000], Step [2/800], Loss: 0.0244\n",
            "Epoch [1806/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1807/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1808/5000], Step [2/800], Loss: 0.0380\n",
            "Epoch [1809/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [1810/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [1811/5000], Step [2/800], Loss: 0.0298\n",
            "Epoch [1812/5000], Step [2/800], Loss: 0.0430\n",
            "Epoch [1813/5000], Step [2/800], Loss: 0.0333\n",
            "Epoch [1814/5000], Step [2/800], Loss: 0.0429\n",
            "Epoch [1815/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [1816/5000], Step [2/800], Loss: 0.0272\n",
            "Epoch [1817/5000], Step [2/800], Loss: 0.0255\n",
            "Epoch [1818/5000], Step [2/800], Loss: 0.0336\n",
            "Epoch [1819/5000], Step [2/800], Loss: 0.0372\n",
            "Epoch [1820/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [1821/5000], Step [2/800], Loss: 0.0344\n",
            "Epoch [1822/5000], Step [2/800], Loss: 0.0364\n",
            "Epoch [1823/5000], Step [2/800], Loss: 0.0275\n",
            "Epoch [1824/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1825/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1826/5000], Step [2/800], Loss: 0.0312\n",
            "Epoch [1827/5000], Step [2/800], Loss: 0.0387\n",
            "Epoch [1828/5000], Step [2/800], Loss: 0.0627\n",
            "Epoch [1829/5000], Step [2/800], Loss: 0.0407\n",
            "Epoch [1830/5000], Step [2/800], Loss: 0.0286\n",
            "Epoch [1831/5000], Step [2/800], Loss: 0.0202\n",
            "Epoch [1832/5000], Step [2/800], Loss: 0.0370\n",
            "Epoch [1833/5000], Step [2/800], Loss: 0.0409\n",
            "Epoch [1834/5000], Step [2/800], Loss: 0.0382\n",
            "Epoch [1835/5000], Step [2/800], Loss: 0.0463\n",
            "Epoch [1836/5000], Step [2/800], Loss: 0.0327\n",
            "Epoch [1837/5000], Step [2/800], Loss: 0.0362\n",
            "Epoch [1838/5000], Step [2/800], Loss: 0.0262\n",
            "Epoch [1839/5000], Step [2/800], Loss: 0.0162\n",
            "Epoch [1840/5000], Step [2/800], Loss: 0.0330\n",
            "Epoch [1841/5000], Step [2/800], Loss: 0.0264\n",
            "Epoch [1842/5000], Step [2/800], Loss: 0.0424\n",
            "Epoch [1843/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1844/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [1845/5000], Step [2/800], Loss: 0.0212\n",
            "Epoch [1846/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1847/5000], Step [2/800], Loss: 0.0342\n",
            "Epoch [1848/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1849/5000], Step [2/800], Loss: 0.0452\n",
            "Epoch [1850/5000], Step [2/800], Loss: 0.0631\n",
            "Epoch [1851/5000], Step [2/800], Loss: 0.0375\n",
            "Epoch [1852/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [1853/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1854/5000], Step [2/800], Loss: 0.0332\n",
            "Epoch [1855/5000], Step [2/800], Loss: 0.0389\n",
            "Epoch [1856/5000], Step [2/800], Loss: 0.0257\n",
            "Epoch [1857/5000], Step [2/800], Loss: 0.0377\n",
            "Epoch [1858/5000], Step [2/800], Loss: 0.0332\n",
            "Epoch [1859/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [1860/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1861/5000], Step [2/800], Loss: 0.0297\n",
            "Epoch [1862/5000], Step [2/800], Loss: 0.0206\n",
            "Epoch [1863/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1864/5000], Step [2/800], Loss: 0.0348\n",
            "Epoch [1865/5000], Step [2/800], Loss: 0.0513\n",
            "Epoch [1866/5000], Step [2/800], Loss: 0.0426\n",
            "Epoch [1867/5000], Step [2/800], Loss: 0.0635\n",
            "Epoch [1868/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1869/5000], Step [2/800], Loss: 0.0153\n",
            "Epoch [1870/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [1871/5000], Step [2/800], Loss: 0.0310\n",
            "Epoch [1872/5000], Step [2/800], Loss: 0.0293\n",
            "Epoch [1873/5000], Step [2/800], Loss: 0.0635\n",
            "Epoch [1874/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [1875/5000], Step [2/800], Loss: 0.0316\n",
            "Epoch [1876/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1877/5000], Step [2/800], Loss: 0.0345\n",
            "Epoch [1878/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1879/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [1880/5000], Step [2/800], Loss: 0.0432\n",
            "Epoch [1881/5000], Step [2/800], Loss: 0.0440\n",
            "Epoch [1882/5000], Step [2/800], Loss: 0.0367\n",
            "Epoch [1883/5000], Step [2/800], Loss: 0.0444\n",
            "Epoch [1884/5000], Step [2/800], Loss: 0.0355\n",
            "Epoch [1885/5000], Step [2/800], Loss: 0.0271\n",
            "Epoch [1886/5000], Step [2/800], Loss: 0.0584\n",
            "Epoch [1887/5000], Step [2/800], Loss: 0.0193\n",
            "Epoch [1888/5000], Step [2/800], Loss: 0.0426\n",
            "Epoch [1889/5000], Step [2/800], Loss: 0.0509\n",
            "Epoch [1890/5000], Step [2/800], Loss: 0.0282\n",
            "Epoch [1891/5000], Step [2/800], Loss: 0.0325\n",
            "Epoch [1892/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [1893/5000], Step [2/800], Loss: 0.0427\n",
            "Epoch [1894/5000], Step [2/800], Loss: 0.0307\n",
            "Epoch [1895/5000], Step [2/800], Loss: 0.0329\n",
            "Epoch [1896/5000], Step [2/800], Loss: 0.0169\n",
            "Epoch [1897/5000], Step [2/800], Loss: 0.0420\n",
            "Epoch [1898/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [1899/5000], Step [2/800], Loss: 0.0224\n",
            "Epoch [1900/5000], Step [2/800], Loss: 0.0358\n",
            "Epoch [1901/5000], Step [2/800], Loss: 0.0326\n",
            "Epoch [1902/5000], Step [2/800], Loss: 0.0335\n",
            "Epoch [1903/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [1904/5000], Step [2/800], Loss: 0.0240\n",
            "Epoch [1905/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [1906/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1907/5000], Step [2/800], Loss: 0.0266\n",
            "Epoch [1908/5000], Step [2/800], Loss: 0.0396\n",
            "Epoch [1909/5000], Step [2/800], Loss: 0.0410\n",
            "Epoch [1910/5000], Step [2/800], Loss: 0.0376\n",
            "Epoch [1911/5000], Step [2/800], Loss: 0.0315\n",
            "Epoch [1912/5000], Step [2/800], Loss: 0.0350\n",
            "Epoch [1913/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [1914/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1915/5000], Step [2/800], Loss: 0.0551\n",
            "Epoch [1916/5000], Step [2/800], Loss: 0.0400\n",
            "Epoch [1917/5000], Step [2/800], Loss: 0.0204\n",
            "Epoch [1918/5000], Step [2/800], Loss: 0.0538\n",
            "Epoch [1919/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1920/5000], Step [2/800], Loss: 0.0405\n",
            "Epoch [1921/5000], Step [2/800], Loss: 0.0419\n",
            "Epoch [1922/5000], Step [2/800], Loss: 0.0395\n",
            "Epoch [1923/5000], Step [2/800], Loss: 0.0238\n",
            "Epoch [1924/5000], Step [2/800], Loss: 0.0343\n",
            "Epoch [1925/5000], Step [2/800], Loss: 0.0288\n",
            "Epoch [1926/5000], Step [2/800], Loss: 0.0261\n",
            "Epoch [1927/5000], Step [2/800], Loss: 0.0222\n",
            "Epoch [1928/5000], Step [2/800], Loss: 0.0251\n",
            "Epoch [1929/5000], Step [2/800], Loss: 0.0281\n",
            "Epoch [1930/5000], Step [2/800], Loss: 0.0231\n",
            "Epoch [1931/5000], Step [2/800], Loss: 0.0305\n",
            "Epoch [1932/5000], Step [2/800], Loss: 0.0381\n",
            "Epoch [1933/5000], Step [2/800], Loss: 0.0357\n",
            "Epoch [1934/5000], Step [2/800], Loss: 0.0287\n",
            "Epoch [1935/5000], Step [2/800], Loss: 0.0534\n",
            "Epoch [1936/5000], Step [2/800], Loss: 0.0237\n",
            "Epoch [1937/5000], Step [2/800], Loss: 0.0439\n",
            "Epoch [1938/5000], Step [2/800], Loss: 0.0436\n",
            "Epoch [1939/5000], Step [2/800], Loss: 0.0311\n",
            "Epoch [1940/5000], Step [2/800], Loss: 0.0265\n",
            "Epoch [1941/5000], Step [2/800], Loss: 0.0385\n",
            "Epoch [1942/5000], Step [2/800], Loss: 0.0392\n",
            "Epoch [1943/5000], Step [2/800], Loss: 0.0256\n",
            "Epoch [1944/5000], Step [2/800], Loss: 0.0318\n",
            "Epoch [1945/5000], Step [2/800], Loss: 0.0369\n",
            "Epoch [1946/5000], Step [2/800], Loss: 0.0346\n",
            "Epoch [1947/5000], Step [2/800], Loss: 0.0295\n",
            "Epoch [1948/5000], Step [2/800], Loss: 0.0365\n",
            "Epoch [1949/5000], Step [2/800], Loss: 0.0304\n",
            "Epoch [1950/5000], Step [2/800], Loss: 0.0314\n",
            "Epoch [1951/5000], Step [2/800], Loss: 0.0249\n",
            "Epoch [1952/5000], Step [2/800], Loss: 0.0289\n",
            "Epoch [1953/5000], Step [2/800], Loss: 0.0356\n",
            "Epoch [1954/5000], Step [2/800], Loss: 0.0545\n",
            "Epoch [1955/5000], Step [2/800], Loss: 0.0437\n",
            "Epoch [1956/5000], Step [2/800], Loss: 0.0302\n",
            "Epoch [1957/5000], Step [2/800], Loss: 0.0276\n",
            "Epoch [1958/5000], Step [2/800], Loss: 0.0415\n",
            "Epoch [1959/5000], Step [2/800], Loss: 0.0301\n",
            "Epoch [1960/5000], Step [2/800], Loss: 0.0313\n",
            "Epoch [1961/5000], Step [2/800], Loss: 0.0306\n",
            "Epoch [1962/5000], Step [2/800], Loss: 0.0267\n",
            "Epoch [1963/5000], Step [2/800], Loss: 0.0361\n",
            "Epoch [1964/5000], Step [2/800], Loss: 0.0542\n",
            "Oh no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLmSNWE5Cpt",
        "colab_type": "code",
        "outputId": "8cca7589-50c8-4f32-9df4-186d83a06add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temp=0\n",
        "batch=20\n",
        "avg_loss_values=[]\n",
        "for i,loss in enumerate(loss_values):\n",
        "  temp+=loss\n",
        "  if i%batch==0:\n",
        "    avg_loss_values.append(temp/batch)\n",
        "    temp=0\n",
        "   \n",
        "plt.plot(avg_loss_values)\n",
        "plt.ylim((0,0.2))\n",
        "plt.show()    "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VfX9+PHXO3tBIIuRBAgkgKCy\nwh4CimK1YBUQtW7rrqOtX/Vra/vVfn+t37pbq+KudW+0KqKIiLLChjCSEEYYSUggIWQn798f9yTc\nhIybEAhy38/H4z6495zPOfl8csJ53888oqoYY4wxPu2dAWOMMScHCwjGGGMACwjGGGMcFhCMMcYA\nFhCMMcY4LCAYY4wBPAwIIjJVRLaISLqI3NfA/t+ISKqIrBORb0Skp9u+q0UkzXld7bZ9mIisd875\ntIhI2xTJGGNMa0hz8xBExBfYCkwBsoAVwGWqmuqWZhKwTFWLReQWYKKqXioiEUAKkAwosBIYpqoH\nRGQ5cAewDPgceFpVv2jzEhpjjPGIJzWEEUC6qm5T1XLgbWC6ewJV/VZVi52PS4E45/15wHxVzVfV\nA8B8YKqIdAM6qupSdUWkfwEXtUF5jDHGtJKfB2ligV1un7OAkU2kvx6o+abf0LGxziurge1HEZEb\ngRsBQkNDh/Xv39+DLBtjjKmxcuXK/aoa3Vw6TwKCx0Tkl7iah85qq3Oq6hxgDkBycrKmpKS01amN\nMcYriMgOT9J50mS0G4h3+xznbKv/A88BHgCmqWpZM8fu5kizUqPnNMYYc+J4EhBWAEkikiAiAcBs\nYK57AhEZAjyPKxjkuO2aB5wrIp1FpDNwLjBPVfcChSIyyhlddBXwSRuUxxhjTCs122SkqpUicjuu\nm7sv8LKqbhSRh4AUVZ0L/A0IA95zRo/uVNVpqpovIg/jCioAD6lqvvP+VuBVIBhXn4ONMDLGmHbU\n7LDTk4n1IRhjTMuJyEpVTW4unc1UNsYYA1hAMMYY47CAYIwxBrCAYIwxxmEBwRhjDGABwRhjjMMC\ngjHGGMACgjHGGIcFBGOMMYAFBGOMMQ4LCMYYYwALCMYYYxwWEIwxxgAWEIwxxjgsIBhjjAEsIBhj\njHFYQDDGGANYQDDGGOPwKCCIyFQR2SIi6SJyXwP7J4jIKhGpFJEZbtsnicgat1epiFzk7HtVRDLd\n9g1uu2IZY4xpKb/mEoiIL/AMMAXIAlaIyFxVTXVLthO4Bvid+7Gq+i0w2DlPBJAOfOWW5B5Vff9Y\nCmCMMaZtNBsQgBFAuqpuAxCRt4HpQG1AUNXtzr7qJs4zA/hCVYtbnVtjjDHHjSdNRrHALrfPWc62\nlpoNvFVv2/+KyDoReUJEAltxTmOMMW3khHQqi0g34Axgntvm+4H+wHAgAri3kWNvFJEUEUnJzc09\n7nk1xhhv5UlA2A3Eu32Oc7a1xCzgI1WtqNmgqnvVpQx4BVfT1FFUdY6qJqtqcnR0dAt/rDHGGE95\nEhBWAEkikiAiAbiafua28OdcRr3mIqfWgIgIcBGwoYXnNMYY04aaDQiqWgncjqu5ZxPwrqpuFJGH\nRGQagIgMF5EsYCbwvIhsrDleRHrhqmF8V+/Ub4jIemA9EAX8+diLY4wxprVEVds7Dx5LTk7WlJSU\n9s6GMcb8pIjISlVNbi6dzVQ2xhgDWEAwxhjjsIBgjDEGsIBgjDHGYQHBGGMMYAHBGGOMwwKCMcYY\nwAKCMcYYhwUEY4wxgAUEY4wxDgsIxhhjAAsIxhhjHBYQjDHGABYQjDHGOCwgGGOMASwgGGOMcVhA\nMMYYA1hAMMYY47CAYIwxBvAwIIjIVBHZIiLpInJfA/sniMgqEakUkRn19lWJyBrnNddte4KILHPO\n+Y6IBBx7cYwxxrRWswFBRHyBZ4DzgQHAZSIyoF6yncA1wJsNnKJEVQc7r2lu2x8BnlDVROAAcH0r\n8m+MMaaNeFJDGAGkq+o2VS0H3gamuydQ1e2qug6o9uSHiogAk4H3nU2vARd5nGtjjDFtzpOAEAvs\ncvuc5WzzVJCIpIjIUhGpuelHAgdVtbK5c4rIjc7xKbm5uS34scYYY1rC7wT8jJ6qultEegMLRGQ9\nUODpwao6B5gDkJycrMcpj8YY4/U8qSHsBuLdPsc52zyiqrudf7cBC4EhQB7QSURqAlKLzmmMMabt\neRIQVgBJzqigAGA2MLeZYwAQkc4iEui8jwLGAqmqqsC3QM2IpKuBT1qaeWOMMW2n2YDgtPPfDswD\nNgHvqupGEXlIRKYBiMhwEckCZgLPi8hG5/DTgBQRWYsrAPxVVVOdffcCvxGRdFx9Ci+1ZcGMMca0\njLi+rP80JCcna0pKSntnwxhjflJEZKWqJjeXzmYqG2OMASwgGGOMcVhAMMYYA1hAMMYY47CAYIwx\nBrCAYIwxxmEBwRhjDGABwRhjjMMCgjHGGMACgjHGGIcFBGOMMYAFBGOMMQ4LCMYYYwALCMYYYxwW\nEIwxxgAWEIwxxjgsIBhjjAEsIBhjjHF4FBBEZKqIbBGRdBG5r4H9E0RklYhUisgMt+2DRWSJiGwU\nkXUicqnbvldFJFNE1jivwW1TJGOMMa3h11wCEfEFngGmAFnAChGZq6qpbsl2AtcAv6t3eDFwlaqm\niUh3YKWIzFPVg87+e1T1/WMthDHGmGPXbEAARgDpqroNQETeBqYDtQFBVbc7+6rdD1TVrW7v94hI\nDhANHMQYY8xJxZMmo1hgl9vnLGdbi4jICCAAyHDb/L9OU9ITIhLYyHE3ikiKiKTk5ua29McaY4zx\n0AnpVBaRbsDrwLWqWlOLuB/oDwwHIoB7GzpWVeeoarKqJkdHR5+I7BpjjFfyJCDsBuLdPsc52zwi\nIh2B/wAPqOrSmu2qulddyoBXcDVNGWOMaSeeBIQVQJKIJIhIADAbmOvJyZ30HwH/qt957NQaEBEB\nLgI2tCTjxhhj2lazAUFVK4HbgXnAJuBdVd0oIg+JyDQAERkuIlnATOB5EdnoHD4LmABc08Dw0jdE\nZD2wHogC/tymJTPGGNMioqrtnQePJScna0pKSntnwxhjflJEZKWqJjeXzmYqG2OMASwgGGOMcVhA\nMMYYA1hAMMYY47CAYIwxBrCAYIwxxmEBwRhjDGABwRhjjMMCgjHGGMACgjHGGIcFBGOMMYAFBGOM\nMQ4LCMYYYwALCMYYYxwWEIwxxgAWEIwxxjgsIBhjjAEsIBhjjHF4FBBEZKqIbBGRdBG5r4H9E0Rk\nlYhUisiMevuuFpE053W12/ZhIrLeOefTIiLHXhxjjDGt1WxAEBFf4BngfGAAcJmIDKiXbCdwDfBm\nvWMjgD8CI4ERwB9FpLOz+1ngV0CS85ra6lIYY4w5Zp7UEEYA6aq6TVXLgbeB6e4JVHW7qq4Dqusd\nex4wX1XzVfUAMB+YKiLdgI6qulRVFfgXcNGxFsYYY0zreRIQYoFdbp+znG2eaOzYWOd9s+cUkRtF\nJEVEUnJzcz38scYYY1rqpO9UVtU5qpqsqsnR0dHtnR1jjDlleRIQdgPxbp/jnG2eaOzY3c771pzT\nGGPMceBJQFgBJIlIgogEALOBuR6efx5wroh0djqTzwXmqepeoFBERjmji64CPmlF/o0xxrSRZgOC\nqlYCt+O6uW8C3lXVjSLykIhMAxCR4SKSBcwEnheRjc6x+cDDuILKCuAhZxvArcCLQDqQAXzRpiUz\nxhjTIuIa5PPTkJycrCkpKe2dDWOM+UkRkZWqmtxcupO+U9kYY8yJYQHBGGMMYAHBGGOMwwKCMcYY\nwAKCMcYYhwUEY4wxgAUEY4wxDgsIxhhjAAsIxhhjHBYQjDHGABYQjDHGOCwgGGOMASwgGGOMcVhA\nMMYYA3hhQNi+/zCLttqzmY0xpj6vCwjPL8rguldXsCu/uL2zYowxJxWvCwhFZVVUVitPf5PW3lkx\nxpiTitcFhNKKKgA+XL2bzP2H2zk3xhhz8vAoIIjIVBHZIiLpInJfA/sDReQdZ/8yEenlbL9CRNa4\nvapFZLCzb6Fzzpp9MW1ZsMaUVlTRMzIEf1/h71ZLMMaYWs0GBBHxBZ4BzgcGAJeJyIB6ya4HDqhq\nIvAE8AiAqr6hqoNVdTBwJZCpqmvcjruiZr+q5rRBeZpVUl5FbKdgrhrdi4/X7CY9p+hE/FhjjDnp\neVJDGAGkq+o2VS0H3gam10szHXjNef8+cLaISL00lznHtqvSyiqC/X25aUJvgvx9efLrre2dJWOM\nOSl4EhBigV1un7OcbQ2mUdVKoACIrJfmUuCtettecZqL/tBAAAFARG4UkRQRScnNPfbhoiXlVQT5\n+xIZFsj14xL4bN1eHvxkAxVV1cd8bmOM+Sk7IZ3KIjISKFbVDW6br1DVM4DxzuvKho5V1Tmqmqyq\nydHR0cecl9KKaoL8fQG465y+/Gp8Av9asoOrX17OweLyYz6/Mcb8VHkSEHYD8W6f45xtDaYRET8g\nHMhz2z+berUDVd3t/HsIeBNX09RxV1pRRZC/q9i+PsIDFwzg0ZmDSNl+gF/880cOl1WeiGwYY8xJ\nx5OAsAJIEpEEEQnAdXOfWy/NXOBq5/0MYIGqKoCI+ACzcOs/EBE/EYly3vsDFwIbOAFKKlx9CO5m\nDIvjuSuHkrn/MF9u2HcismGMMSedZgOC0ydwOzAP2AS8q6obReQhEZnmJHsJiBSRdOA3gPvQ1AnA\nLlXd5rYtEJgnIuuANbhqGC8cc2maoaqUVlQRHOB71L5J/WLoERHCh6uzjnc2jDHmpOTnSSJV/Rz4\nvN62B93elwIzGzl2ITCq3rbDwLAW5vWYlVdVU63U9iG4ExEuHhrLU9+ksedgCd07BTd7vq3Zh+ga\nHkTHIP/jkV1jjDmhvGqmcmmFayRRQwEB4OIhcajCx2vqd5E0dK4qpv/jB55ZkN6meTTGmPbiZQHB\ntWxFTadyfT0iQxjRK4IPVmbhdIE0al1WASUVVazfXdDm+TTGmPbgVQGhpNwVEOp3Kru7eGgsGbmH\nWZfV9I1+xfZ8ADbvO9Rs8DDGmJ8CrwoIpZU1NYTGA8LPzuxGgJ8PH65qunM5xQkI+YfLyS0qa7tM\nGmNMO/GqgOBJDaFjkD/nDujC3LV7KC5veE5CdbWycscBekWGALB576G2z6wxxpxgXhUQmutUrnH5\niB4cKK5g0qMLeXPZTirrLWuRllNEYWklV4zsCcCWfad+QPjLF5v4MWN/e2fDGHMceVlAaLpTucaY\nxCjevWk0sZ2C+e+P1nPuE4vqPGEtZYeruejcgV2I6RDIpn2Fxy/TJ4GS8iqe/24b76XYHA1jTmVe\nFRBKnIDQ0MS0+kYkRPDBLWOYc+Uw9hSU1HnCWsr2A0SFBdIjIoT+3ToeVUP409yN/PdH69s28+0o\n64ArGG7NPvVrQsZ4M68KCLU1BL/mAwK4JqudO7Ars5Lj+XjNbrILSwHXCKPhvTojIpzWtQNpOUW1\nzUrF5ZW8tXwnby7byfpmRir9VOx0akfpOUVUVduIKmNOVV4VEFpSQ3B3w7jeVFUrL/+Qyb6CUrIO\nlDCsZ2cA+nXtQHllNdvzXI/jXLR1P2WV1fj6CI/P39K2BWgnNQGhrLK6trZgjDn1eFVA8LRTub4e\nkSH87IxuvLl0Jwu3uB7sNrxXBAD9u3YEYJMz0mh+ajYdg/y48+wkvt2Sy8odB9oq++1mp1v/ydZs\ne8KcMacqLwsInnUqN+SmCX04VFbJX7/cTLC/LwO6uwJBn5hQfH2ELfsOUVlVzYLN2UzuH8P14xKI\nDA1otJbwxrIdXPr8EgpKKjzOQ8r2/GaX5162LY8f0tt2NNCu/GLiOrvWdrJ+BGNOXV4VEErKq/AR\nCPBtebHPiAtnbGIkB4srGBzfCX/nHIF+vvSJDmXzvkJW7jjAgeIKpgzoSmigH7dM7MMP6Xksycir\nc65nF2bwwEcbWJaZzz8WpDX04+qorlb+8vkmZjy3hL9+sbnRdF+nZnPFi8u48+3VVLdhW//O/GIG\ndOtI9/Ag0iwgGHPK8qqA4Ho4ji+NPK2zWTdO6ANAcq/Odbb369qRzfsOMT81mwBfH87q53qy2y9H\n9SSmQyC/e28tT8zfyobdBTz21RYe+XIzPx/UnUuGxvHqj9vJ3H+4yTzf8fZqnl+0jaiwQD5es7u2\npuNu0dZcbn1jFSEBvuwvKm92KKynncOqys78YnpEhJDUxdWBbow5NXlVQGjo4TgtMSEpiv+75Eyu\nGt2rzvb+XTuQdaCEz9btZXSfSMICXauKB/n78uSlg+neKYi/L0jjwr8v5u8L0pk9PJ4nLx3Mvef3\nI8DXh//3+aYGf15pRRVXvbycz9bt5YGfncZTswdzqLSSeRvrPsRn2bY8bnw9hT4xYbx/yxgAvk9r\nuNmoqlp56NNUBj/0Ve16TE3JLSqjtKKaHpEh9O0SZiONjDmFefQ8hFOF+/OUW0NEmDU8/qjt/bt2\nAGBfYSm3T06ss29MYhRjEqPIKypjweYcFJg5LA4RIaZDELdNTuT/vtzCD+n7GZsYVefYV3/czvLM\nfJ64dBC/GBJHdbUS1zmY91KymD44FoCC4gpufWMVsZ2Cef36EUSFBdK/awcWbc3l5rP61DnfodIK\n7nhrNd9uySU82J8bXkvhg1vGkBgT1miZaybkxUeEEOTnS1llNbvyi+kVFdri358x5uTmVTUE9+cp\nt6X+3TrWvp8yoEuDaSLDApmZHM+s5Pg6TVbXjU0gPiKYhz9LpcJtiYz9RWX8Y0E655wWwy+GxAHg\n4yPMHBbPDxn7a2/Uj8zbzMGSCv5+2VCiwgIBGJ8URcr2A3XWYtpbUMKMZ5ewKG0/f77odD69fRz+\nvsI1rywn51Bpo2WrGWHkajJyBQ73juUV2/M9qmk0ZFd+MXe8tbrOLPCTTUVVdZv2xxhzMvOqgFDS\nyOMzj1X38CA6BPkxKC6cLh2DWnRskL8vv79gAJv3HeKe99bW3nwen7+V0ooq7v/ZaXXSXzLMVTP4\nYFUWq3ce4K3lO7lmTK/aUU8A45OiKa+qZlnmkRv1/8xNZdeBYl67dgS/HNWTHpEhvHzNcPKKyrn+\n1ZRGRzvtzCtBBGI7BZPUxVUTqulHKK+s5pZ/r+Lud9a0eAnw0ooqbv73Suau3cMTX29t0bEnSnW1\nMv0fP/DAxyfkcd9tSlXZtLfQlmY3LeJRQBCRqSKyRUTSReS+BvYHisg7zv5lItLL2d5LREpEZI3z\nes7tmGEist455mlpbU9vC5RWVHk8S7klRISHp5/Of9e7eXvqvIFduee8fny8Zg8Pzt3A5n2FvL18\nJ78c1ZM+0XWbc+I6hzAuMYr3UrJ44KMNdOkQxN1T+tZJMyIhgkA/H77f6upHSN1TyJcb93HD+N6M\nSzrSLHVmXCeeuWIIm/YW8otnfiAj9+gO4535xXTtGESQvy9hgX7EdgquHWn0xYa97C8qI+tACZtb\nuMDfg59sYOOeQob36swna/awM6/xWoKqtvrGtnbXQX7z7hp+zNjf6DlUlY17CiivrLuI4cKtOaTu\nLeSztXuO2lej2umTueiZHxpN01oHDpe3aFiyu4Vbcjn/qe+Zu3ZPm+appQqKK5jy+Hd8n5bbrvkw\nnmk2IIiIL/AMcD4wALhMRAbUS3Y9cEBVE4EngEfc9mWo6mDndbPb9meBXwFJzmtq64vhmeNVQwC4\naEgsI3tHtvr42yYlcvNZffj30p1c/sIyOgT5c9c5SQ2mnZkcz+6DJaTuLeTBnw+o7cSuEeTvy4iE\nCBY5/wmf+mYrHQL9uH5swlHnmty/C2/+ahQFJRVc9I8f+NaZeFdjV34x8REhtZ+TuoTVTk577cft\ndAsPQgS+2pjtcVnfXr6Td1Oy+PXkRJ65fCi+PsI/Fzb8KNIdeYeZ/Nh3PPZVy2sRSzLyuPyFpXy4\najeXv7CMS579kfmp2XWa0tZnFXD5C8u44OnF/OnTjXWOf2FRJv6+wqGyygZXei2vrObOd9bw8g+Z\nrNl1kG82ef478MRNr6/kiheXtqoT/8sNroEHT32ddtRqvSfS5xv2kpZTxEerm38srWl/ntQQRgDp\nqrpNVcuBt4Hp9dJMB15z3r8PnN3UN34R6QZ0VNWl6vra9i/gohbnvoWOtVP5eLt3aj8uH9mD/MPl\n3HF2Ep1CAhpMd+6ALkSEBjCxXzTnn961wTQTkqJJzynim03ZzNuYzbXjEggP8W8w7YiECD65fSzx\nESFc9+oKlm07Mm9iZ34x8Z3dAkJMGBm5RazddZBVOw9yw/jeDOvRma9S9zV06qP8mL6fBz/ZyPik\nKO46py8xHYO4bHg876/MOmpZjB15h7lszlIy9x/mpcWZFBR7/m352805XPPKcrp3CmbRPZN4ePpA\nsgvL+NW/Ujj9j/M474lFXPnSMn7+j8VsyT7EmD6RvLV8J2t3HQRgw+4ClmzL447JSYQG+DKvXsAr\nKa/ixtdT+HTtHu45rx/dw4N4c/lOj/PXnOpqZcOeAjbsLmz2YU0NHfvN5my6hwexbf9hPlnTfrWE\nj51A8H1a4zU0c/LwJCDEArvcPmc52xpMo6qVQAFQ83U5QURWi8h3IjLeLb37X3lD5wRARG4UkRQR\nScnNPbZqZ808hJNVTdPTh7eO4doxvRpNF+Tvy+d3jOfZK4Y1OqdiQl/XXIi731lDhyA/rh93dO3A\nXVznEN67eTQdg/x5y7mxlVZUsa+wlB51aggdKKus5n//s4lgf19mDItjyoAubNxTyO6DJbXpSiuq\nSM+p+3jRj1ZncfUry+kZGcJTs4fg6+PK+01n9UEEnvsuozZtTTAorqjisZmDKKmo4u0Vnt1wl2S4\nhuEmdQnjnZtG0yMyhCtH92LhPRN5+Zpkbp+cRPdOQezML+aWiX1YeM9Enr9yGFFhgfzhkw2udasW\nZxIa4MtVY3oxsX8M81P31fmmft+H61i0NZe/XnwGt01KZNbweL5P299mHeR7CkooLq/Cz0d49Kst\ntQ938sTqXQfZX1TOvef3Z2D3jjy9IK12wMLmfYVc+PfveXlxZpvksyl7DpawfHs+faJDyT1U1uJm\nRXPiHe9O5b1AD1UdAvwGeFNEOjZzTB2qOkdVk1U1OTo6+pgyU1JeRfBxGGXUlnx9hKE9OuPj03SX\nStfwoCabv/p2CSOmQyCFpZVcNzaB8OCGawfuQgP9uODMbszbmM3hskqyDrhu8D0ig93O6+pYXr49\nn4uHxhIe7M+5A121lPnO/AhV5ddvreacxxcx9cnvefH7bTw+fyt3v7OW5J4RvH/LGCJCj9R+uncK\nZmZyPO+uyOL+D9cx87kf+dlT31NcUcWbN4zikmFxjO4dyWs/bveo+eO9lbsIC/TjzV+NqvNz/H19\nmNy/C7+Z0pdXrh3Bd/dM4t6p/ekY5E+HIH9+f8FprMsq4KmvtzJ37R5mDY8nPNifqQO7sr+onFU7\nXetSrcs6yCdr9nDrxERmj+gBwKzkeHwE3lmxq8E8tVSa0yz323P7kV1Yxgvfb/P42K83ZePnI0zs\nF8NvpvRlR14xH63azY8Z+5n57BK27DvEQ5+l8sy3R5rpMnKLuOG1FW0aKD5duwdV+H+/OANwTZ5s\nrR15h/nDxxu49pXldUbjeUJVWbglh1++uIz7P1zf5Ki6xnyxfi/nPbGIdA8nZq7ccYAL//49f/1i\n809q3o4nd8fdgPvg+zhnW4NpRMQPCAfyVLVMVfMAVHUlkAH0ddLHNXPONldaeXLXENqSiHD2aTGE\nB/tzXTO1A3e/GBJLSUUVX6Xuq/22W6eG4DZnoWaCXkJUKEkxYXyV6mpW+XTdXuanZnPR4O4EB/jy\n5/9s4ulv0rh4SCyvXTeiweB0y1l98PcVvnDavqcNjuXdm0bXjp66blwCewpK+dJtUt7W7EOk7qk7\nI1tVWZqRx+g+kXQMaj4Iups2qDujekfw9IJ0qlW5dozr9zaxXzQBvj617fKPfLmZiNAAbjqrd+2x\n3TsFM6lfDO+m7PLohvVj+v4GZ5zXSMtxfZu+bEQ8Uwd25bnvMsgp9OxG9nVqNiN7RxAe7M/k/jEM\nigvnr19u5uqXl9M1PIgFv53IRYO787d5W3h8/lb+sSCN85/8nq835fD60h0e/Yz6CoorePWHzDr9\nM5+s2cPg+E6M7B1J3y5hjU6WdKeq/GNBGjOf+5Ffv7Wav3yxiVvfWMmkRxfy+tIdfLsll3VZB+sc\nU1xeyRPztzb4yNs1uw4ye85SrnllBek5Rby/cheT/raQZ75Nb/L37y6nsJT7PlzPluxDXP/aCvIP\nlzeatryymr/N28zM535kV34Jz32XwY3/SqGomTXIThaeBIQVQJKIJIhIADAbmFsvzVzgauf9DGCB\nqqqIRDud0ohIb1ydx9tUdS9QKCKjnL6Gq4BP2qA8TXLVELwjIAD8/oIBzLtrgke1gxrDenQmrnMw\nH63eUzsHwb1TOTTQj4SoUMb0iaSfMyEPXPMvlmXmk5FbxB8/2cCg+E48NmswH982lvl3T+DVa4fz\n2KxBBPg1/CcXHxHC2j+ey+o/TOG9m8fwl4vPqK2NAJzdP4aekSG8vDgTVeWNZTu44OnvufbV5XXm\nCezKL2FPQSmjW9HBX9Nk5+cjnDewKz2cZ2Z3CPJnbGIk8zbuY9HWXH5Iz+P2SYl0qBdwLhvRg5xD\nrgmITdm0t5DLX1zGU980vo5VWnYR0R0C6RQSwH3n96eiqtqjjvXt+w+TllPEOad1qS3T3VP6kn+4\nnKE9OvP+zWOIjwjhsVmDmTEsjqe/SePRr7Zy7sAu3HxWHzL3H2ZvQUkzP6UuVeX+j9bxp09TueXf\nqyivrCYt+xCpewuZPrg74OrTWr49v8mmL1Xl4c828ehXWykqq2Jd1kFeXpzJ92n7uemsPnx19wRE\nYHFa3bXBPl27h6e+SasN2DUKiiuYPWcJGblFPDR9IIv+axJf3X0WYxOj+Nu8LVz1UvO1DVXl9x9v\noKSiikdnDmJvQSk3v76Sssqjy1FaUcWM537kmW8zmDksnsX3uvquFm7N5ZJ//sjyzOYXp2xvzc5U\nVtVKEbkdmAf4Ai+r6kYReQhIUdW5wEvA6yKSDuTjChoAE4CHRKQCqAZuVtWawfG3Aq8CwcAXzuu4\nqa5WyipP7k7lthYa6EdoYMu5qmU0AAAVjUlEQVQmo/v4CBcNjuWfC9PpFOxPkL8P0c6EtxqvXDOc\nDkF1z3vuwK78c2EGV7ywjMNlVTw648zaPoKkLh1q5zA0xa+JRQd9fIRrx/TiT5+mct2rK/h2Sy5x\nnYPJOlDCut0FDI7vBMBSp0N8VCtHfCV16cCHt44hzq0jHWDq6V359oP13PP+WuI6B3PFqB5HHTux\nXzRdOwbx0uJMDpdVsn3/YYrKqrjnvH51mvdqlh55c9lO7pic1GDTX1pOUW1trFdUKNeM6cUL32cy\na3gcw3pG1Kb7dnMOb6/YyZ+mDaRbeDBfOyOdagKCK18xfPbrcSR1CSPQGXbt6yP83yVn0r9rB3pH\nhzK5fxc27C7gue8yWJKRx8VD3SvwTft03V4+X7+P8UlRfLc1l9++5/od+QhceKYrIIzvG82LizNZ\nlpnHxH4xVFZV89v31lJRVc20QbFM7BfNX7/YzKs/bufasb148MIBiAjV1Uq1au3fxundw/khfT93\nuo3A+2aTKwAv25ZfJ99LM/MorajmtWuH1o4ATIgKZc5Vyby/MovfvbeWR77YzO8vrD9osm7ZvkrN\n5r7z+zNjWBwBfj7c8dZq7v9gPY/NGlSnD+/z9XtZl1XAozMHMWOYKx9Xju5FQlQYt76xklnPL0EE\nEiJDSYwJo0dECPERIUzoG03CSTLz36O7hap+Dnxeb9uDbu9LgZkNHPcB8EEj50wBTm9JZo9FWWXr\nnoXgjS4a0p1/fJvOZ+v2kBgTdlTHdUPLVpwZG05Mh0D2FZZyz3n9PAoALTUjOZ7HvtrKt1tyuX1S\nIteNS2D4/37N/NR9dQJCVFhAk8txNOfMuE5HbTvntC74yHqyC8t48tLBtTdWd36+Plw6PJ6nvklj\neWY+IqAK/bqGcenwIwFk3sZsojsEknuojA9WZfHLUT3rnEdVSc8p4pKhR8ZZ3HVOXz5fv4/7P1zP\nZ78eT4CfD9tyi/j1W6spKqtkza6DvHT1cL5KzaZ/1w51anUAp8eGH5VfHx/hhvFHmr0GdOtIeLB/\niwJCTmEpf/h4A4PjO/HKNcN5cXEmf/1iMz4CYxOjiO7g+jIxMiGCAD8fFm3dz8R+MTz5dRqfrNlD\npxB/Pl+/j0A/H8oqq7lhXAIPXHBa7d+cj4/gw5G/v7GJUby0eBuHyyoJDfSjrLKKxc5y78sy69Yc\nlmTkEeTvw+AeR1/PGcPiWJ91kBcXZzK0Z2d+dka3o9LsLyqrre3e4DS7ThvUnczcwzzxtatWNfX0\nI8e9sWwnvaNC61w3gHFJUSy8ZxIrdxwgdU8hG/cUsG3/Yb7bmktZZTWdQvz57p5JdWryu/KL+Xz9\nXi4f2eOomujxdHL3sLah2qelneSdyieDxJgOnBEbTrXW7T9oio+PcMXInoxNjOTGCb2bP6AVwgL9\neO7KYbxxw0h+d14/IkIDGN6rc+0cCFVlybY8RvaObPWKto2JDAtkXFI0Z8SGM21Q90bT3TKxD/++\nfiTf/PYstjx8Pn2iQ3k35ciAul35xWzaW8iN43tzZlw4r/yQedTSGPsKSykqq6wTVEMD/Xj4ooFs\nzS5izqIMSsqruPWNVfj7Ci9fk4yvCLOeX0LK9vxGl09pjo+PMKp3BEu25TWfGNfv+74P11NaUcVj\nswbh5+vDzWf14aazelOtcIlbUAny92VkQgTfp+WyaGsuzyxMZ1ZyHCkPnMNr143g54O6c895/eoE\ng4aMTYykokpZ7iyXsmxbPsXlVYxNjGR7XjH7Co70syzdlkdyz4gGgzfAAxcMYEiPTtzz3toGO4uf\n/y6DQ6WV/G3GmXVqr7dN6kPvqFCemJ9We+027XUtf3/5yB4N5j8iNIApA7pw5zlJzLkqma9/cxab\nH57KB7eM5mBxRZ0RdtXVyh1vr+YvX2xmyuOLmJ/atvNbmuI1d8cjD8exGoInatp+63/TbMqd5yTx\nxg2jap8VcTyMTYyqswjguQO6kpZTxPb9h9mZX8zegtJWNxc1Z86Vw3j3ptFNjgAL8vdlXFIUfaLD\nCPDzYVZyPCt3HKi94dQ0F503sCvXjU0gI/cw39WbxVszwiipXi1ncv8uXHBGN55ekM7tb65iS/Yh\nnrh0MJP7d+Hj28bSJzqManX9TlprdO9Isg6U1Bk++8ma3Q0+h+PTdXtZsDmHe6f2rzOj/r6p/fn8\njvG1f0M1JiRFk5ZTxB1vryYpJoz/mXY6fr4+nNU3mkdnDuK2SYnNBvLhvVw1jR+dWsGCzTkE+ftw\n1zmu2fo1tYS8Itcw19F9Gv9bCPDz4Z9XDCXI35d7P1hXZ5+q8vn6fZzVN7pOXxa4aoJ3npPEluxD\ntYMg3li2g0A/n9qmIk+ICMN6RnDR4O68vDizNpi9uXwnq3ce5JaJfegU4s+v/pXCLf9e2arRUS3l\nNQGhtc9T9lbTBncnJMCXgd2Pbmo4mdR8G56fml3bfzC6d0RTh7RakL9vi/9+fjE0Fl8f4b2VruGo\n8zbuo3/XDrWPZY3pEHjUUM+ataIaanb7488HEOjnwzebc/j1pEQm9osBIKZjEO/eNJqPbh3DGXGt\nv2aj+7iCbU0t4VBpBX+cu5HnvsuonbQHrhvmMwvS6dsljGvqzZkREQZ073jUzb1mbkxZRTXPXD60\nVf8Xg/x9GdajM4vT81B1TcAb2yeKoT060yHQj6XbnJqDs45Xc18OuoUHc8vEPnWCNsC6rAJ2Hyzh\n/AaaksDVN5IYE8aTX2+lsLSCj1bt5sIzuzc6mbQpvz23H6rw5NdbyTlUyiNfbmZ070j+67x+fPrr\ncdxzXj+WbsujrOL4zzj3moBgNYSWiekQxNL/PpuLhzQ4X/CkER8RQv+uHfgqdR9LMvKICgs8av2n\n9hTTIYjJ/WP4YOVusgtLSdlxgPOceRsBfj5cPaYX36ftr7OCbHrOISJDA+rMoag9X8cgnpg1mOvG\nJnDnOXXXsAoO8GVIj85HHdMSfbuEERkawFLnKX8vL97OweIKQgJ868xbWLglly3Zh7hpQp9m58y4\nn3v64O48PmvQMfUxjUuKYtPeQpZl5rMrv4TJp8Xg6yMk9+pcW0NYkpFHSIAvZ3oQHKcPdgXtD9xm\nhH++YS9+PsKU0xpufvP1Ee48O4m0nCJue2MVh8urGhxo4In4iBB+Oaon76bs4tdvrqasopo//+J0\nRAR/Xx9um5TID/dNblFtvbUsIJhGdQzy9/g/e3s6d0AXVu44wHdbcxnVO6LN+w+O1azkePYXlfHA\nRxtQpTYggGuoaqCfDy99f6SWsDW7qMlO8XMGdOHBnw+oHcXVlkSEUX0i+TEjj4LiCl5cvI1zB3Th\nhvG9+So1uzZwPftdBt3Dg5g2uPH+lIbO/dTsIY1+6/ZUTZNhzYOlJjm1pJG9I9mWe5icQ6Us2ZbH\n8F4RHjVfRncIZGLfaD5clUVVtWshxS/W72NsYlSjy70AXHBGt9r5Fad168iQ+KM7rz11++REQgL8\nWJaZz62T+hz1pSYk4MQ8usZrAkJJuau65U3zELzFlAFdqVY4UFxx3PoPjsXEftFEhQXy9aZs4joH\nc1q3I9+OI0IDuGRYHB+t2c3+ojJUlbTsQ7XPnmgPo3tHsq+wlAc+Xs+h0kruntKXa8f0IiTAl39+\nm86qnQdYnpnPdeMSjmt/UWPOiA2nQ5Af67IKOK1bR7p3cs2kH5ngair8z7q9pOcUNdl/UN8lw+LI\nLixjcfp+Nu4pZGd+MT87o+m+GB8f4W6nlnblqJ7H9EUkIjSAP1x4GhP7RXPLxD7NH3CceE1AOFJD\n8Joie43TYzvSLdz1HIqW3AROFH9fn9rnWJw3sOtRN47rxyVQXlnN60t2kHuojMLSSpJi2n7Yrqdq\nfoefrdvLBWd047RuHekcGsAVI3swd+0e/vxZKuHB/lw2onVNJMfK10dqJx5O7n9kOZvTY8MJCfDl\n2YWuETstmZxYM6v/g5VZfLFhL74+whQPOuennt6V928ezaUNPEmxpS4d3oNXrx3R6KioE8Fr7o5H\nhp1aDeFUIyL8YkgsvaND6X2STPCp7/IRPegZGVJnKGaNPtFhnN0/hn8v3cH63QUA7VpD6B0VSkyH\nQESoswT7r8b3xs/Hh1U7D3LV6J4tnvTYlsY7z/U4262N39/Xh+ReEeQcKqNDoB8Du3u+bFqgny/T\nBnVn3sZ9fLx6D6N7RzbYh1OfiJDcK+K4NN+1B68JCNaHcGr73bn9mHfXhJOu/6BGz8hQvrtnUp0n\n27m7fnwCeYfLa5enaM8agohw01l9uOvsvnU6f2M6BjF7RDwhAb5c3cRqvCfCpcN78Np1IxharxO9\nptloZO+IJme+N2TGsDjKKqud0UWtH7r7U9Z+If4Es4Bwaqs/o/WnZnTvSAZ278jGPYV0CvEnKqzl\nwxfbUmPLpT9wwWncOjGx9vnd7SXAzzV/ob6aPqTW9CWdGRdOYkwY23KLjmkux0+ZF9UQnE5lm4dg\nTkIiwg3jXTfhpAaWCzlZBPr50jW8Zc8NP5GG9ujE47MGtap/Q0R44ILTuHdq/9olN7yN1wSEmj6E\noEZW2zSmvV1wRnd6RoYc1QxiPCciXDw0rtX9G5P6xXDTWe03yqe9eU2TUUlFFf6+0uJ2RWNOlAA/\nH+bdNaFdhnIaA14UEEorqghqx+FcxnjC+rhMe/KaryKlFVUEWf+BMcY0yosCQrXNQTDGmCZ4TUAo\nKa+yWcrGGNMEr7lDllR41/OUjTGmpTwKCCIyVUS2iEi6iNzXwP5AEXnH2b9MRHo526eIyEoRWe/8\nO9ntmIXOOdc4r5i2KlRDSiuqCLSAYIwxjWp2lJGI+ALPAFOALGCFiMxV1VS3ZNcDB1Q1UURmA48A\nlwL7gZ+r6h4ROR2YB7gvsH+F82zl4660oqpVD68wxhhv4UkNYQSQrqrbVLUceBuYXi/NdOA15/37\nwNkiIqq6WlX3ONs3AsEi0i5TAK1T2RhjmuZJQIgFdrl9zqLut/w6aVS1EigA6i8mcgmwSlXL3La9\n4jQX/UGO81z9kgrrVDbGmKackDukiAzE1Yx0k9vmK1T1DGC887qykWNvFJEUEUnJzc1tKIlHSiqq\nbB0jY4xpgicBYTfg/vSHOGdbg2lExA8IB/Kcz3HAR8BVqppRc4Cq7nb+PQS8iatp6iiqOkdVk1U1\nOTr66NUNPVVaUdWuD54wxpiTnScBYQWQJCIJIhIAzAbm1kszF7jaeT8DWKCqKiKdgP8A96nqDzWJ\nRcRPRKKc9/7AhcCGYytK00qthmCMMU1qNiA4fQK34xohtAl4V1U3ishDIjLNSfYSECki6cBvgJqh\nqbcDicCD9YaXBgLzRGQdsAZXDeOFtiyYu8qqaiqq1DqVjTGmCR4tbqeqnwOf19v2oNv7UmBmA8f9\nGfhzI6cd5nk2j01ppetZCNapbIwxjfOKO2RJuT1P2RhjmuMVAaHm8Zk2U9kYYxrnVQHBagjGGNM4\nLwkIzvOULSAYY0yjvCIg1D5P2QKCMcY0yqsCQnCAVxTXGGNaxSvukLWdyjZT2RhjGuVVAcFmKhtj\nTOO8KyBYH4IxxjTKKwJCzcQ061Q2xpjGeUdAsGGnxhjTLK8ICEc6lb2iuMYY0ypecYd0PQvBBx+f\n4/pQNmOM+UnzmoBgI4yMMaZpXhEQSiqqCLI5CMYY0yQvCQjVVkMwxphmeEVAqOlDMMYY0ziPnpj2\nU9elYyABFhCMMaZJXhEQ/nzRGe2dBWOMOel59LVZRKaKyBYRSReR+xrYHygi7zj7l4lIL7d99zvb\nt4jIeZ6e0xhjzInVbEAQEV/gGeB8YABwmYgMqJfseuCAqiYCTwCPOMcOAGYDA4GpwD9FxNfDcxpj\njDmBPKkhjADSVXWbqpYDbwPT66WZDrzmvH8fOFtExNn+tqqWqWomkO6cz5NzGmOMOYE86UOIBXa5\nfc4CRjaWRlUrRaQAiHS2L613bKzzvrlzAiAiNwI3Oh+LRGSLB3luSBSwv5XH/pRZub2Lt5YbvLfs\nnpS7pycnOuk7lVV1DjDnWM8jIimqmtwGWfpJsXJ7F28tN3hv2duy3J40Ge0G4t0+xznbGkwjIn5A\nOJDXxLGenNMYY8wJ5ElAWAEkiUiCiATg6iSeWy/NXOBq5/0MYIGqqrN9tjMKKQFIApZ7eE5jjDEn\nULNNRk6fwO3APMAXeFlVN4rIQ0CKqs4FXgJeF5F0IB/XDR4n3btAKlAJ3KaqVQANnbPti1fHMTc7\n/URZub2Lt5YbvLfsbVZucX2RN8YY4+1sPQdjjDGABQRjjDEOrwgI3rJMhojEi8i3IpIqIhtF5E5n\ne4SIzBeRNOffzu2d17bmzIBfLSKfOZ8TnGVU0p1lVQLaO4/Hg4h0EpH3RWSziGwSkdFecr3vdv7G\nN4jIWyISdCpecxF5WURyRGSD27YGr6+4PO2Uf52IDG3pzzvlA4KXLZNRCfxWVQcAo4DbnLLeB3yj\nqknAN87nU82dwCa3z48ATzjLqRzAtbzKqegp4EtV7Q8MwvU7OKWvt4jEAncAyap6Oq6BKbM5Na/5\nq7iW/XHX2PU9H9dIziRck3mfbekPO+UDAl60TIaq7lXVVc77Q7huDrHUXVrkNeCi9snh8SEiccAF\nwIvOZwEm41pGBU7BMgOISDgwAdcoP1S1XFUPcopfb4cfEOzMewoB9nIKXnNVXYRr5Ka7xq7vdOBf\n6rIU6CQi3Vry87whIDS09EZsI2lPGc6Ks0OAZUAXVd3r7NoHdGmnbB0vTwL/BVQ7nyOBg6pa6Xw+\nVa95ApALvOI0l70oIqGc4tdbVXcDjwI7cQWCAmAl3nHNofHre8z3Om8ICF5HRMKAD4C7VLXQfZ8z\nYfCUGWssIhcCOaq6sr3z0g78gKHAs6o6BDhMveahU+16Azht5tNxBcTuQChHN6t4hba+vt4QELxq\nmQwR8ccVDN5Q1Q+dzdk1VUfn35z2yt9xMBaYJiLbcTUHTsbVrt7JaU6AU/eaZwFZqrrM+fw+rgBx\nKl9vgHOATFXNVdUK4ENcfwfecM2h8et7zPc6bwgIXrNMhtN2/hKwSVUfd9vlvrTI1cAnJzpvx4uq\n3q+qcaraC9e1XaCqVwDf4lpGBU6xMtdQ1X3ALhHp52w6G9eqAKfs9XbsBEaJSIjzN19T7lP+mjsa\nu75zgauc0UajgAK3piXPqOop/wJ+BmwFMoAH2js/x7Gc43BVH9cBa5zXz3C1qX8DpAFfAxHtndfj\nVP6JwGfO+9641s1KB94DAts7f8epzIOBFOeafwx09obrDfwPsBnYALwOBJ6K1xx4C1c/SQWuGuH1\njV1fQHCNqMwA1uMahdWin2dLVxhjjAG8o8nIGGOMBywgGGOMASwgGGOMcVhAMMYYA1hAMMYY47CA\nYIwxBrCAYIwxxvH/AQoX6aTrcBI3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYJvQSZBnVyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.modules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAAvkWGhto3Z",
        "colab_type": "text"
      },
      "source": [
        "##Setting up Test Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caESXMB9tohO",
        "colab_type": "code",
        "outputId": "64c80684-27e5-488a-c6c3-77a5c360d358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "label_encoders={}\n",
        "for cat_col in categorical_features:\n",
        "  label_encoders[cat_col]=LabelEncoder()\n",
        "  val_data[cat_col]=label_encoders[cat_col].fit_transform(val_data[cat_col])\n",
        "  \n",
        "val_data=val_data.drop(\"Weights\",axis=1)\n",
        "val_data_org=val_data\n",
        "val_data=val_data.drop(\"id\",axis=1)\n",
        "\n",
        "val_dataset=TabularDataset2(data=val_data,cat_cols=categorical_features,\n",
        "                      output_col=output_feature)\n",
        "val_dataset[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0.], dtype=float32),\n",
              " array([41.  , 62.17, 41.2 ], dtype=float32),\n",
              " array([0, 0, 0, 1, 3, 0, 1])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTzTD-9117i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchsize=1454\n",
        "val_dataloader=DataLoader(val_dataset,batchsize,shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOWrIfF4FvW1",
        "colab_type": "code",
        "outputId": "523242ab-f80a-4811-fc32-e33a56bff39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "prec_values,rec_values,f1_values=[],[],[]\n",
        "\n",
        "with torch.no_grad():\n",
        "  loss,i=0,0\n",
        "  \n",
        "  for y, cont_x, cat_x in val_dataloader:\n",
        "    cat_x = cat_x.to(device)\n",
        "    cont_x = cont_x.to(device)\n",
        "    y  = y.to(device)\n",
        "    preds=model(cont_x, cat_x)\n",
        "    #preds=torch.round(preds)\n",
        "#     print(y[:5])\n",
        "    #loss+=criterion(preds,y)\n",
        "    loss=weighted_binary_cross_entropy(preds,y)\n",
        "    i+=1\n",
        "    precision,recall,fscore,_=precision_recall_fscore_support(y, torch.round(preds), average='macro')\n",
        "    prec_values.append(precision)\n",
        "    rec_values.append(recall)\n",
        "    f1_values.append(fscore)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  print('Loss of the model: {} '.format( loss / i))\n",
        "torch.save(model.state_dict(),'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss of the model: 1.387656331062317 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eog81IDaOrAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhyWTIrI7ngb",
        "colab_type": "code",
        "outputId": "967f5029-0efd-4c6d-c75b-ef05a3bb4e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scikitplot as skplt\n",
        "# y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
        "# y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
        "\n",
        "# print (y)\n",
        "# print (preds)\n",
        "\n",
        "y_true = y.tolist()\n",
        "y_pred = torch.round(preds).tolist()\n",
        "print (precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
        "\n",
        "print (accuracy_score(y_true, y_pred))\n",
        "print (accuracy_score(y_true, y_pred, normalize=False))\n",
        "\n",
        "#skplt.metrics.plot_roc_curve(y_true, y_pred)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.3115543328748281, 0.5, 0.3838983050847458, None)\n",
            "0.6231086657496562\n",
            "906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function plot_roc_curve is deprecated; This will be removed in v0.5.0. Please use scikitplot.metrics.plot_roc instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-43989d316237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mskplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scikitplot/metrics.py\u001b[0m in \u001b[0;36mplot_roc_curve\u001b[0;34m(y_true, y_probas, title, curves, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         fpr[i], tpr[i], _ = roc_curve(y_true, probas[:, i],\n\u001b[0m\u001b[1;32m    258\u001b[0m                                       pos_label=classes[i])\n\u001b[1;32m    259\u001b[0m         \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBgo8gnOGJY",
        "colab_type": "code",
        "outputId": "5437397d-2489-491d-dab2-afed29c42c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (len(rec_values))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w5av744MOrC",
        "colab_type": "code",
        "outputId": "6fbb10cc-89fb-4e1b-a89b-f89cc34ba4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average precision-recall score: 0.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDfgFQfuRjCD",
        "colab_type": "code",
        "outputId": "7cf58975-3c4b-463b-a1d7-840512e1ca35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# print(prec_values)\n",
        "# print(rec_values)\n",
        "# #plt.plot(rec_values,prec_values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.09, 0.5714285714285714, 0.59375, 0.5918367346938775, 0.12, 0.625, 0.6666666666666666, 0.71875, 0.12, 0.7551020408163265, 0.7083333333333334, 0.6979166666666666, 0.7551020408163265, 0.33, 0.32, 0.8367346938775511, 0.8775510204081632, 0.39, 0.9693877551020409, 0.44, 0.48, 0.46, 0.9897959183673469, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5]\n",
            "[0.5, 0.5116279069767442, 0.524390243902439, 0.5121951219512195, 0.5, 0.5263157894736842, 0.5294117647058824, 0.5344827586206896, 0.5, 0.52, 0.5333333333333333, 0.532258064516129, 0.52, 0.5, 0.5, 0.5294117647058824, 0.5384615384615384, 0.5, 0.625, 0.5, 0.5, 0.5, 0.75, 1.0, 0.49, 1.0, 1.0, 0.48, 1.0, 0.375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFXDagDANXzP",
        "colab_type": "code",
        "outputId": "92a163ea-8f9d-4d1b-e2ef-6191d66799f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "precision, recall = prec_values, rec_values\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
        "          average_precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.38')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np/8PEBGc4/9XZs0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}